{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb79f-b472-4171-b0ab-30c1b6585412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-04 07:39:22.289930: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-04 07:39:24.931139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-04 07:39:38.175956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from denseweight import DenseWeight\n",
    "import tensorflow as tf\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff432a1e-b491-4afe-92cd-0388dd67ee8f",
   "metadata": {},
   "source": [
    "# Our goal is to weight individual data points based on the rarity of their target values. \n",
    "\n",
    "## source https://link.springer.com/article/10.1007/s10994-021-06023-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b1622f-fad1-4b70-8684-09a53794cc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create a normalized distribution for the observations\n",
    "if region_name == 'CONUS':\n",
    "    obs = xr.open_dataset('Data/GLEAM/RZSM_anomaly.nc').sel(time = slice('2000-01-01','2015-12-31'))\n",
    "elif region_name == 'australia':\n",
    "    obs = xr.open_dataset('Data_australia/GLEAM/RZSM_anomaly.nc').sel(time = slice('2000-01-01','2015-12-31'))\n",
    "\n",
    "#Now compute the min max standardization (because this is what we are using as our prediction with deep learning model\n",
    "min_= obs.min(dim='time')\n",
    "max_ = obs.max(dim='time')\n",
    "\n",
    "#Now min max\n",
    "stand = (obs - min_) / (max_-min_).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8f74a1e-e3bf-4807-aa12-7564e9e7e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "#small subset\n",
    "sub = stand.RZSM[:,10,10].values\n",
    "# Define DenseWeight\n",
    "dw = DenseWeight(alpha=1.0)\n",
    "# Fit DenseWeight and get the weights for the 1000 samples\n",
    "weights = dw.fit(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91de3ea2-d0de-4ffc-999e-756fd2c1bdf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.63728292])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_idx = [idx for idx,i in enumerate(sub) if i ==np.nanmax(sub)] #Maximum value\n",
    "weights[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87b0a73-c53f-471f-835c-ee37851143c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.655096])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_idx = [idx for idx,i in enumerate(sub) if i ==np.nanmin(sub)] #Maximum value\n",
    "weights[min_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3783879-0e61-4bca-8c60-16d930806679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45902889, 0.45891538, 0.45892979, 0.45892236, 0.45898656,\n",
       "       0.45893287, 0.45888535, 0.45898887])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This shows that the most common values have the lowest weight\n",
    "min_weight_idx = [idx for idx,i in enumerate(weights) if i ==np.nanmin(weights)] #Maximum value\n",
    "sub[min_weight_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17207211-3934-4102-99a6-f273fa11e8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5844, 48, 96, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now apply the function to every grid cell\n",
    "#We want to save the original data into the first index in the last channel and the new kde into the 2nd index in last channel\n",
    "outKDE = np.empty(shape=(obs.time.shape[0], obs.latitude.shape[0],obs.longitude.shape[0],2))\n",
    "outKDE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f561bdba-1005-46f4-b3f2-3e7fef3b3fd0",
   "metadata": {},
   "source": [
    "# Alpha value of 1.0 doesn't seem to be helping much with learning. It may be too sensitive.\n",
    "\n",
    "## Let's try value of 0.8 for alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3884e02-289c-43c3-b1dc-3fd83801dadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f79de6d4-9182-4a00-a525-6e72ab9caf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,i in enumerate(range(obs.latitude.shape[0])):\n",
    "    for jdx,j in enumerate(range(obs.longitude.shape[0])):\n",
    "        if np.count_nonzero(np.isnan(stand.RZSM[:,idx,jdx].values)) > 0:\n",
    "            outKDE[:,idx,jdx,0] = 0\n",
    "            outKDE[:,idx,jdx,1] = 0\n",
    "        else:\n",
    "            # Define DenseWeight\n",
    "            dw = DenseWeight(alpha=alpha)\n",
    "            # Fit DenseWeight and get the weights for the number of time samples\n",
    "            outKDE[:,idx,jdx,0] = stand.RZSM[:,idx,jdx].values\n",
    "            outKDE[:,idx,jdx,1] = dw.fit(stand.RZSM[:,idx,jdx].values)\n",
    "    \n",
    "            #Now sort the indices based only on the kde values\n",
    "            sorted_indices = np.argsort(outKDE[:, idx,jdx, 0])\n",
    "            outKDE[:,idx,jdx,0] = outKDE[sorted_indices,idx,jdx,0]\n",
    "            outKDE[:,idx,jdx,1] = outKDE[sorted_indices,idx,jdx,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdefe9b1-764a-4605-bc6f-507a289c4d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now save the data for later use with loss function\n",
    "save_dir = 'Data/model_npy_inputs/weighted_density'\n",
    "os.system(f'mkdir -p {save_dir}')\n",
    "np.save(f'{save_dir}/weighted_density_{region_name}_{alpha}.npy',outKDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ffe1a-f977-41ff-bae2-cea937320414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "82eb51fc-35a0-45dd-88d8-20d41ecdf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now test with the loss functions\n",
    "y_pred = np.load('Data/model_npy_inputs/Wk_0_EX_input_data/EX10_RZSM_testing_input.npy')[:,:,:,-1]\n",
    "y_true = np.load('Data/model_npy_inputs/Verification_data/OBS_RZSM_GLEAM_lead_0_test_masked.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56935939-b432-491e-9135-32b46ae33058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1144, 48, 96)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee4eb638-1596-447e-924c-4c788dc27e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1144, 48, 96)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb738c60-6243-4918-a00f-7371417216a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred[0:11,:,:]\n",
    "y_true = y_true[0:11,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04ba28a-126a-405c-a3eb-ca8cbcefea17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 48, 96)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "92666a4a-8f06-43c8-8085-d82e3308d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Source https://github.com/yingkaisha/keras-unet-collection/blob/main/keras_unet_collection/losses.py\n",
    "def _crps_tf_dense(y_true, y_pred, factor=0.08):\n",
    "    \n",
    "    '''\n",
    "    core of (pseudo) CRPS loss.\n",
    "    \n",
    "    y_true: two-dimensional arrays\n",
    "    y_pred: two-dimensional arrays\n",
    "    factor: importance of std term\n",
    "    '''\n",
    "    def mae_by_whole_region_mean(y_true, y_pred, factor=0.08):\n",
    "        #MAE by grid cell\n",
    "        mae = tf.cast(K.mean(tf.abs(y_pred - y_true)),dtype=tf.float32)\n",
    "        dist = tf.cast(np.nanmean(np.nanstd(y_pred,axis=0)),dtype=tf.float32)\n",
    "        crps_exp = mae - factor*dist\n",
    "    \n",
    "        #Just provide the names so we know what they are\n",
    "        norm_obs = density[:,:,:,0] #Shape is (5844, 48, 96)\n",
    "        norm_kde = density[:,:,:,1] #Shape is (5844, 48, 96)\n",
    "    \n",
    "        #y_pred shape is TensorShape([11, 48, 96])\n",
    "        y_pred_mean = tf.experimental.numpy.nanmean(y_pred,axis=0)\n",
    "        #Try with just taking the mean of the predictions\n",
    "        #Find the absolute value difference which is equal to us finding which observation value is closest to the prediction value (y_pred)\n",
    "        # y_pred shape is (48,96)\n",
    "        abs_diff = tf.math.abs(norm_obs - tf.experimental.numpy.nanmean(y_pred,axis=0)) # Shape is TensorShape([5844, 48, 96])\n",
    "        \n",
    "        #Find the date index that is the lowest\n",
    "        min_day_indices = tf.math.argmin(abs_diff,axis=0) #Shape is (48,96)\n",
    "    \n",
    "        #Check some values (looks good)\n",
    "        # x,y=10,20\n",
    "        # min_arg = argmin[x,y]\n",
    "        # abs_diff\n",
    "        # norm_obs[min_arg,x,y]\n",
    "        # y_pred_mean[x,y]\n",
    "    \n",
    "        #Now we need to subset norm_kde to find its value\n",
    "        # Use tf.gather to obtain the subset of 'norm_kde'\n",
    "        subset_array = tf.gather(norm_kde, min_day_indices[:, tf.newaxis, tf.newaxis], axis=0)\n",
    "        # Squeeze the tensor to remove singleton dimensions\n",
    "        avg_kde = K.mean(tf.squeeze(subset_array, axis=[1, 2]))\n",
    "    \n",
    "    \n",
    "        \n",
    "        #Now multiply the grid with the data and take the mean to produce a scalar\n",
    "        dense_crps = crps_exp * avg_kde\n",
    "\n",
    "        # #check the other result where we apply the mean first to produce a scalar with no dense crps\n",
    "        # mae = tf.cast(K.mean(tf.abs(y_pred - y_true)),dtype=tf.float32)\n",
    "        # #Then find another scalar\n",
    "        # dist = np.nanmean(np.nanstd(y_pred,axis=0))\n",
    "        # dist = tf.cast(dist,dtype=tf.float32)\n",
    "        # #Then compute a scalar\n",
    "        # (mae - factor*dist)\n",
    "        \n",
    "        return(dense_crps)\n",
    "\n",
    "\n",
    "\n",
    "    def mae_by_grid_cell_then_take_mean(y_true, y_pred, factor=0.08):\n",
    "        #right now this is too difficult to try and get to work performance wise, So it's currently unfinished. \n",
    "        #MAE by grid cell\n",
    "        mae = tf.abs(y_pred - y_true)\n",
    "        dist = np.nanstd(y_pred,axis=0)\n",
    "        crps_exp = mae - factor*dist\n",
    "    \n",
    "        #Just provide the names so we know what they are\n",
    "        norm_obs = density[:,:,:,0] #Shape is (5844, 48, 96)\n",
    "        norm_kde = density[:,:,:,1] #Shape is (5844, 48, 96)\n",
    "    \n",
    "        \n",
    "\n",
    "        #Find the absolute value difference which is equal to us finding which observation value is closest to the prediction value (y_pred)\n",
    "        # y_pred shape is (48,96)\n",
    "        abs_diff = tf.math.abs(norm_obs -y_pred[:,tf.newaxis,:,:]) # Shape is TensorShape([11, 5844, 48, 96])\n",
    "        \n",
    "        #Find the date index that is the lowest\n",
    "        min_day_indices = tf.math.argmin(abs_diff,axis=1) #Shape is (11,48,96)\n",
    "    \n",
    "        #Check some values (looks good)\n",
    "        # x,y=10,20\n",
    "        # min_arg = argmin[x,y]\n",
    "        # abs_diff\n",
    "        # norm_obs[min_arg,x,y]\n",
    "        # y_pred_mean = tf.experimental.numpy.nanmean(y_pred,axis=0) #y_pred shape is TensorShape([48, 96])\n",
    "        # y_pred_mean[x,y]\n",
    "    \n",
    "        #Now we need to subset norm_kde to find its value\n",
    "        # Use tf.gather to obtain the subset of 'norm_kde'\n",
    "        subset_array = tf.gather(norm_kde, min_day_indices[tf.newaxis, :], axis=0)\n",
    "        subset_array.shape\n",
    "        # Squeeze the tensor to remove singleton dimensions\n",
    "        avg_kde = K.mean(tf.squeeze(subset_array, axis=[1, 2]))\n",
    "\n",
    "        #Now multiply the grid with the data and take the mean to produce a scalar\n",
    "        dense_crps = crps_exp * avg_kde\n",
    "        return(dense_crps)\n",
    "\n",
    "    return mae_by_whole_region_mean(y_true, y_pred, factor=0.08)\n",
    "\n",
    "def crps2d_tf_dense(y_true, y_pred, factor=0.08):\n",
    "    \n",
    "    '''\n",
    "    (Experimental)\n",
    "    An approximated continuous ranked probability score (CRPS) loss function:\n",
    "    \n",
    "        CRPS = mean_abs_err - factor * std\n",
    "        \n",
    "    * Note that the \"real CRPS\" = mean_abs_err - mean_pairwise_abs_diff\n",
    "    \n",
    "     Replacing mean pairwise absolute difference by standard deviation offers\n",
    "     a complexity reduction from O(N^2) to O(N*logN) \n",
    "    \n",
    "    ** factor > 0.1 may yield negative loss values.\n",
    "    \n",
    "    Compatible with high-level Keras training methods\n",
    "    \n",
    "    Input\n",
    "    ----------\n",
    "        y_true: training target with shape=(batch_num, x, y, 1)\n",
    "        y_pred: a forward pass with shape=(batch_num, x, y, 1)\n",
    "        factor: relative importance of standard deviation term.\n",
    "        \n",
    "    '''\n",
    "    # tf.print(f'y_true shape = {y_true.shape}')\n",
    "    # tf.print(f'y_pred shape = {y_pred.shape}')\n",
    "    \n",
    "    y_pred = tf.squeeze(tf.convert_to_tensor(y_pred))\n",
    "    y_true = tf.squeeze(tf.cast(y_true, y_pred.dtype))\n",
    "\n",
    "    #For testing\n",
    "    # y_pred = y_pred[0:66,:,:]\n",
    "    # y_true = y_true[0:66,:,:]\n",
    "    \n",
    "    # tf.print(y_true.shape)\n",
    "    # tf.print(y_pred.shape)\n",
    "    # batch_num = y_pred.shape.as_list()[0]\n",
    "    \n",
    "    crps_out = 0\n",
    "    start_,end_ = 0,11\n",
    "\n",
    "    #testing\n",
    "    # factor=0.08\n",
    "    \n",
    "    #Split the batch sizes to not get an Nan at the last part of the batch\n",
    "    new_range = y_true.shape[0]//11\n",
    "    for i in range(new_range):\n",
    "        # print(i)\n",
    "        # break\n",
    "        crps_out += _crps_tf_dense(y_true=y_true[start_:end_, ...], y_pred =y_pred[start_:end_, ...], factor=factor)\n",
    "        #must add 11 each time\n",
    "        start_ +=11\n",
    "        end_ += 11\n",
    "    \n",
    "    #Divide by total number of inits\n",
    "    # crps_out = crps_out/new_range\n",
    "\n",
    "    #Have them all as a sum\n",
    "    crps_out = crps_out\n",
    "        \n",
    "    return crps_out\n",
    "\n",
    "global density\n",
    "density = np.load('Data/model_npy_inputs/weighted_density/weighted_density.npy')\n",
    "density = tf.cast(np.nan_to_num(density, nan=0),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f88d98-c138-419a-87a7-236ccc56888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a418ae1-fb36-48e1-a77e-72cab4ae7525",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b62d5-8907-4ab0-87b3-0bd57b0f9490",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5faf269-fbd9-4cf7-9103-d3aebcfa2f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb83b8-79c7-4910-9c59-538d9b8d80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b12e4-ad43-42b1-ae5d-a3a39f56a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b989d0eb-c15b-4901-ae95-fbfaee685b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ca5c0f-1fa2-4728-b73e-802396299327",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load sample data\n",
    "test_ref = np.load('Data/model_npy_inputs/Wk_0_EX_input_data/EX10_RZSM_testing_input.npy')\n",
    "obs = np.load("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5bcc0-d22b-415f-bae4-fabd0d2bed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check loss values\n",
    "# def open_loss(experiment):\n",
    "#     loss = pd.read_csv(f'Losses_with_OBS/Wk_1/Wk1_{experiment}')\n",
    "#     print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8064e6b7-3144-43d3-8809-09bd4b039b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_loss('EX12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4deceee-9448-4e7a-8d21-e621f522852e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # #Load model and check summary\n",
    "# model = load_model(f'checkpoints/Wk_1/Wk1_EX5',compile=False) #don't need the custom loss function for predictions\n",
    "# model.summary() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351577f5-3861-4f53-8677-123a84e39de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78fa58-8c54-4d6f-a3a4-105ca26f05cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Iterate through the layers and print the shape of each layer's weights\n",
    "# num\n",
    "# for layer in m.layers:\n",
    "#     if layer.weights:\n",
    "#         print(f\"Layer: {layer.name}\")\n",
    "#         for weight in layer.weights:\n",
    "#             print(f\"  {weight.name}: {weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69887618-888d-4ba0-b681-3b1d1eb6c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "lead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb86ad8-68bb-4231-b032-d7b591afb6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31c8ac-b8e4-4590-a1a4-1277cafeb359",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666089cd-509a-426d-93d7-79d01b097df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c36c8d-cd2e-4e6b-b4ff-159bc7ca8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cae89c3-4759-4d69-a752-77fef53de2f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
