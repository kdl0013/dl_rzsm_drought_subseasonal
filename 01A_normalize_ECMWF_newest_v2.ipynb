{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bdbb79f-b472-4171-b0ab-30c1b6585412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from metpy.units import units\n",
    "import preprocessUtils as putils\n",
    "from typing import Tuple\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b451f-7893-4304-813a-eca432631ab6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3ff845-71d9-422d-9e33-09a3af5be48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.) Needs about 50GB RAM for CONUS (shape of file is 48 x 96 lat/lon)\n",
    "\n",
    "Select a specific region (either CONUS, australia, or china)\n",
    "\n",
    "Takes about 2 hours to fully complete everything.\n",
    "'''\n",
    "\n",
    "''' \n",
    "GEFSv12 reforecast appears to have some missing values for some init dates. \n",
    "We are replacing those with the mean because this is messing up min/max scaling\n",
    "very badly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44223dd6-1a0f-4c8d-a95b-cdf86d30fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run_OBS_reformat_to_SubX = True #For reformatting the observations to the same lead-time format as GEFSv12\n",
    "plot_distribution = False #This set of functions takes a long time to plot. But if you want to see the anomaly and min-max distribution, select True\n",
    "\n",
    "'''Daily lags for observations -  we can specify up to 12 weeks in lagged RZSM data (these are 7-day increments). \n",
    "You can select whatever values you want (as long as they are before the reforecast period'''\n",
    "daily_lags = putils.make_daily_lags()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d845cd-a1b4-454a-b548-c9ff8577e2a8",
   "metadata": {},
   "source": [
    "# Data directory locations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "705e8a43-188b-46c5-a45f-02a2637c28c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n"
     ]
    }
   ],
   "source": [
    "region_name = 'CONUS' #or ['australia', 'CONUS', 'china']\n",
    "\n",
    "reforecast_input = 'ECMWF'  #['GEFSv12', 'ECMWF']\n",
    "\n",
    "start_date = '1999-01-01' #for reanalysis data. Select earlier dates than the forecasts to allow for smoothing and because we need lagged observations\n",
    "end_date = '2020-03-01' #for reanalysis data\n",
    "\n",
    "\n",
    "global verification_var\n",
    "verification_var = 'soilw_bgrnd_GLEAM' #this is for what we are verifying with DL outputs\n",
    "\n",
    "#Gleam observations\n",
    "gleam_dir = 'Data/GLEAM'\n",
    "\n",
    "#ERA5 observations\n",
    "era5_dir = 'Data/ERA5'\n",
    "\n",
    "\n",
    "global fcst_dir_for_template\n",
    "\n",
    "if region_name != 'CONUS':\n",
    "    #Gleam observations\n",
    "    gleam_dir = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "    #Forecast predictions\n",
    "    if reforecast_input == 'ECMWF':\n",
    "        fcst_dir = f'Data/ECMWF'\n",
    "        \n",
    "    elif reforecast_input == 'GEFSv12':\n",
    "        fcst_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "        \n",
    "    fcst_dir_for_template = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "    \n",
    "    #ERA5 observations\n",
    "    era5_dir = f'Data_{region_name}/ERA5'\n",
    "elif region_name == 'CONUS':\n",
    "    #Gleam observations\n",
    "    gleam_dir = f'Data/GLEAM'\n",
    "    \n",
    "    #Forecast predictions\n",
    "    if reforecast_input == 'ECMWF':\n",
    "        fcst_dir = f'Data/ECMWF'\n",
    "        \n",
    "    elif reforecast_input == 'GEFSv12':\n",
    "        fcst_dir = f'Data/GEFSv12_reforecast'\n",
    "\n",
    "    fcst_dir_for_template = f'Data/GEFSv12_reforecast'\n",
    "    \n",
    "    #ERA5 observations\n",
    "    era5_dir = f'Data/ERA5'\n",
    "\n",
    "\n",
    "#Set years for training, validation, and testing\n",
    "train_end = 2015\n",
    "val_end = 2017\n",
    "test_start = 2018\n",
    "\n",
    "\n",
    "spfh_unit = 'kg/kg' #Make sure that our reforecast and observation have the same unit for specific humidity\n",
    "\n",
    "global mask, mask_vals\n",
    "mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "mask_vals = mask[putils.xarray_varname(mask)][0,:,:].values\n",
    "\n",
    "#Get initialized dates from GEFSv12 to convert observations for easier stacking\n",
    "global init_date_list\n",
    "\n",
    "#Forecast predictions\n",
    "#Only use GEFSv12 dates\n",
    "init_date_list = putils.get_init_date_list(forecast_variable_path=f'Data/GEFSv12_reforecast/soilw_bgrnd')\n",
    "\n",
    "init_date_list_datetime = [pd.to_datetime(i) for i in init_date_list]\n",
    "\n",
    "global var_list, var_list_obs, var_list_ref\n",
    "\n",
    "#for observations\n",
    "var_list_obs = ['soilw_bgrnd', 'tmax_2m', 'spfh_2m', 'pwat_eatm','diff_temp_2m','hgt_pres']\n",
    "\n",
    "if reforecast_input == 'GEFSv12':\n",
    "    var_list_ref = var_list_obs #Observation variables to pre-process\n",
    "elif reforecast_input == 'ECMWF':\n",
    "    var_list_ref = ['soilw_bgrnd','d2m', 't2m', 'tcw']\n",
    "\n",
    "\n",
    "\n",
    "global lead_select\n",
    "lead_select = [6,13,20,27,34] #For subsetting data by leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023cfa8-d5b4-4bc2-8031-d4d71d939e39",
   "metadata": {},
   "source": [
    "# Load Observation Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cc6fa0-f9ae-4c0a-89db-5bbb73ddff50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Loading observations. Applying a 7-day rolling mean between {start_date} and {end_date} for {region_name} Region.')\n",
    "\n",
    "RZSM_obs_GLEAM,proper_date_time = putils.open_reanalysis_files_and_preprocess_rolling_mean_RZSM_only(path_to_file=f'{gleam_dir}/RZSM_weighted_mean_0_100cm.nc4',\n",
    "                                                             file_variable='GLEAM_RZSM_obs',start_date=start_date,end_date=end_date,region_name = region_name)\n",
    "\n",
    "precipitable_water_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/total_column_water_merged.nc4',\n",
    "                                                             file_variable='ERA_precipitable_water_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "geopotential_z200_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/geopotential_merged.nc4',\n",
    "                                                             file_variable='ERA_geopotential_z200_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "\n",
    "max_temp_ERA, diff_temp_ERA = putils.open_temperature_files_and_preprocess_rolling_mean(path_to_tmax =f'{era5_dir}/maximum_2m_temperature_merged.nc4', \n",
    "                                                                                 path_to_tmin =f'{era5_dir}/minimum_2m_temperature_merged.nc4',\n",
    "                                                                                 file_variable_tmax = 'ERA_Tmax_obs',file_variable_tmin = 'ERA_Tmin_obs',\n",
    "                                                                                 start_date=start_date, end_date=end_date, \n",
    "                                                                                 region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "spfh_ERA = putils.calculate_2m_specific_humidity_and_preprocess(path_to_dewpoint=f'{era5_dir}/2m_dewpoint_temperature_merged.nc4', \n",
    "                                              path_to_pressure=f'{era5_dir}/surface_pressure_merged.nc4', \n",
    "                                              file_variable_dewpoint='ERA_2m_dewpoint_obs', file_variable_pressure='ERA_surface_pressure_obs', \n",
    "                                              start_date=start_date, end_date=end_date, \n",
    "                                              region_name = region_name, proper_date_time=proper_date_time,\n",
    "                                              obs_dir = era5_dir, spfh_unit = spfh_unit)\n",
    "\n",
    "print('Loaded all observations.')\n",
    "\n",
    "#Print saving the anomaly for each region as the standard format\n",
    "climatology_season = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.year'] <= train_end)).groupby(f\"time.season\").mean()\n",
    "\n",
    "summer_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'JJA')) - climatology_season.sel(season='JJA')\n",
    "fall_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'SON')) - climatology_season.sel(season='SON')\n",
    "winter_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'DJF')) - climatology_season.sel(season='DJF')\n",
    "spring_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'MAM')) - climatology_season.sel(season='MAM')\n",
    "\n",
    "combined_files = xr.concat([summer_,fall_,winter_,spring_],dim='time').sortby('time')\n",
    "combined_files=combined_files.astype(np.float32)\n",
    "\n",
    "#Now save anomaly for later use\n",
    "if region_name == 'CONUS':\n",
    "    location = 'Data/GLEAM'\n",
    "else:\n",
    "    location = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "combined_files.to_netcdf(f'{location}/RZSM_anomaly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af54dc1-ab3a-4d10-afa1-9b81b016884d",
   "metadata": {},
   "source": [
    "# Now restructure the observation data to the same lead-time format as GEFSv12. This will allow for an easier computation of anomalies using climpred functions.\n",
    "\n",
    "### New indexes will be from range (-84 days to -1 day and then the typical 7-day lead times starting from 0 to 34). Because we have applied a 7-day rolling mean to observations so far, even index -1 for the day has the data from the previous 7-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35812da6-4e93-4db3-b0d2-6c23269410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_info_for_processing(var: str) -> Tuple[str, xr.DataArray]:\n",
    "    #Names to save the data so that it is in the exact same format as the Reforecasts\n",
    "    \n",
    "    if var == 'soilw_bgrnd':\n",
    "        # var_name = var #var_name is the actual name of the variable in the xarray file\n",
    "        model_name = 'GLEAM' #source of the data\n",
    "        obs_file = RZSM_obs_GLEAM #the actual data file\n",
    "    elif var == 'tmax_2m':\n",
    "        # var_name = 'tasmax'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = max_temp_ERA\n",
    "    elif var == 'spfh_2m':\n",
    "        # var_name = 'huss'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = spfh_ERA\n",
    "    elif var == 'pwat_eatm':\n",
    "        # var_name = 'precipitable_water'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = precipitable_water_ERA\n",
    "    elif var == 'diff_temp_2m':\n",
    "        # var_name = 'diff_tmax_tmin'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = diff_temp_ERA\n",
    "    elif var == 'hgt_pres':\n",
    "        # var_name = 'z'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = geopotential_z200_ERA\n",
    "    \n",
    "    output_OBSERVATIONS_reformat_dir = f'Data/{model_name}/reformat_to_reforecast_shape/{region_name}' #Directory to save \n",
    "    save_dir = f'{output_OBSERVATIONS_reformat_dir}/{var}'\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    return(model_name,obs_file,output_OBSERVATIONS_reformat_dir,save_dir)\n",
    "\n",
    "\n",
    "\n",
    "def convert_OBS_to_SubX_format(_date: str) -> None:  \n",
    "# for _date in init_date_list:\n",
    "    # var='RZSM_weighted'\n",
    "    # _date=init_date_list[0]\n",
    "\n",
    "    \n",
    "    '''We are going to create new leads that are different than reforecast. The reasoning for this is that we want the actual weekly lags (and 1 day lag) and this will\n",
    "    assist with future predictions within the deep learning model'''\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        #Currently the GLEAM and ERA observations are in a format with only positive values for the longitude.\n",
    "        #The current reforecast has negative values for those West of 0 degrees\n",
    "        \n",
    "        new_X_coords = [i+360 if i < 0 else i for i in template.X.values]\n",
    "        single_file = template.assign_coords({'X':new_X_coords})\n",
    "        single_file = xr.zeros_like(single_file)\n",
    "        open_date_SubX = putils.restrict_to_bounding_box(single_file,mask)\n",
    "        \n",
    "    elif region_name != 'CONUS':\n",
    "        open_date_SubX = template\n",
    "        open_date_SubX = xr.zeros_like(open_date_SubX)\n",
    "    \n",
    "    for var in var_list_obs:\n",
    "        # break\n",
    "        model_name, obs_file, output_OBSERVATIONS_reformat_dir, save_dir = return_info_for_processing(var)     \n",
    "\n",
    "        obs_file_name = f'{var}_reformat_{reforecast_input}_{_date}.nc4'\n",
    "        save_file = f'{save_dir}/{obs_file_name}'\n",
    "        \n",
    "        # os.system(f'rm {save_file}')\n",
    "        if os.path.exists(save_file):\n",
    "            print(f'Completed date {_date}')\n",
    "            pass\n",
    "        else:\n",
    "            # print(f'Working on variable {var} for date {_date}')\n",
    "            out_file = open_date_SubX.copy(deep=True)\n",
    "    \n",
    "            '''We are going to create a new lead day that represents the previous day before the forecast was initialized\n",
    "            #New shape will be (1x11x48xlatxlon)\n",
    "            This will include the day lag 1, and weekly lags 1-12'''\n",
    "            \n",
    "            file_shape = out_file[putils.xarray_varname(out_file)].shape\n",
    "            \n",
    "            named_str_leads = [str(i) for i in np.arange(-1,open_date_SubX.L.shape[0])]\n",
    "            new_shape = np.empty(shape=(1,file_shape[1],file_shape[2]+13,file_shape[3],file_shape[4]))\n",
    "            new_shape.shape\n",
    "            \n",
    "            daily_lead_lags = daily_lags + list(out_file.L.values[:])\n",
    "            \n",
    "            new_lead_days = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    test = (['S','M','L','Y','X'], new_shape[:,:,:,:,:]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    S = np.atleast_1d(_date),\n",
    "                    X = open_date_SubX.X.values,\n",
    "                    Y = open_date_SubX.Y.values,\n",
    "                    L = daily_lead_lags,\n",
    "                    M = open_date_SubX.M.values,\n",
    "    \n",
    "                ),\n",
    "                attrs = dict(\n",
    "                    Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                    cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "            )   \n",
    "            \n",
    "            #Create a file to overwrite\n",
    "            out_file = xr.zeros_like(new_lead_days)\n",
    "    \n",
    "            print(f'Working on variable {var} and initialized day {_date} to find values integrating with SubX models, leads, & coordinates and saving data into {save_dir}.')\n",
    "            \n",
    "            for idx,i_lead in enumerate(new_lead_days.L.values):\n",
    "                # break\n",
    "    \n",
    "                date_val = pd.to_datetime(pd.to_datetime(_date) + dt.timedelta(days=int(i_lead)+0)) #Adding +1 may be suitable for other forecasts which predict the next day. But GEFSv12 predicts lead 0 as 12 UTC on the same date it is initialized\n",
    "                #But be careful if you adapt this code to a new script. We are looking backwards in time from the first date.\n",
    "                    \n",
    "                date_val = f'{date_val.year}-{date_val.month:02}-{date_val.day:02}'\n",
    "    \n",
    "                out_file[putils.xarray_varname(out_file)][0,:, idx, :, :] = \\\n",
    "                    obs_file[putils.xarray_varname(obs_file)].sel(time = date_val).values\n",
    "    \n",
    "    \n",
    "            if var == 'soilw_bgrnd':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        RZSM = (['S','M','L','Y','X'],    out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )                    \n",
    "            elif var == 'tmax_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        tmax = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'spfh_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        spfh_2m = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'pwat_eatm':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        pwat = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )      \n",
    "            elif var == 'diff_temp_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        diff_tmax_tmin = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            elif var == 'hgt_pres':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        z200 = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            var_OUT = var_OUT.astype(np.float32) #all the files need to be in float32 for the deep learning algorithm to work best\n",
    "            \n",
    "            #Save as a netcdf for later processing\n",
    "            var_OUT.to_netcdf(path = save_file, mode ='w')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "#template for overwriting\n",
    "if region_name == 'CONUS':\n",
    "    ref_dir = f'Data/GEFSv12_reforecast/soilw_bgrnd' #Just use a single reference directory to serve as the template for file creation\n",
    "else:\n",
    "    ref_dir = f'Data_{region_name}/GEFSv12_reforecast/soilw_bgrnd'\n",
    "\n",
    "global template\n",
    "#Grab a single SubX to use as the template. Doesn't matter if it is the same variable or not or the same date\n",
    "\n",
    "template = sorted(glob(f'{ref_dir}/*.n*'))[0]\n",
    "template = xr.open_dataset(template)\n",
    "\n",
    "# init_date_list.reverse()\n",
    "for _date  in init_date_list:\n",
    "    convert_OBS_to_SubX_format(_date)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p = Pool(5)\n",
    "#     p.map(convert_OBS_to_SubX_format,init_date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0b05a-0cad-435a-b67b-d5b08427c9a8",
   "metadata": {},
   "source": [
    "# Now process all the observations including:\n",
    "### 1.) Create anomaly\n",
    "### 2.) Plot distribution after anomaly\n",
    "### 3.) Create min-max standardization\n",
    "### 4.) Plot distribution after min-max standardization\n",
    "### 5.) Stack verification file (for RZSM) into a seperate directory (as a pickle file)\n",
    "### 6.) Stack all the different lags for all variables into seperate directory (as a pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6618f5-cc10-483d-a630-ddc197f251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_observations_only(obs_source: str, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                    return_obs_minmax: bool, fcst_dir: str, rerun_function: bool) -> None:\n",
    "\n",
    "\n",
    "    #Args for creating anomaly (testing)\n",
    "    # obs_source = 'GLEAM'\n",
    "    # region_name = region_name\n",
    "    # var_name= 'soilw_bgrnd'\n",
    "    # obs_or_forecast='obs' \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "        #7-day rolling mean has already been applied to observations\n",
    "        \n",
    "        #Create the seasonal mean using climpred functions\n",
    "        file_locations = f'Data/{obs_source}/reformat_to_reforecast_shape/{region_name}/{var_name}/*{reforecast_input}*.n*'\n",
    "\n",
    "        print(f'\\nRetrieving data from {file_locations}\\n')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(xr.open_mfdataset(file_locations),train_end=train_end)\n",
    "        \n",
    "        print(f'\\nCreated seasonal anomalies on all {anom.L.values} lead days')\n",
    "        \n",
    "        print('\\n7-day rolling mean not being applied. We already did this when creating the data.')\n",
    "\n",
    "        anom = anom.load()\n",
    "        \n",
    "        anom_mean_subset = anom.sel(L=lead_select) #Make a small subset of the data\n",
    "\n",
    "\n",
    "        if var_source_combined == verification_var:\n",
    "            #Save the anomaly files\n",
    "            print(f'Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads {anom_mean_subset.L.values}')\n",
    "            putils.convert_OBS_anomaly_to_SubX_format(init_date_list = init_date_list, region_name = region_name, anomaly_file = anom_mean_subset, fcst_dir = fcst_dir_for_template, lead_select = lead_select)\n",
    "            \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('\\nLooking at what the first and last dates are within the anomaly file.')\n",
    "        print_dates(anom_mean_subset)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('\\nPlotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'anomaly', region_name = region_name,\n",
    "                                                       lead_select=lead_select)\n",
    "    \n",
    "        #Min max standardize\n",
    "        min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, obs_min_max = None,\n",
    "                                                                lead_select = lead_select)\n",
    "        \n",
    "        #Plot min max distribution\n",
    "        if plot_distribution:\n",
    "            print('\\nPlotting min max distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "                                                       lead_select=lead_select)\n",
    "    \n",
    "        if var_source_combined == verification_var:\n",
    "            #We only need to save this as the verification file for deep learning model\n",
    "            putils.create_stacked_files_by_lead_for_verification(file = min_max, train_end=train_end, val_end=val_end, test_start=test_start, variable=var_source_combined,\n",
    "                                                          obs_or_forecast=obs_or_forecast, region_name = region_name, init_date_list = init_date_list,\n",
    "                                                                lead_select=lead_select)\n",
    "    \n",
    "        #Now create stacked files for each individual variable for each lead time\n",
    "        putils.retrieve_stacked_files_and_save_for_model_inputs(file = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                                                val_end = val_end, test_start = test_start, init_date_list = init_date_list)\n",
    "\n",
    "    if return_obs_minmax == False:\n",
    "        return(f'Completed variable {var_source_combined}')\n",
    "    else:\n",
    "        return(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d01fdc-5a08-4b64-9803-f2c3b3203dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rerun_function = True\n",
    "\n",
    "\n",
    "################### OBSERVATIONS ###########################\n",
    "\n",
    "obs_RZSM_minmax = anomaly_plot_standardize_save_observations_only(obs_source= 'GLEAM', region_name = region_name, var_name='soilw_bgrnd', train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = True, fcst_dir = fcst_dir,\n",
    "                                                                 rerun_function = rerun_function)\n",
    "\n",
    "for var_name in var_list_obs:\n",
    "    if 'soil' in var_name:\n",
    "        pass\n",
    "    else:\n",
    "        anomaly_plot_standardize_save_observations_only(obs_source= 'ERA5', region_name = region_name, var_name=var_name, train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                        obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = False, fcst_dir = fcst_dir,\n",
    "                                                       rerun_function = rerun_function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26ab6c-dbcf-4358-b755-6462445d54a3",
   "metadata": {},
   "source": [
    "# Now create datasets for GEFSV12 reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddf36a-6343-405a-8079-b2387f36edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "# print(f'\\nLoading files for variable {name_of_var}')\n",
    "\n",
    "# file_paths = sorted(glob(f'{dir_path}/{name_of_var}/*.n*'))\n",
    "\n",
    "# # Open individual xarray datasets from the files\n",
    "# datasets = [xr.open_dataset(file) for file in file_paths]\n",
    "\n",
    "# #We have different coordinate systems. So we need to add 360 to each of the X coordinates if they are negative\n",
    "# if region_name == 'CONUS':\n",
    "#     print(f'We are changing the coordinates of CONUS to match similar format as GLEAM')\n",
    "#     new_X_coords = [i+360 if i < 0 else i for i in datasets[0].X.values]\n",
    "#     #Add the new coordinates\n",
    "#     datasets = [file.assign_coords({'X':new_X_coords}) for file in datasets]\n",
    "#     datasets = [putils.restrict_to_bounding_box(file,mask) for file in datasets]\n",
    "    \n",
    "# # Concatenate the datasets along a specific dimension\n",
    "# print(f'\\nConcatenating files for {name_of_var}. Takes about 10 minutes for CONUS.')\n",
    "# file = xr.concat(datasets, dim='S', combine_attrs=\"override\")\n",
    "\n",
    "# putils._count_zero_values(file, f'{name_of_var}_GEFSv12')\n",
    "# # file = dask.delayed(xr.concat)(datasets, dim=\"S\")\n",
    "\n",
    "# # Close individual datasets to free up resources\n",
    "# for dataset in datasets:\n",
    "#     dataset.close()\n",
    "\n",
    "# # Some files have a value of zero which is skewing results. So we need to replace with the mean.\n",
    "# print('Masking files if they have value of zero')\n",
    "# file_masked = file.where(file == 0, np.nan, file)\n",
    "# #Now take the mean \n",
    "# file_masked_mean = file_masked.mean(dim='S')\n",
    "# #Now replace the values where there was a zero\n",
    "# file = xr.where(np.isnan(file), file_masked_mean, file)\n",
    "\n",
    "# _count_np_nan_values(file, f'{name_of_var}_GEFSv12')\n",
    "# _count_zero_values(file, f'{name_of_var}_GEFSv12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad12ffb-6b77-4e23-9970-c7729c4c52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_reforecast(obs_source, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                  obs_min_max: xr.DataArray, rerun_function: bool, init_date_list: list, fcst_dir: str) -> None:\n",
    "    # #Testing\n",
    "    # obs_source= 'GEFSv12'\n",
    "    # region_name = region_name\n",
    "    # var_name='tmax_2m'\n",
    "    # train_end = train_end\n",
    "    # val_end = val_end\n",
    "    # test_start = test_start\n",
    "    # obs_or_forecast = 'reforecast'\n",
    "\n",
    "    \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "\n",
    "        # file = putils.return_GEFSv12_reforecast_files(dir_path = fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "        if ('ECMWF' in var_source_combined) and (var_name == 'soilw_bgrnd'):\n",
    "            fcst_dir = f'{fcst_dir}/{var_name}_processed/{region_name}'\n",
    "            file = putils.return_ECMWF_reforecast_files_by_concatenation(dir_path = fcst_dir, name_of_var = var_name, region_name = region_name,init_date_list = init_date_list)\n",
    "        elif ('ECMWF' in var_source_combined):\n",
    "            fcst_dir = f'{fcst_dir}/temp_pwat_dewpoint_processed/{region_name}'\n",
    "            file = putils.return_ECMWF_reforecast_files_by_concatenation(dir_path = fcst_dir, name_of_var = var_name, region_name = region_name,init_date_list = init_date_list)\n",
    "        elif 'GEFSv12' in var_source_combined:\n",
    "            file = putils.return_reforecast_files_by_concatenation(dir_path = fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "\n",
    "        if var_name == 'hgt_pres':\n",
    "            file = file* 9.80665 #must place in the same unit as ERA5. the unit is gravity\n",
    "        \n",
    "        #Testing if rolling mean first fixes anything. Currenlty the hgt_pres values are way too low for the min\n",
    "\n",
    "        file_test_full = file.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        file_test_small = file_test_full.sel(L=lead_select)\n",
    "        \n",
    "        print(f'\\nCreating seasonal anomalies on all {file_test_small.L.values} lead days')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(file_test_small,train_end=train_end)\n",
    "        \n",
    "        anom_full, mean_season_full = putils.create_seasonal_anomaly(file_test_full,train_end=train_end)\n",
    "        # print('\\nNow taking the 7-day rolling mean of anomalies. We are not replacing lead day 0.')\n",
    "        # anom_mean = anom.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        # anom_mean[putils.xarray_varname(file)][:,:,0,:,:] = anom[putils.xarray_varname(anom)][:,:,0,:,:]\n",
    "        \n",
    "        anom = anom.load()\n",
    "        \n",
    "        print(f'\\nShape of anomaly mean = {anom[putils.xarray_varname(anom)].shape}')\n",
    "        \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('Looking at what dates are within the file')\n",
    "        print_dates(anom)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('Plotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, \n",
    "                                                        anomaly_or_min_max = 'anomaly', region_name = region_name,lead_select=lead_select)\n",
    "    \n",
    "        #Min max standardize\n",
    "        if 'soilw_bgrnd' in var_source_combined:\n",
    "            putils.save_baseline_RZSM_anomaly(anom_full,region_name,fcst_dir)\n",
    "            min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = obs_min_max, lead_select = lead_select)\n",
    "            min_max = xr.where(mask_vals == 1,min_max,0)\n",
    "           \n",
    "        else:\n",
    "            min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = None, lead_select = lead_select)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Plot min max distribution\n",
    "        if plot_distribution:\n",
    "            print('Plotting min min distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "                                                       lead_select=lead_select)\n",
    "\n",
    "        putils.stack_reforecasts(var_min_max = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                 val_end = val_end, test_start = test_start, lead_select=lead_select)\n",
    "\n",
    "    return(f'Completed variable {var_source_combined}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12858852-e5c8-43ed-bcef-71de866056a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load tmax and tmin and then create diff_temp_2m (difference in temperature)\n",
    "def create_diff_temp_reforecast(region_name: str) -> None:\n",
    "    if region_name == 'CONUS':\n",
    "        data_dir = f'Data/GEFSv12_reforecast'\n",
    "    else:\n",
    "        data_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "\n",
    "    save_dir = f'{data_dir}/diff_temp_2m'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    tmax_ = sorted(glob(f'{data_dir}/tmax_2m/*.n*'))\n",
    "    tmin_ = sorted(glob(f'{data_dir}/tmin_2m/*.n*'))\n",
    "\n",
    "    for idx, file in enumerate(tmax_):\n",
    "        # break\n",
    "        max_ = xr.open_dataset(file)\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        \n",
    "        date_ = pd.to_datetime(max_.S.values[0])\n",
    "        save_file = f'{save_dir}/diff_temp_2m_{date_.year}-{date_.month:02}-{date_.day:02}.nc4'\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            pass\n",
    "        else:\n",
    "            min_ = xr.open_dataset(tmin_[idx])\n",
    "            # print(f'Min: {min_[putils.xarray_varname(min_)].min().values}')\n",
    "            assert max_.S.values == min_.S.values, 'Dates are not equal, something is wrong with either glob or there are missing files from either tmax or tmin'\n",
    "    \n",
    "            diff_temp = (max_.tasmax - min_.tasmin).to_dataset(name='diff_tmax_tmin')\n",
    "            \n",
    "            if region_name == 'china':\n",
    "                '''There is some error in the data when downloading from new source'''\n",
    "                diff_temp['diff_tmax_tmin'][:,:,-1,:,:] = diff_temp['diff_tmax_tmin'][:,:,-2,:,:]\n",
    "            \n",
    "            diff_temp.to_netcdf(save_file)\n",
    "    return(0)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e074531-409b-4f90-9bf6-a8c5bf49193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reforecast_input == 'GEFSv12':\n",
    "    create_diff_temp_reforecast(region_name)\n",
    "elif reforecast_input == 'ECMWF':\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cc7d0c9-4836-49ec-808d-8a50dcd58aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Loading files for variable tcw\n",
      "We are changing the coordinates of CONUS to match similar format as GLEAM\n",
      "\n",
      "Concatenating files for tcw. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for tcw_ECMWF: 50688\n",
      "Number of 0 values after min max scaling tcw_ECMWF: 0\n",
      "Loaded files for tcw\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for tcw_ECMWF. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable tcw_ECMWF is 35.30778503417969 for lead 6.\n",
      "\n",
      "Minimum value for variable tcw_ECMWF is -34.50571823120117 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tcw_ECMWF is 69.81350326538086.\n",
      "\n",
      "Standardizing data with min max for tcw_ECMWF. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tcw_ECMWF is 36.11422348022461 for lead 13.\n",
      "\n",
      "Minimum value for variable tcw_ECMWF is -38.562889099121094 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tcw_ECMWF is 74.6771125793457.\n",
      "\n",
      "Standardizing data with min max for tcw_ECMWF. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tcw_ECMWF is 35.5618782043457 for lead 20.\n",
      "\n",
      "Minimum value for variable tcw_ECMWF is -36.04944610595703 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tcw_ECMWF is 71.61132431030273.\n",
      "\n",
      "Standardizing data with min max for tcw_ECMWF. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tcw_ECMWF is 38.16874694824219 for lead 27.\n",
      "\n",
      "Minimum value for variable tcw_ECMWF is -32.25213623046875 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tcw_ECMWF is 70.42088317871094.\n",
      "\n",
      "Standardizing data with min max for tcw_ECMWF. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tcw_ECMWF is 38.43235397338867 for lead 34.\n",
      "\n",
      "Minimum value for variable tcw_ECMWF is -33.396575927734375 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tcw_ECMWF is 71.82892990112305.\n",
      "\n",
      "Standardizing data with min max for tcw_ECMWF. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tcw_ECMWF: 0\n",
      "Number of 0 values after min max scaling tcw_ECMWF: 5\n",
      "\n",
      "Stacking together REFORECAST variable tcw_ECMWF\n",
      "Loading file into memory. This takes a while.\n",
      "Saved tcw_ECMWF data into Data/model_npy_inputs/CONUS/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "rerun_function = True\n",
    "plot_distribution=False\n",
    "\n",
    "for var_name in var_list_ref[-1:]:\n",
    "    if var_name == 'soilw_bgrnd':\n",
    "        obs_min_max = obs_RZSM_minmax\n",
    "    else:\n",
    "        obs_min_max = None\n",
    "    anomaly_plot_standardize_save_reforecast(obs_source= reforecast_input, region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                  val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                  obs_min_max = obs_min_max, rerun_function=rerun_function, init_date_list = init_date_list,fcst_dir=fcst_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985201bf-8dd8-4523-afd4-0e8fdc1b0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_function = True\n",
    "\n",
    "for var_name in ['soilw_bgrnd']:\n",
    "    if var_name == 'obs_RZSM_minmax':\n",
    "        obs_min_max = obs_RZSM_minmax\n",
    "    else:\n",
    "        obs_min_max = None\n",
    "    anomaly_plot_standardize_save_reforecast_only_GEFSv12(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                  val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                  obs_min_max = obs_min_max, rerun_function=rerun_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0621d8-1972-4f6d-8e58-fb3b2e98e1bf",
   "metadata": {},
   "source": [
    "# Double check the units with precipitable water and surface specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b37468-bc7d-4af9-8359-45ef70a7e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precipitable water units\n",
    "#Reforecast (kg m-2, i.e., mm)   Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations (kg m-2)    Source; https://codes.ecmwf.int/grib/param-db/136 \n",
    "\n",
    "#check the scale of the data for pwat and spfh between forecasts and observations\n",
    "pwat_ref = xr.open_dataset('Data/GEFSv12_reforecast/pwat_eatm/precipitable_water_EMC_2000-01-05.nc4')\n",
    "pwat_obs = xr.open_dataset('Data/ERA5/reformat_to_reforecast_shape/CONUS/pwat_eatm/pwat_eatm_reformat_2000-01-05.nc4')\n",
    "\n",
    "pwat_ref.max()\n",
    "pwat_obs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33a018-0b72-4626-874d-ea50580df62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific humidity units\n",
    "#Reforecast (kg kg-1 dry air) Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations ('kg/kg'). I computed this using Metpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136936-2c05-4788-9d93-1645f74b7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check stacked files\n",
    "import pickle\n",
    "with open(f'/glade/work/klesinger/FD_RZSM_deep_learning/Data/model_npy_inputs/CONUS/Model_input_data/reforecast_tmax_2m_GEFSv12_training.pickle', 'rb') as handle:\n",
    "    a= pickle.load(handle)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745becd8-5f48-4432-8a19-cefb688ab4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if verification data is fine\n",
    "# output_npy_dir = f'Data/model_npy_inputs/{}Verification_data'\n",
    "# from glob import glob\n",
    "\n",
    "# for file in glob(f'{output_npy_dir}/OBS*'):\n",
    "#     print(file)\n",
    "#     print(np.load(file))\n",
    "\n",
    "##### The Data loads fine and fast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
