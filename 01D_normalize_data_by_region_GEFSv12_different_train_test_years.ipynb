{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb79f-b472-4171-b0ab-30c1b6585412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from metpy.units import units\n",
    "import preprocessUtils as putils\n",
    "from typing import Tuple\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b451f-7893-4304-813a-eca432631ab6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3ff845-71d9-422d-9e33-09a3af5be48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nGEFSv12 reforecast appears to have some missing values for some init dates. \\nWe are replacing those with the mean because this is messing up min/max scaling\\nvery badly\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.) Needs about 50GB RAM for CONUS (shape of file is 48 x 96 lat/lon)\n",
    "\n",
    "Select a specific region (either CONUS, NH (Northern Hemisphere), or australia)\n",
    "\n",
    "Takes about 4 hours to fully complete everything.\n",
    "'''\n",
    "\n",
    "''' \n",
    "GEFSv12 reforecast appears to have some missing values for some init dates. \n",
    "We are replacing those with the mean because this is messing up min/max scaling\n",
    "very badly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44223dd6-1a0f-4c8d-a95b-cdf86d30fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run_OBS_reformat_to_SubX = True #For reformatting the observations to the same lead-time format as GEFSv12\n",
    "plot_distribution = False #This set of functions takes a long time to plot. But if you want to see the anomaly and min-max distribution, select True\n",
    "\n",
    "'''Daily lags for observations -  we can specify up to 12 weeks in lagged RZSM data (these are 7-day increments). \n",
    "You can select whatever values you want (as long as they are before the reforecast period'''\n",
    "daily_lags = putils.make_daily_lags()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d845cd-a1b4-454a-b548-c9ff8577e2a8",
   "metadata": {},
   "source": [
    "# Data directory locations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705e8a43-188b-46c5-a45f-02a2637c28c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n"
     ]
    }
   ],
   "source": [
    "region_name = 'australia' #or ['australia', 'CONUS', 'china']\n",
    "\n",
    "reforecast_input = 'GEFSv12'  #['GEFSv12', 'ECMWF']\n",
    "\n",
    "start_date = '1999-01-01' #for reanalysis data. Select earlier dates than the forecasts to allow for smoothing and because we need lagged observations\n",
    "end_date = '2020-03-01' #for reanalysis data\n",
    "\n",
    "\n",
    "global verification_var\n",
    "verification_var = 'soilw_bgrnd_GLEAM' #this is for what we are verifying with DL outputs\n",
    "\n",
    "#Gleam observations\n",
    "gleam_dir = 'Data/GLEAM'\n",
    "\n",
    "#Forecast predictions\n",
    "if reforecast_input == 'GEFSv12':\n",
    "    gefsv12_fcst_dir = 'Data/GEFSv12_reforecast'\n",
    "\n",
    "#ERA5 observations\n",
    "era5_dir = 'Data/ERA5'\n",
    "\n",
    "if region_name != 'CONUS':\n",
    "    #Gleam observations\n",
    "    gleam_dir = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "    #Forecast predictions\n",
    "    gefsv12_fcst_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "    \n",
    "    #ERA5 observations\n",
    "    era5_dir = f'Data_{region_name}/ERA5'\n",
    "\n",
    "\n",
    "#Set years for training, validation, and testing\n",
    "train_start, train_end = 2000,2010\n",
    "train_start2, train_end2 = 2013,2017\n",
    "\n",
    "val_start, val_end = 2018, 2019\n",
    "\n",
    "test_start,test_end = 2011,2012\n",
    "\n",
    "\n",
    "spfh_unit = 'kg/kg' #Make sure that our reforecast and observation have the same unit for specific humidity\n",
    "\n",
    "global mask\n",
    "mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "#Get initialized dates from GEFSv12 to convert observations for easier stacking\n",
    "global init_date_list\n",
    "\n",
    "init_date_list = putils.get_init_date_list(forecast_variable_path=f'{gefsv12_fcst_dir}/soilw_bgrnd')\n",
    "init_date_list_datetime = [pd.to_datetime(i) for i in init_date_list]\n",
    "\n",
    "global var_list\n",
    "var_list = ['soilw_bgrnd', 'tmax_2m', 'spfh_2m', 'pwat_eatm','diff_temp_2m','hgt_pres'] #Observation variables to pre-process\n",
    "\n",
    "global lead_select\n",
    "lead_select = [6,13,20,27,34] #For subsetting data by leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023cfa8-d5b4-4bc2-8031-d4d71d939e39",
   "metadata": {},
   "source": [
    "# Load Observation Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cc6fa0-f9ae-4c0a-89db-5bbb73ddff50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading observations. Applying a 7-day rolling mean between 1999-01-01 and 2020-03-01 for australia Region.\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "Number of np.nan values before rolling mean for GLEAM_RZSM_obs: 15284187\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "Number of np.nan values before rolling mean for ERA_precipitable_water_obs: 0\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "Number of np.nan values before rolling mean for ERA_geopotential_z200_obs: 0\n",
      "Creating tmax and difference in tmax and tmin variables.\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "Number of np.nan values before rolling mean for ERA_Tmax_obs: 0\n",
      "Number of np.nan values before rolling mean for ERA_Tmin_obs: 0\n",
      "\n",
      "Loading the previously created specific humidity from Data_australia/ERA5\n",
      "Loaded all observations.\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading observations. Applying a 7-day rolling mean between {start_date} and {end_date} for {region_name} Region.')\n",
    "\n",
    "RZSM_obs_GLEAM,proper_date_time = putils.open_reanalysis_files_and_preprocess_rolling_mean_RZSM_only(path_to_file=f'{gleam_dir}/RZSM_weighted_mean_0_100cm.nc4',\n",
    "                                                             file_variable='GLEAM_RZSM_obs',start_date=start_date,end_date=end_date,region_name = region_name)\n",
    "\n",
    "precipitable_water_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/total_column_water_merged.nc4',\n",
    "                                                             file_variable='ERA_precipitable_water_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "geopotential_z200_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/geopotential_merged.nc4',\n",
    "                                                             file_variable='ERA_geopotential_z200_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "\n",
    "max_temp_ERA, diff_temp_ERA = putils.open_temperature_files_and_preprocess_rolling_mean(path_to_tmax =f'{era5_dir}/maximum_2m_temperature_merged.nc4', \n",
    "                                                                                 path_to_tmin =f'{era5_dir}/minimum_2m_temperature_merged.nc4',\n",
    "                                                                                 file_variable_tmax = 'ERA_Tmax_obs',file_variable_tmin = 'ERA_Tmin_obs',\n",
    "                                                                                 start_date=start_date, end_date=end_date, \n",
    "                                                                                 region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "spfh_ERA = putils.calculate_2m_specific_humidity_and_preprocess(path_to_dewpoint=f'{era5_dir}/2m_dewpoint_temperature_merged.nc4', \n",
    "                                              path_to_pressure=f'{era5_dir}/surface_pressure_merged.nc4', \n",
    "                                              file_variable_dewpoint='ERA_2m_dewpoint_obs', file_variable_pressure='ERA_surface_pressure_obs', \n",
    "                                              start_date=start_date, end_date=end_date, \n",
    "                                              region_name = region_name, proper_date_time=proper_date_time,\n",
    "                                              obs_dir = era5_dir, spfh_unit = spfh_unit)\n",
    "\n",
    "print('Loaded all observations.')\n",
    "\n",
    "#Print saving the anomaly for each region as the standard format\n",
    "climatology_season = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.year'] <= train_end)).groupby(f\"time.season\").mean()\n",
    "\n",
    "summer_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'JJA')) - climatology_season.sel(season='JJA')\n",
    "fall_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'SON')) - climatology_season.sel(season='SON')\n",
    "winter_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'DJF')) - climatology_season.sel(season='DJF')\n",
    "spring_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'MAM')) - climatology_season.sel(season='MAM')\n",
    "\n",
    "combined_files = xr.concat([summer_,fall_,winter_,spring_],dim='time').sortby('time')\n",
    "combined_files=combined_files.astype(np.float32)\n",
    "\n",
    "#Now save anomaly for later use\n",
    "if region_name == 'CONUS':\n",
    "    location = 'Data/GLEAM'\n",
    "else:\n",
    "    location = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "combined_files.to_netcdf(f'{location}/RZSM_anomaly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af54dc1-ab3a-4d10-afa1-9b81b016884d",
   "metadata": {},
   "source": [
    "# Now restructure the observation data to the same lead-time format as GEFSv12. This will allow for an easier computation of anomalies using climpred functions.\n",
    "\n",
    "### New indexes will be from range (-84 days to -1 day and then the typical 7-day lead times starting from 0 to 34). Because we have applied a 7-day rolling mean to observations so far, even index -1 for the day has the data from the previous 7-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35812da6-4e93-4db3-b0d2-6c23269410bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_info_for_processing(var: str) -> Tuple[str, xr.DataArray]:\n",
    "    #Names to save the data so that it is in the exact same format as the Reforecasts\n",
    "    \n",
    "    if var == 'soilw_bgrnd':\n",
    "        # var_name = var #var_name is the actual name of the variable in the xarray file\n",
    "        model_name = 'GLEAM' #source of the data\n",
    "        obs_file = RZSM_obs_GLEAM #the actual data file\n",
    "    elif var == 'tmax_2m':\n",
    "        # var_name = 'tasmax'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = max_temp_ERA\n",
    "    elif var == 'spfh_2m':\n",
    "        # var_name = 'huss'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = spfh_ERA\n",
    "    elif var == 'pwat_eatm':\n",
    "        # var_name = 'precipitable_water'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = precipitable_water_ERA\n",
    "    elif var == 'diff_temp_2m':\n",
    "        # var_name = 'diff_tmax_tmin'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = diff_temp_ERA\n",
    "    elif var == 'hgt_pres':\n",
    "        # var_name = 'z'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = geopotential_z200_ERA\n",
    "    \n",
    "    output_OBSERVATIONS_reformat_dir = f'Data/{model_name}/reformat_to_reforecast_shape/{region_name}' #Directory to save \n",
    "    save_dir = f'{output_OBSERVATIONS_reformat_dir}/{var}'\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    return(model_name,obs_file,output_OBSERVATIONS_reformat_dir,save_dir)\n",
    "\n",
    "\n",
    "\n",
    "def convert_OBS_to_SubX_format(_date: str) -> None:  \n",
    "# for _date in init_date_list:\n",
    "    # var='RZSM_weighted'\n",
    "    # _date=init_date_list[0]\n",
    "\n",
    "    \n",
    "    '''We are going to create new leads that are different than reforecast. The reasoning for this is that we want the actual weekly lags (and 1 day lag) and this will\n",
    "    assist with future predictions within the deep learning model'''\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        #Currently the GLEAM and ERA observations are in a format with only positive values for the longitude.\n",
    "        #The current reforecast has negative values for those West of 0 degrees\n",
    "        \n",
    "        new_X_coords = [i+360 if i < 0 else i for i in template.X.values]\n",
    "        single_file = template.assign_coords({'X':new_X_coords})\n",
    "        single_file = xr.zeros_like(single_file)\n",
    "        open_date_SubX = putils.restrict_to_bounding_box(single_file,mask)\n",
    "        \n",
    "    elif region_name != 'CONUS':\n",
    "        open_date_SubX = template\n",
    "        open_date_SubX = xr.zeros_like(open_date_SubX)\n",
    "    \n",
    "    for var in var_list:\n",
    "        # break\n",
    "        model_name, obs_file, output_OBSERVATIONS_reformat_dir, save_dir = return_info_for_processing(var)     \n",
    "\n",
    "        obs_file_name = f'{var}_reformat_{reforecast_input}_{_date}.nc4'\n",
    "        save_file = f'{save_dir}/{obs_file_name}'\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            # print(f'Completed date {_date}')\n",
    "            pass\n",
    "        else:\n",
    "            print(f'Working on variable {var} for date {_date}')\n",
    "            out_file = open_date_SubX.copy(deep=True)\n",
    "    \n",
    "            '''We are going to create a new lead day that represents the previous day before the forecast was initialized\n",
    "            #New shape will be (1x11x48xlatxlon)\n",
    "            This will include the day lag 1, and weekly lags 1-12'''\n",
    "            \n",
    "            file_shape = out_file[putils.xarray_varname(out_file)].shape\n",
    "            \n",
    "            named_str_leads = [str(i) for i in np.arange(-1,open_date_SubX.L.shape[0])]\n",
    "            new_shape = np.empty(shape=(1,file_shape[1],file_shape[2]+13,file_shape[3],file_shape[4]))\n",
    "            new_shape.shape\n",
    "            \n",
    "            daily_lead_lags = daily_lags + list(out_file.L.values[:])\n",
    "            \n",
    "            new_lead_days = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    test = (['S','M','L','Y','X'], new_shape[:,:,:,:,:]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    S = np.atleast_1d(_date),\n",
    "                    X = open_date_SubX.X.values,\n",
    "                    Y = open_date_SubX.Y.values,\n",
    "                    L = daily_lead_lags,\n",
    "                    M = open_date_SubX.M.values,\n",
    "    \n",
    "                ),\n",
    "                attrs = dict(\n",
    "                    Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                    cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "            )   \n",
    "            \n",
    "            #Create a file to overwrite\n",
    "            out_file = xr.zeros_like(new_lead_days)\n",
    "    \n",
    "            print(f'Working on initialized day {_date} to find values integrating with SubX models, leads, & coordinates and saving data into {save_dir}.')\n",
    "            \n",
    "            for idx,i_lead in enumerate(new_lead_days.L.values):\n",
    "                # break\n",
    "    \n",
    "                date_val = pd.to_datetime(pd.to_datetime(_date) + dt.timedelta(days=int(i_lead)+0)) #Adding +1 may be suitable for other forecasts which predict the next day. But GEFSv12 predicts lead 0 as 12 UTC on the same date it is initialized\n",
    "                #But be careful if you adapt this code to a new script. We are looking backwards in time from the first date.\n",
    "                    \n",
    "                date_val = f'{date_val.year}-{date_val.month:02}-{date_val.day:02}'\n",
    "    \n",
    "                out_file[putils.xarray_varname(out_file)][0,:, idx, :, :] = \\\n",
    "                    obs_file[putils.xarray_varname(obs_file)].sel(time = date_val).values\n",
    "    \n",
    "    \n",
    "            if var == 'soilw_bgrnd':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        RZSM = (['S','M','L','Y','X'],    out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )                    \n",
    "            elif var == 'tmax_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        tmax = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'spfh_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        spfh_2m = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'pwat_eatm':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        pwat = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )      \n",
    "            elif var == 'diff_temp_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        diff_tmax_tmin = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            elif var == 'hgt_pres':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        z200 = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            var_OUT = var_OUT.astype(np.float32) #all the files need to be in float32 for the deep learning algorithm to work best\n",
    "            \n",
    "            #Save as a netcdf for later processing\n",
    "            var_OUT.to_netcdf(path = save_file, mode ='w')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "#template for overwriting\n",
    "ref_dir = f'{gefsv12_fcst_dir}/soilw_bgrnd' #Just use a single reference directory to serve as the template for file creation\n",
    "\n",
    "global template\n",
    "#Grab a single SubX to use as the template. Doesn't matter if it is the same variable or not or the same date\n",
    "\n",
    "template = sorted(glob(f'{ref_dir}/*.n*'))[0]\n",
    "template = xr.open_dataset(template)\n",
    "\n",
    "# init_date_list.reverse()\n",
    "for _date  in init_date_list:\n",
    "    convert_OBS_to_SubX_format(_date)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p = Pool(5)\n",
    "#     p.map(convert_OBS_to_SubX_format,init_date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0b05a-0cad-435a-b67b-d5b08427c9a8",
   "metadata": {},
   "source": [
    "# Now process all the observations including:\n",
    "### 1.) Create anomaly\n",
    "### 2.) Plot distribution after anomaly\n",
    "### 3.) Create min-max standardization\n",
    "### 4.) Plot distribution after min-max standardization\n",
    "### 5.) Stack verification file (for RZSM) into a seperate directory (as a pickle file)\n",
    "### 6.) Stack all the different lags for all variables into seperate directory (as a pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6618f5-cc10-483d-a630-ddc197f251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_observations_only(obs_source: str, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                    return_obs_minmax: bool, gefsv12_fcst_dir: str, rerun_function: bool) -> None:\n",
    "\n",
    "\n",
    "    #Args for creating anomaly (testing)\n",
    "    # obs_source = 'GLEAM'\n",
    "    # region_name = region_name\n",
    "    # var_name= 'soilw_bgrnd'\n",
    "    # obs_or_forecast='obs' \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "        #7-day rolling mean has already been applied to observations\n",
    "\n",
    "        #Create the seasonal mean using climpred functions\n",
    "        file_locations = f'Data/{obs_source}/reformat_to_reforecast_shape/{region_name}/{var_name}/*{reforecast_input}*.n*'\n",
    "\n",
    "        print(f'\\nRetrieving data from {file_locations}\\n')\n",
    "\n",
    "        file = xr.open_mfdataset(file_locations)\n",
    "        #Fix dates\n",
    "        dates_new = [pd.to_datetime(i) for i in file.S.values]\n",
    "        file = file.assign_coords({'S':dates_new})\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly_with_different_testing_years(file,train_start, train_end, train_start2, train_end2)\n",
    "        \n",
    "        print(f'\\nCreated seasonal anomalies on all {anom.L.values} lead days')\n",
    "        \n",
    "        print('\\n7-day rolling mean not being applied. We already did this when creating the data.')\n",
    "\n",
    "        anom = anom.load()\n",
    "        \n",
    "        anom_mean_subset = anom.sel(L=lead_select) #Make a small subset of the data\n",
    "\n",
    "\n",
    "        if var_source_combined == verification_var:\n",
    "            #Save the anomaly files\n",
    "            print(f'Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads {anom_mean_subset.L.values}')\n",
    "            putils.convert_OBS_anomaly_to_SubX_format(init_date_list = init_date_list, region_name = region_name, anomaly_file = anom_mean_subset, fcst_dir = gefsv12_fcst_dir, lead_select = lead_select)\n",
    "            \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('\\nLooking at what the first and last dates are within the anomaly file.')\n",
    "        print_dates(anom_mean_subset)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('\\nPlotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader_with_different_testing_years(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'anomaly', region_name = region_name,\n",
    "                                                       lead_select=lead_select,test_start = test_start, test_end = test_end)\n",
    "    \n",
    "        #Min max standardize\n",
    "        min_max = putils.choose_training_years_and_min_max_scale_with_different_testing_years(file = anom, train_start = train_start, train_start2 = train_start2, train_end2 = train_end2, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, obs_min_max = None,\n",
    "                                                                lead_select = lead_select, val_start = val_start, val_end = val_end, test_start = test_start, test_end= test_end)\n",
    "        \n",
    "        #Plot min max distribution\n",
    "        if plot_distribution:\n",
    "            print('\\nPlotting min max distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader_with_different_testing_years(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "                                                       lead_select=lead_select,test_start = test_start, test_end = test_end)\n",
    "    \n",
    "        if var_source_combined == verification_var:\n",
    "            #We only need to save this as the verification file for deep learning model\n",
    "            putils.create_stacked_files_by_lead_for_verification_with_different_testing_years(file = min_max, train_end=train_end, val_end=val_end, test_start=test_start, variable=var_source_combined,\n",
    "                                                          obs_or_forecast=obs_or_forecast, region_name = region_name, init_date_list = init_date_list,\n",
    "                                                                lead_select=lead_select, train_start = train_start, train_start2 = train_start2, train_end2 = train_end2,\n",
    "                                                                                             val_start = val_start, test_end = test_end)\n",
    "    \n",
    "        #Now create stacked files for each individual variable for each lead time\n",
    "        putils.retrieve_stacked_files_and_save_for_model_inputs_with_different_testing_years(file = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                                                val_end = val_end, test_start = test_start, init_date_list = init_date_list,test_end = test_end,\n",
    "                                                                                             train_start = train_start, train_start2 = train_start2, train_end2 = train_end2,\n",
    "                                                                                             val_start = val_start)\n",
    "\n",
    "    if return_obs_minmax == False:\n",
    "        return(f'Completed variable {var_source_combined}')\n",
    "    else:\n",
    "        return(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d01fdc-5a08-4b64-9803-f2c3b3203dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from Data/GLEAM/reformat_to_reforecast_shape/australia/soilw_bgrnd/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads [ 6 13 20 27 34]\n",
      "\n",
      "Adding data within function convert_OBS_anomaly_to_SubX_format to file for leads [6, 13, 20, 27, 34]\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for soilw_bgrnd_GLEAM. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.23945410549640656 for lead -84.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18885290622711182 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.42830701172351837.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24792860448360443 for lead -77.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.20016467571258545 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4480932801961899.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.250914067029953 for lead -70.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1983318328857422 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4492458999156952.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.256581574678421 for lead -63.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18998201191425323 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.44656358659267426.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.250895231962204 for lead -56.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19335032999515533 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4442455619573593.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24600043892860413 for lead -49.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19445954263210297 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4404599815607071.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24668961763381958 for lead -42.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18472671508789062 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4314163327217102.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2505556643009186 for lead -35.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.16481350362300873 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4153691679239273.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2532012462615967 for lead -28.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.16735291481018066 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.42055416107177734.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25487104058265686 for lead -21.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.16489958763122559 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.41977062821388245.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2567538022994995 for lead -14.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.16207075119018555 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.41882455348968506.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.23639599978923798 for lead -7.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.17375563085079193 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4101516306400299.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2442563772201538 for lead -1.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.17556092143058777 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4198172986507416.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24006704986095428 for lead 6.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1872882843017578 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4273553341627121.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24888306856155396 for lead 13.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19677096605300903 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.445654034614563.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2505459189414978 for lead 20.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19964787364006042 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4501937925815582.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25856536626815796 for lead 27.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18778258562088013 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4463479518890381.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25186148285865784 for lead 34.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19338491559028625 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4452463984489441.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GLEAM: 0\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GLEAM: 408278475\n",
      "\n",
      "Working on file soilw_bgrnd_GLEAM to save as verification observation data. This takes a while because data is not loaded into memory\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on validation data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on testing data for soilw_bgrnd_GLEAM\n",
      "Saved soilw_bgrnd_GLEAM data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/australia/tmax_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for tmax_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 17.135467529296875 for lead -84.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -16.813079833984375 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 33.94854736328125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 16.267578125 for lead -77.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.441741943359375 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 33.709320068359375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 15.465484619140625 for lead -70.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.892181396484375 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 33.357666015625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 15.473175048828125 for lead -63.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -15.835784912109375 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 31.3089599609375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.671356201171875 for lead -56.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -15.44769287109375 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 30.119049072265625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.57196044921875 for lead -49.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -14.919158935546875 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 29.491119384765625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.090911865234375 for lead -42.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -14.966796875 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 29.057708740234375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.71990966796875 for lead -35.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.56890869140625 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 32.288818359375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.153656005859375 for lead -28.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -16.790374755859375 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 30.94403076171875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 14.0184326171875 for lead -21.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -16.037933349609375 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 30.056365966796875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 15.6363525390625 for lead -14.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -15.2579345703125 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 30.894287109375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 16.406463623046875 for lead -7.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -15.1510009765625 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 31.557464599609375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 15.88909912109375 for lead -1.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -16.5648193359375 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 32.45391845703125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 16.67242431640625 for lead 6.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.338775634765625 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 34.011199951171875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 16.683929443359375 for lead 13.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.99139404296875 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 34.675323486328125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 15.250213623046875 for lead 20.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -18.499298095703125 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 33.74951171875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 16.314239501953125 for lead 27.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.3170166015625 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 33.631256103515625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 13.213775634765625 for lead 34.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -17.062652587890625 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 30.27642822265625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tmax_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling tmax_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for tmax_2m_ERA5\n",
      "\n",
      "Working on validation data for tmax_2m_ERA5\n",
      "\n",
      "Working on testing data for tmax_2m_ERA5\n",
      "Saved tmax_2m_ERA5 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/australia/spfh_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for spfh_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011942079290747643 for lead -84.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.01105579175055027 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022997871041297913.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.012163538485765457 for lead -77.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010984079912304878 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023147618398070335.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011806460097432137 for lead -70.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011619752272963524 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02342621237039566.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011326621286571026 for lead -63.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.01214841939508915 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023475040681660175.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011495698243379593 for lead -56.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.01120089367032051 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022696591913700104.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011320292018353939 for lead -49.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011598890647292137 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022919182665646076.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011165311560034752 for lead -42.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010326961055397987 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02149227261543274.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011074699461460114 for lead -35.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009629590436816216 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02070428989827633.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011370941996574402 for lead -28.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.012125392444431782 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023496334441006184.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011634212918579578 for lead -21.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011520805768668652 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02315501868724823.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011921611614525318 for lead -14.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011478543281555176 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023400154896080494.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.012267599813640118 for lead -7.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010796448215842247 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023064048029482365.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011310695670545101 for lead -1.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011362763121724129 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02267345879226923.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011713768355548382 for lead 6.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011467191390693188 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02318095974624157.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011918248608708382 for lead 13.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011151344515383244 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023069593124091625.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01192329078912735 for lead 20.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011783122085034847 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023706412874162197.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011606842279434204 for lead 27.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.012312667444348335 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02391950972378254.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011225182563066483 for lead 34.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011732121929526329 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02295730449259281.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for spfh_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling spfh_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for spfh_2m_ERA5\n",
      "\n",
      "Working on validation data for spfh_2m_ERA5\n",
      "\n",
      "Working on testing data for spfh_2m_ERA5\n",
      "Saved spfh_2m_ERA5 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/australia/pwat_eatm/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for pwat_eatm_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 37.087799072265625 for lead -84.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.86963653564453 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 67.95743560791016.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 38.733482360839844 for lead -77.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.776630401611328 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 68.51011276245117.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.688812255859375 for lead -70.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.629348754882812 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 65.31816101074219.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.98920822143555 for lead -63.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.735689163208008 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 65.72489738464355.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.161224365234375 for lead -56.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.552318572998047 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 65.71354293823242.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.35671615600586 for lead -49.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -31.968517303466797 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 67.32523345947266.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.76197052001953 for lead -42.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -28.93422508239746 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 64.69619560241699.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 36.39231872558594 for lead -35.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -27.511062622070312 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 63.90338134765625.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 37.52537155151367 for lead -28.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.079734802246094 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 66.60510635375977.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 38.39978790283203 for lead -21.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -28.688018798828125 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 67.08780670166016.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 39.261940002441406 for lead -14.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.156494140625 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 68.4184341430664.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.506752014160156 for lead -7.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.54120445251465 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 65.0479564666748.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.95711898803711 for lead -1.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -33.311798095703125 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 69.26891708374023.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 37.35253143310547 for lead 6.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.51181411743164 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 69.86434555053711.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 38.99745178222656 for lead 13.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.628440856933594 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 71.62589263916016.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.314292907714844 for lead 20.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.82048797607422 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 68.13478088378906.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 36.70435333251953 for lead 27.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.31200408935547 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 69.016357421875.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.11934280395508 for lead 34.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -31.445363998413086 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 66.56470680236816.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for pwat_eatm_ERA5: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for pwat_eatm_ERA5\n",
      "\n",
      "Working on validation data for pwat_eatm_ERA5\n",
      "\n",
      "Working on testing data for pwat_eatm_ERA5\n",
      "Saved pwat_eatm_ERA5 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/australia/diff_temp_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for diff_temp_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.971641540527344 for lead -84.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.497930526733398 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.469572067260742.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.681589126586914 for lead -77.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.710176467895508 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.391765594482422.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.498591423034668 for lead -70.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.996448516845703 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.49503993988037.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.44469165802002 for lead -63.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.218633651733398 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.663325309753418.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.658793449401855 for lead -56.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.395257949829102 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.054051399230957.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.671499252319336 for lead -49.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.546991348266602 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.218490600585938.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.782727241516113 for lead -42.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.797550201416016 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 18.58027744293213.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 8.981436729431152 for lead -35.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.781871795654297 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 18.76330852508545.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.196017265319824 for lead -28.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.972921371459961 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.168938636779785.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 10.035754203796387 for lead -21.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.124395370483398 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.160149574279785.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.771201133728027 for lead -14.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.334476470947266 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.105677604675293.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.484511375427246 for lead -7.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.06817626953125 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 19.552687644958496.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.579290390014648 for lead -1.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.965513229370117 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.544803619384766.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.039044380187988 for lead 6.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.197229385375977 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.236273765563965.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.313837051391602 for lead 13.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.347381591796875 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 20.661218643188477.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.523298263549805 for lead 20.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.507753372192383 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 21.031051635742188.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.504704475402832 for lead 27.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.559467315673828 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 21.06417179107666.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 9.46697998046875 for lead 34.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.748291969299316 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 21.215271949768066.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling diff_temp_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for diff_temp_2m_ERA5\n",
      "\n",
      "Working on validation data for diff_temp_2m_ERA5\n",
      "\n",
      "Working on testing data for diff_temp_2m_ERA5\n",
      "Saved diff_temp_2m_ERA5 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/australia/hgt_pres/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for hgt_pres_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3807.8125 for lead -84.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3636.28125 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7444.09375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3745.8984375 for lead -77.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3983.2734375 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7729.171875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3946.7734375 for lead -70.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3732.8359375 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7679.609375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4205.078125 for lead -63.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3542.3203125 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7747.3984375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4413.9375 for lead -56.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3388.75 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7802.6875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4606.953125 for lead -49.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3300.890625 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7907.84375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4777.109375 for lead -42.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3251.640625 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 8028.75.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4917.859375 for lead -35.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3519.3203125 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 8437.1796875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3581.203125 for lead -28.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3348.328125 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 6929.53125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3771.2578125 for lead -21.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3648.3203125 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7419.578125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4004.328125 for lead -14.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3412.8828125 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7417.2109375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3630.875 for lead -7.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3646.5859375 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7277.4609375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3770.8828125 for lead -1.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3880.9921875 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7651.875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3798.6875 for lead 6.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3731.6796875 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7530.3671875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3677.90625 for lead 13.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3498.15625 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7176.0625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3917.8046875 for lead 20.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3745.2734375 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7663.078125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 3876.9765625 for lead 27.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3534.3515625 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7411.328125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4081.0390625 for lead 34.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -3597.5234375 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 7678.5625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for hgt_pres_ERA5: 0\n",
      "Number of 0 values after min max scaling hgt_pres_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for hgt_pres_ERA5\n",
      "\n",
      "Working on validation data for hgt_pres_ERA5\n",
      "\n",
      "Working on testing data for hgt_pres_ERA5\n",
      "Saved hgt_pres_ERA5 data into Data/model_npy_inputs/australia/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rerun_function = True\n",
    "\n",
    "\n",
    "################### OBSERVATIONS ###########################\n",
    "\n",
    "obs_RZSM_minmax = anomaly_plot_standardize_save_observations_only(obs_source= 'GLEAM', region_name = region_name, var_name='soilw_bgrnd', train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = True, gefsv12_fcst_dir = gefsv12_fcst_dir,\n",
    "                                                                 rerun_function = rerun_function)\n",
    "\n",
    "for var_name in var_list:\n",
    "    if 'soil' in var_name:\n",
    "        pass\n",
    "    else:\n",
    "        anomaly_plot_standardize_save_observations_only(obs_source= 'ERA5', region_name = region_name, var_name=var_name, train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                        obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = False, gefsv12_fcst_dir = gefsv12_fcst_dir,\n",
    "                                                       rerun_function = rerun_function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26ab6c-dbcf-4358-b755-6462445d54a3",
   "metadata": {},
   "source": [
    "# Now create datasets for GEFSV12 reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fddf36a-6343-405a-8079-b2387f36edfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "# print(f'\\nLoading files for variable {name_of_var}')\n",
    "\n",
    "# file_paths = sorted(glob(f'{dir_path}/{name_of_var}/*.n*'))\n",
    "\n",
    "# # Open individual xarray datasets from the files\n",
    "# datasets = [xr.open_dataset(file) for file in file_paths]\n",
    "\n",
    "# #We have different coordinate systems. So we need to add 360 to each of the X coordinates if they are negative\n",
    "# if region_name == 'CONUS':\n",
    "#     print(f'We are changing the coordinates of CONUS to match similar format as GLEAM')\n",
    "#     new_X_coords = [i+360 if i < 0 else i for i in datasets[0].X.values]\n",
    "#     #Add the new coordinates\n",
    "#     datasets = [file.assign_coords({'X':new_X_coords}) for file in datasets]\n",
    "#     datasets = [putils.restrict_to_bounding_box(file,mask) for file in datasets]\n",
    "    \n",
    "# # Concatenate the datasets along a specific dimension\n",
    "# print(f'\\nConcatenating files for {name_of_var}. Takes about 10 minutes for CONUS.')\n",
    "# file = xr.concat(datasets, dim='S', combine_attrs=\"override\")\n",
    "\n",
    "# putils._count_zero_values(file, f'{name_of_var}_GEFSv12')\n",
    "# # file = dask.delayed(xr.concat)(datasets, dim=\"S\")\n",
    "\n",
    "# # Close individual datasets to free up resources\n",
    "# for dataset in datasets:\n",
    "#     dataset.close()\n",
    "\n",
    "# # Some files have a value of zero which is skewing results. So we need to replace with the mean.\n",
    "# print('Masking files if they have value of zero')\n",
    "# file_masked = file.where(file == 0, np.nan, file)\n",
    "# #Now take the mean \n",
    "# file_masked_mean = file_masked.mean(dim='S')\n",
    "# #Now replace the values where there was a zero\n",
    "# file = xr.where(np.isnan(file), file_masked_mean, file)\n",
    "\n",
    "# _count_np_nan_values(file, f'{name_of_var}_GEFSv12')\n",
    "# _count_zero_values(file, f'{name_of_var}_GEFSv12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad12ffb-6b77-4e23-9970-c7729c4c52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_reforecast(obs_source, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                  obs_min_max: xr.DataArray, rerun_function: bool) -> None:\n",
    "    # #Testing\n",
    "    # obs_source= 'GEFSv12'\n",
    "    # region_name = region_name\n",
    "    # var_name='tmax_2m'\n",
    "    # train_end = train_end\n",
    "    # val_end = val_end\n",
    "    # test_start = test_start\n",
    "    # obs_or_forecast = 'reforecast'\n",
    "\n",
    "    \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "\n",
    "        # file = putils.return_GEFSv12_reforecast_files(dir_path = gefsv12_fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "        file = putils.return_reforecast_files_by_concatenation(dir_path = gefsv12_fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "\n",
    "        if var_name == 'hgt_pres':\n",
    "            file = file* 9.80665 #must place in the same unit as ERA5. the unit is gravity\n",
    "\n",
    "        #Testing if rolling mean first fixes anything. Currenlty the hgt_pres values are way too low for the min\n",
    "\n",
    "        file_test_full = file.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        file_test_small = file_test_full.sel(L=lead_select)\n",
    "        \n",
    "        print(f'\\nCreating seasonal anomalies on all {file_test_small.L.values} lead days')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly_with_different_testing_years(file_test_small,train_start, train_end, train_start2, train_end2)\n",
    "        \n",
    "        anom_full, mean_season_full = putils.create_seasonal_anomaly_with_different_testing_years(file_test_full,train_start, train_end, train_start2, train_end2)\n",
    "        # print('\\nNow taking the 7-day rolling mean of anomalies. We are not replacing lead day 0.')\n",
    "        # anom_mean = anom.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        # anom_mean[putils.xarray_varname(file)][:,:,0,:,:] = anom[putils.xarray_varname(anom)][:,:,0,:,:]\n",
    "        \n",
    "        anom = anom.load()\n",
    "        \n",
    "        print(f'\\nShape of anomaly mean = {anom[putils.xarray_varname(anom)].shape}')\n",
    "        \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('Looking at what dates are within the file')\n",
    "        print_dates(anom)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('Plotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader_with_different_testing_years(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, \n",
    "                                                        anomaly_or_min_max = 'anomaly', region_name = region_name,lead_select=lead_select,test_start = test_start, test_end = test_end)\n",
    "\n",
    "\n",
    "    \n",
    "        #Min max standardize\n",
    "        if 'soilw_bgrnd' in var_source_combined:\n",
    "            putils.save_baseline_RZSM_anomaly(anom_full.load(),region_name,gefsv12_fcst_dir)\n",
    "            \n",
    "            min_max = putils.choose_training_years_and_min_max_scale_with_different_testing_years(file = anom, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = obs_min_max, lead_select = lead_select, train_start = train_start, train_start2 = train_start2, train_end2 = train_end2, train_end = train_end, val_start = val_start, val_end = val_end, test_start = test_start, test_end= test_end)\n",
    "\n",
    " \n",
    "        else:\n",
    "            min_max = putils.choose_training_years_and_min_max_scale_with_different_testing_years(file = anom, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = None, lead_select = lead_select, train_start = train_start, train_start2 = train_start2, train_end2 = train_end2, \n",
    "                                                                                                  train_end = train_end, val_start = val_start, val_end = val_end, test_start = test_start, test_end= test_end)\n",
    "\n",
    "        \n",
    "        \n",
    "        #Plot min max distribution\n",
    "        if plot_distribution:\n",
    "            print('Plotting min min distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader_with_different_testing_years(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, \n",
    "                                                        anomaly_or_min_max = 'min_max', region_name = region_name,lead_select=lead_select,test_start = test_start, test_end = test_end)\n",
    "\n",
    "\n",
    "        putils.stack_reforecasts_with_different_testing_years(var_min_max = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                 val_end = val_end, test_start = test_start, lead_select=lead_select, init_date_list = init_date_list, train_start =train_start,\n",
    "                                                             train_start2 = train_start2, train_end2 = train_end2, test_end = test_end, val_start = val_start)\n",
    "\n",
    "    return(f'Completed variable {var_source_combined}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12858852-e5c8-43ed-bcef-71de866056a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load tmax and tmin and then create diff_temp_2m (difference in temperature)\n",
    "def create_diff_temp_reforecast(region_name: str) -> None:\n",
    "    if region_name == 'CONUS':\n",
    "        data_dir = f'Data/GEFSv12_reforecast'\n",
    "    else:\n",
    "        data_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "\n",
    "    save_dir = f'{data_dir}/diff_temp_2m'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    tmax_ = sorted(glob(f'{data_dir}/tmax_2m/*.n*'))\n",
    "    tmin_ = sorted(glob(f'{data_dir}/tmin_2m/*.n*'))\n",
    "\n",
    "    for idx, file in enumerate(tmax_):\n",
    "        # break\n",
    "        max_ = xr.open_dataset(file)\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        \n",
    "        date_ = pd.to_datetime(max_.S.values[0])\n",
    "        save_file = f'{save_dir}/diff_temp_2m_{date_.year}-{date_.month:02}-{date_.day:02}.nc4'\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            pass\n",
    "        else:\n",
    "            min_ = xr.open_dataset(tmin_[idx])\n",
    "            # print(f'Min: {min_[putils.xarray_varname(min_)].min().values}')\n",
    "            assert max_.S.values == min_.S.values, 'Dates are not equal, something is wrong with either glob or there are missing files from either tmax or tmin'\n",
    "    \n",
    "            diff_temp = (max_.tasmax - min_.tasmin).to_dataset(name='diff_tmax_tmin')\n",
    "            \n",
    "            if region_name == 'china':\n",
    "                '''There is some error in the data when downloading from new source'''\n",
    "                diff_temp['diff_tmax_tmin'][:,:,-1,:,:] = diff_temp['diff_tmax_tmin'][:,:,-2,:,:]\n",
    "            \n",
    "            diff_temp.to_netcdf(save_file)\n",
    "    return(0)\n",
    "\n",
    "create_diff_temp_reforecast(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc7d0c9-4836-49ec-808d-8a50dcd58aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable soilw_bgrnd\n",
      "\n",
      "Concatenating files for soilw_bgrnd. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GEFSv12: 460800\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GEFSv12: 0\n",
      "Loaded files for soilw_bgrnd\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for soilw_bgrnd_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.2771676182746887 for lead 6.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.2001960575580597 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.4773636758327484.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.28082871437072754 for lead 13.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.20449015498161316 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.4853188693523407.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.3064708113670349 for lead 20.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.2067367136478424 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.5132075250148773.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.29171332716941833 for lead 27.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.21090614795684814 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.5026194751262665.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.2883948087692261 for lead 34.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.20837801694869995 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.496772825717926.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GEFSv12: 0\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GEFSv12: 113452761\n",
      "\n",
      "Stacking together REFORECAST variable soilw_bgrnd_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved soilw_bgrnd_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable spfh_2m\n",
      "\n",
      "Concatenating files for spfh_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for spfh_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling spfh_2m_GEFSv12: 3009024\n",
      "Loaded files for spfh_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for spfh_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.010381998523272769 for lead 6.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.009677275164984804 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.02005927368825757.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.010735490368587735 for lead 13.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.012640298465292655 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.02337578883388039.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.010961668116444707 for lead 20.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.010382945175268455 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.021344613291713162.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.010626913262556388 for lead 27.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.010590690156287556 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.021217603418843944.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.012478277080430136 for lead 34.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.01946050829826318 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.03193878537869332.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for spfh_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling spfh_2m_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable spfh_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved spfh_2m_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable tmax_2m\n",
      "\n",
      "Concatenating files for tmax_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for tmax_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling tmax_2m_GEFSv12: 0\n",
      "Loaded files for tmax_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for tmax_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 18.351249328076108 for lead 6.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -17.756263397552402 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 36.10751272562851.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 17.52530366471666 for lead 13.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -16.708812824730728 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 34.23411648944739.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 18.746870512242253 for lead 20.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -16.23569492717371 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 34.982565439415964.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 17.412276592883416 for lead 27.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -19.794014039930232 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 37.20629063281365.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 17.282648808421754 for lead 34.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -16.61186176341971 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 33.894510571841465.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tmax_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling tmax_2m_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable tmax_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved tmax_2m_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable hgt_pres\n",
      "\n",
      "Concatenating files for hgt_pres. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 0\n",
      "Loaded files for hgt_pres\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for hgt_pres_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 3956.9453125 for lead 6.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -4188.203125 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 8145.1484375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 4466.75 for lead 13.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -4531.53125 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 8998.28125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 4448.9453125 for lead 20.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -4143.125 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 8592.0703125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 4362.328125 for lead 27.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -4275.078125 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 8637.40625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 4359.734375 for lead 34.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -4557.0 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 8916.734375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable hgt_pres_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved hgt_pres_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable diff_temp_2m\n",
      "\n",
      "Concatenating files for diff_temp_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling diff_temp_2m_GEFSv12: 425444\n",
      "Loaded files for diff_temp_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for diff_temp_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 11.847053051656228 for lead 6.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -8.820590205728266 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 20.667643257384494.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 10.134172648563464 for lead 13.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -11.699396966730504 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 21.833569615293968.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 10.459562833365387 for lead 20.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -14.077436174665177 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 24.536999008030563.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 11.341541348482465 for lead 27.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -13.624904024121498 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 24.966445372603964.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 11.577012737977853 for lead 34.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -13.095514347748347 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 24.672527085726202.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling diff_temp_2m_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable diff_temp_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved diff_temp_2m_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n",
      "\n",
      "Latitude values for mask is [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Longitude values for mask is [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Loading files for variable pwat_eatm\n",
      "\n",
      "Concatenating files for pwat_eatm. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for pwat_eatm_GEFSv12: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_GEFSv12: 0\n",
      "Loaded files for pwat_eatm\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.  116.5 117.  117.5\n",
      " 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122.  122.5 123.  123.5\n",
      " 124.  124.5 125.  125.5 126.  126.5 127.  127.5 128.  128.5 129.  129.5\n",
      " 130.  130.5 131.  131.5 132.  132.5 133.  133.5 134.  134.5 135.  135.5\n",
      " 136.  136.5 137.  137.5 138.  138.5 139.  139.5 140.  140.5 141.  141.5\n",
      " 142.  142.5 143.  143.5 144.  144.5 145.  145.5 146.  146.5 147.  147.5\n",
      " 148.  148.5 149.  149.5 150.  150.5 151.  151.5 152.  152.5 153.  153.5\n",
      " 154.  154.5 155.  155.5 156.  156.5 157.  157.5 158.  158.5 159.  159.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [-13.  -13.5 -14.  -14.5 -15.  -15.5 -16.  -16.5 -17.  -17.5 -18.  -18.5\n",
      " -19.  -19.5 -20.  -20.5 -21.  -21.5 -22.  -22.5 -23.  -23.5 -24.  -24.5\n",
      " -25.  -25.5 -26.  -26.5 -27.  -27.5 -28.  -28.5 -29.  -29.5 -30.  -30.5\n",
      " -31.  -31.5 -32.  -32.5 -33.  -33.5 -34.  -34.5 -35.  -35.5 -36.  -36.5]\n",
      "\n",
      "Getting all data before year 2011\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Creating min max for pwat_eatm_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      " We have a split training set for these dates.\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2017-12-27T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 36.46660108379035 for lead 6.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -31.21387430963501 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 67.68047539342535.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 40.62099399224437 for lead 13.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -32.8955581112146 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 73.51655210345896.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 43.631344607010355 for lead 20.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -33.168192684237084 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 76.79953729124745.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 41.24040685512208 for lead 27.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -30.731997329219848 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 71.97240418434193.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 41.76330362708823 for lead 34.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -28.994068661609095 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 70.75737228869733.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for pwat_eatm_GEFSv12: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable pwat_eatm_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved pwat_eatm_GEFSv12 data into Data/model_npy_inputs/australia/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "rerun_function = True\n",
    "\n",
    "if reforecast_input == 'GEFSv12':\n",
    "    for var_name in ['soilw_bgrnd', 'spfh_2m', 'tmax_2m', 'hgt_pres', 'diff_temp_2m', 'pwat_eatm']:\n",
    "        if var_name == 'soilw_bgrnd':\n",
    "            obs_min_max = obs_RZSM_minmax\n",
    "        else:\n",
    "            obs_min_max = None\n",
    "        anomaly_plot_standardize_save_reforecast(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                      val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                      obs_min_max = obs_min_max, rerun_function=rerun_function)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985201bf-8dd8-4523-afd4-0e8fdc1b0733",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anomaly_plot_standardize_save_reforecast_only_GEFSv12' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     obs_min_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43manomaly_plot_standardize_save_reforecast_only_GEFSv12\u001b[49m(obs_source\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGEFSv12\u001b[39m\u001b[38;5;124m'\u001b[39m, region_name \u001b[38;5;241m=\u001b[39m region_name, var_name\u001b[38;5;241m=\u001b[39mvar_name, train_end \u001b[38;5;241m=\u001b[39m train_end, \n\u001b[1;32m      9\u001b[0m                                               val_end \u001b[38;5;241m=\u001b[39m val_end, test_start \u001b[38;5;241m=\u001b[39m test_start, obs_or_forecast \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreforecast\u001b[39m\u001b[38;5;124m'\u001b[39m, plot_distribution \u001b[38;5;241m=\u001b[39m plot_distribution,\n\u001b[1;32m     10\u001b[0m                                               obs_min_max \u001b[38;5;241m=\u001b[39m obs_min_max, rerun_function\u001b[38;5;241m=\u001b[39mrerun_function)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anomaly_plot_standardize_save_reforecast_only_GEFSv12' is not defined"
     ]
    }
   ],
   "source": [
    "rerun_function = True\n",
    "\n",
    "for var_name in ['soilw_bgrnd']:\n",
    "    if var_name == 'obs_RZSM_minmax':\n",
    "        obs_min_max = obs_RZSM_minmax\n",
    "    else:\n",
    "        obs_min_max = None\n",
    "    anomaly_plot_standardize_save_reforecast_only_GEFSv12(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                  val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                  obs_min_max = obs_min_max, rerun_function=rerun_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0621d8-1972-4f6d-8e58-fb3b2e98e1bf",
   "metadata": {},
   "source": [
    "# Double check the units with precipitable water and surface specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b37468-bc7d-4af9-8359-45ef70a7e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precipitable water units\n",
    "#Reforecast (kg m-2, i.e., mm)   Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations (kg m-2)    Source; https://codes.ecmwf.int/grib/param-db/136 \n",
    "\n",
    "#check the scale of the data for pwat and spfh between forecasts and observations\n",
    "pwat_ref = xr.open_dataset('Data/GEFSv12_reforecast/pwat_eatm/precipitable_water_EMC_2000-01-05.nc4')\n",
    "pwat_obs = xr.open_dataset('Data/ERA5/reformat_to_reforecast_shape/CONUS/pwat_eatm/pwat_eatm_reformat_2000-01-05.nc4')\n",
    "\n",
    "pwat_ref.max()\n",
    "pwat_obs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33a018-0b72-4626-874d-ea50580df62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific humidity units\n",
    "#Reforecast (kg kg-1 dry air) Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations ('kg/kg'). I computed this using Metpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136936-2c05-4788-9d93-1645f74b7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check stacked files\n",
    "import pickle\n",
    "with open(f'/glade/work/klesinger/FD_RZSM_deep_learning/Data/model_npy_inputs/CONUS/Model_input_data/reforecast_tmax_2m_GEFSv12_training.pickle', 'rb') as handle:\n",
    "    a= pickle.load(handle)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745becd8-5f48-4432-8a19-cefb688ab4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if verification data is fine\n",
    "# output_npy_dir = f'Data/model_npy_inputs/{}Verification_data'\n",
    "# from glob import glob\n",
    "\n",
    "# for file in glob(f'{output_npy_dir}/OBS*'):\n",
    "#     print(file)\n",
    "#     print(np.load(file))\n",
    "\n",
    "##### The Data loads fine and fast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
