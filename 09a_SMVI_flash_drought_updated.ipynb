{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7302f239-5aba-4164-8125-86df134c9bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 06:42:43.944312: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 06:42:43.989866: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 06:43:37.288395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/glade/work/klesinger/conda-envs/tf212gpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import functions as f\n",
    "#import climpredNEW.climpred \n",
    "#from climpredNEW.climpred.options import OPTIONS\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from numpy import meshgrid\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "import cartopy.feature as cfeature\n",
    "import itertools\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter, LatitudeLocator\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, TwoSlopeNorm\n",
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import percentileofscore as pos\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import confusion_matrix as CM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "612950b1-1fcf-4522-b79c-7d85f090e726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Set script parameters\n",
    "CONUS_mask = f.load_CONUS_mask() #Mask of CONUS which serves as our bounding box. Can later change this to a larger file but then we would have to edit the data from the previous scripts. \n",
    "\n",
    "#Unet final experiment name (week 5)\n",
    "experiment_name='EX26_RZSM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42762c4a-b6c6-4895-9afd-1cc842efcd01",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5626547-7cfc-4960-8bba-ca5787f98ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open observation files\n",
    "obs_percentile_full = xr.open_mfdataset('Data/GLEAM/RZSM_percentile_reformat/*.nc4',combine='nested',concat_dim=['S']).sel(L=[0,6,13,20,27,34]).astype(np.float16).load()\n",
    "obs_anomaly_SubX_format = xr.open_mfdataset('Data/GLEAM/reformat_to_reforecast_shape/RZSM_weighted/*.nc4').sel(L=[0,6,13,20,27,34]).astype(np.float32).load()\n",
    "\n",
    "#Convert back to anomaly\n",
    "max_RZSM_OBS, min_RZSM_OBS = f.load_observation_min_max_RZSM()\n",
    "obs_anomaly_SubX_format = f.reverse_min_max_scaling(obs_anomaly_SubX_format,max_RZSM_OBS,min_RZSM_OBS)\n",
    "\n",
    "obs_anomaly = xr.open_dataset('Data/GLEAM/RZSM_anomaly.nc')\n",
    "\n",
    "\n",
    "#######################################   Reforecast baseline files   ###########################################################################\n",
    "baseline_anomaly_file_list = sorted(glob('Data/GEFSv12_reforecast/soilw_bgrnd/baseline_RZSM_anomaly/RZSM*.nc'))\n",
    "baseline_percentile_file_list = sorted(glob('Data/GEFSv12_reforecast/soilw_bgrnd/percentiles_baseline/RZSM_percentiles_2*.nc'))\n",
    "baseline_percentile_MEM_file_list = sorted(glob('Data/GEFSv12_reforecast/soilw_bgrnd/percentiles_baseline/RZSM_percentiles_MEM_2*.nc'))\n",
    "\n",
    "baseline_percentile = xr.open_mfdataset(baseline_percentile_file_list,combine='nested',concat_dim=['S']).sel(L=[0,6,13,20,27,34]).astype(np.float32).load()\n",
    "baseline_percentile_MEM = xr.open_mfdataset(baseline_percentile_MEM_file_list,combine='nested',concat_dim=['S']).sel(L=[0,6,13,20,27,34]).astype(np.float32).load()\n",
    "\n",
    "#########################################   Prediction (UNET) files   ######################################################################################\n",
    "unet_anomaly_file_list = sorted(glob(f'predictions/no_julian_dates/{experiment_name}_*.nc'))\n",
    "unet_percentile_file_list = sorted(glob(f'predictions/UNET/percentiles/{experiment_name}/RZSM_percentiles_2*.nc'))\n",
    "unet_percentile_MEM_file_list = sorted(glob(f'predictions/UNET/percentiles/{experiment_name}/RZSM_percentiles_MEM_2*.nc'))\n",
    "\n",
    "unet_percentile = xr.open_mfdataset(unet_percentile_file_list,combine='nested',concat_dim=['S']).sel(L=[0,6,13,20,27,34]).astype(np.float32).load()\n",
    "unet_percentile_MEM = xr.open_mfdataset(unet_percentile_MEM_file_list,combine='nested',concat_dim=['S']).sel(L=[0,6,13,20,27,34]).astype(np.float32).load()\n",
    "\n",
    "#Test\n",
    "# anomaly_file_list=baseline_anomaly_file_list\n",
    "# percentile_file_list = baseline_percentile_file_list\n",
    "# percentile_file_list_MEM=baseline_percentile_MEM_file_list\n",
    "# obs_anomaly=obs_anomaly\n",
    "# save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI'\n",
    "# MEM_or_by_model='MEM'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701cacb-6d6d-4751-8b3c-885c1da8ebde",
   "metadata": {},
   "source": [
    "# Create SMVI on observations\n",
    "\n",
    "## 7 day running average anomaly is < 21 day running average anomaly for at least 3 weeks. AND at the end of the 3 weeks the RZSM percentile is <= 20th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ba266-6586-4fb3-8269-27cb11f730d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SMVI_OBSERVATIONS_ONLY(anomaly_file_obs,obs_anomaly_fun,obs_percentile_full_run):\n",
    "    #test\n",
    "    # anomaly_file_SubX_format = obs_anomaly_SubX_format\n",
    "    \n",
    "    save_dir = 'Data/GLEAM/SMVI'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    save_count_name = f'{save_dir}/obs_smvi_count.txt'\n",
    "    \n",
    "    smvi_count = []\n",
    "    for idx,date in enumerate(anomaly_file_obs.S.values):\n",
    "        # break\n",
    "        save_name = f'{save_dir}/SMVI_{date}.nc'\n",
    "        if os.path.exists(save_name):\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            smvi = anomaly_file_obs.sel(S=date).copy(deep=True).load()\n",
    "            smvi.RZSM[:,:,:,:] = 0\n",
    "            \n",
    "            #To save the flash drought occurrence\n",
    "            fd = smvi.copy(deep=True)\n",
    "            \n",
    "            #First get the previous 21 days average from the observation based on the initialization date\n",
    "            previous_21_days = pd.to_datetime(date) - dt.timedelta(days=21)\n",
    "            previous_day = pd.to_datetime(date) - dt.timedelta(days=1)\n",
    "\n",
    "            mean_21_days = obs_anomaly.sel(time=slice(previous_21_days, previous_day)).mean(dim='time').RZSM\n",
    "            mean_21_days = mean_21_days.rename({'latitude':'Y','longitude':'X'})\n",
    "\n",
    "            #Now find the first 3 leads and if they are all less than the 21 day mean average, AND the percentile is less than 20th, then FD is occurring. \n",
    "            smvi_subset = smvi.copy(deep=True)\n",
    "\n",
    "            #Now apply the function for if first 3 weeks are less than the previous 21 day average\n",
    "            #Even though we have multiple models, this is only to make comparisons easier later\n",
    "            less_than_21_day_average = np.all(anomaly_file_obs.isel(L=[1,2,3],M=0).RZSM.values < mean_21_days.values,axis=(0,1))\n",
    "\n",
    "            arr = smvi_subset.RZSM[0,3,:,:].values #blank array\n",
    "            # Replace values where less_than_21_day_average is True with 1 (all 3 must be true)\n",
    "            arr[less_than_21_day_average] = 1\n",
    "            #Replace values\n",
    "            smvi_subset.RZSM[:,3,:,:] = arr\n",
    "\n",
    "            #Now check percentile values. If True for smvi_subset and if obs_percentile is less than 20th, then FD has begun\n",
    "            #Only need the first model because all the other models are the same values (just so that it's in SubX format)\n",
    "            arr_percentile = obs_percentile_full_run.sel(S=date).RZSM_percentile[0,3,:,:].values\n",
    "            fd_begin = np.where((arr_percentile[:,:] <= 20) & (smvi_subset.RZSM[0,3,:,:].values == 1),1,0)\n",
    "\n",
    "            #Now continue flash drought if below the 20th percentile\n",
    "            fd_wk1 = np.where((obs_percentile_full_run.sel(S=date).RZSM_percentile[0,4,:,:].values <= 20) & (fd_begin[:,:] == 1),1,0)\n",
    "            fd_wk2 = np.where((obs_percentile_full_run.sel(S=date).RZSM_percentile[0,5,:,:].values <= 20) & (fd_wk1[:,:] == 1),1,0)\n",
    "            np.count_nonzero(fd_wk2)\n",
    "            #Make sure the flash drought lasts at least 2 weeks\n",
    "            fd_duration = np.where((fd_begin==1) & (fd_wk1==1) & (fd_wk2==1),1,0)\n",
    "            np.count_nonzero(fd_duration)\n",
    "            \n",
    "            #Now add back to dataset\n",
    "            fd.RZSM[:,3,:,:] = fd_duration\n",
    "            fd.RZSM[:,4,:,:] = fd_duration\n",
    "            fd.RZSM[:,5,:,:] = fd_duration\n",
    "            \n",
    "            fd = fd.astype(np.int16)\n",
    "            \n",
    "            fd = fd.expand_dims({'S':1}).rename({'RZSM':'smvi'})\n",
    "            fd.to_netcdf(save_name)\n",
    "            \n",
    "            MEM_count = np.count_nonzero(fd.smvi.values)//(fd.M.shape[0]*3)\n",
    "            print(f'Total FD events:{MEM_count}')\n",
    "            \n",
    "            smvi_count.append(MEM_count)\n",
    "            \n",
    "    return(smvi_count)\n",
    "\n",
    "\n",
    "####### RUN FUNCTION ##############################\n",
    "\n",
    "smvi_count_by_week_obs = SMVI_OBSERVATIONS_ONLY(anomaly_file_obs = obs_anomaly_SubX_format, obs_anomaly_fun=obs_anomaly,\n",
    "                                               obs_percentile_full_run = obs_percentile_full)\n",
    "\n",
    "save_smvi_obs_text = f\"Data/GLEAM/SMVI/obs_smvi_count.txt\"\n",
    "\n",
    "if os.path.exists(save_smvi_obs_text):\n",
    "    # Create an empty list to store the lines\n",
    "    smvi_count_by_week_obs = []\n",
    "\n",
    "    # Open file in read mode\n",
    "    with open(save_smvi_obs_text, \"r\") as file:\n",
    "        # Read each line and append to the list\n",
    "        for line in file:\n",
    "            smvi_count_by_week_obs.append(int(line.strip()))  # strip() removes leading and trailing whitespaces\n",
    "\n",
    "else:\n",
    "    with open(save_smvi_obs_text, \"w\") as file:\n",
    "        for item in smvi_count_by_week_obs:\n",
    "            file.write(\"%s\\n\" % item)\n",
    "\n",
    "#Total weeks in flash drought is simply a single model realization. And it has the number of EVENTS because they all last 2 weeks\n",
    "smvi_count_by_week_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378f277-5420-4b4c-8693-274428b5d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SMVI_REFORECASTS_ONLY(anomaly_file_list,percentile_file_list,percentile_file_list_MEM,obs_anomaly,save_dir,MEM_or_by_model):\n",
    "    #test\n",
    "    # anomaly_file_SubX_format = obs_anomaly_SubX_format\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Get S values (init dates)\n",
    "    init_dates = percentile_file_list.S.values\n",
    "    \n",
    "    if MEM_or_by_model == 'MEM':\n",
    "        save_name_m = 'SMVI_MEM'\n",
    "        save_smvi_obs_text = f'{save_dir}/smvi_count_MEM.txt'\n",
    "    else:\n",
    "        save_name_m = 'SMVI'\n",
    "        save_smvi_obs_text = f'{save_dir}/smvi_count.txt'\n",
    "\n",
    "    #Loop through\n",
    "    smvi_count_by_week_obs = []\n",
    "    for idx,date in enumerate(init_dates):\n",
    "        # break\n",
    "        save_dates = anomaly_file_list[idx].split('_')[-1].split('.')[0]\n",
    "        save_name = f'{save_dir}/{save_name_m}_{save_dates}.nc'\n",
    "        \n",
    "        if os.path.exists(save_name):\n",
    "            pass\n",
    "        else:\n",
    "            # print(f'Working on date {date}')\n",
    "            anomaly_file_to_open = [i for i in anomaly_file_list if save_dates in i][0]\n",
    "            \n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                percentile_file_to_open = percentile_file_list_MEM.sel(S=date)\n",
    "            else:\n",
    "                percentile_file_to_open = percentile_file_list.sel(S=date)\n",
    "            \n",
    "            \n",
    "            #First get the previous 21 days average from the observation based on the initialization date\n",
    "            previous_21_days = pd.to_datetime(date) - dt.timedelta(days=21)\n",
    "            previous_day = pd.to_datetime(date) - dt.timedelta(days=1)\n",
    "\n",
    "            mean_21_days = obs_anomaly.sel(time=slice(previous_21_days, previous_day)).mean(dim='time').RZSM\n",
    "            mean_21_days = mean_21_days.rename({'latitude':'Y','longitude':'X'})\n",
    "\n",
    "            #Now find the first 3 leads and if they are all less than the 21 day mean average, AND the percentile is less than 20th, then FD is occurring. \n",
    "            anomaly_file = xr.open_dataset(anomaly_file_to_open).sel(L=[0,6,13,20,27,34])\n",
    "            \n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                anomaly_file = anomaly_file.mean(dim='M')\n",
    "            \n",
    "            smvi = anomaly_file.copy(deep=True)\n",
    "            fd = smvi.copy(deep=True)\n",
    "            \n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                smvi.RZSM[:,:,:,:] = 0\n",
    "                smvi = smvi.load()\n",
    "            else:\n",
    "                smvi.RZSM[:,:,:,:,:] = 0\n",
    "                smvi = smvi.load()\n",
    "            \n",
    "            #Now apply the function for if first 3 weeks are less than the previous 21 day average\n",
    "            #Even though we have multiple models, this is only to make comparisons easier later\n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                less_than_21_day_average = np.all(anomaly_file.isel(L=[1,2,3]).RZSM.values < mean_21_days.values,axis=(0,1))\n",
    "            else:\n",
    "                less_than_21_day_average = np.zeros(shape=(anomaly_file.M.shape[0], mean_21_days.values.shape[0],mean_21_days.values.shape[1]))\n",
    "                \n",
    "                for model in range(anomaly_file.M.shape[0]):\n",
    "                    less_than_21_day_average[model,:,:] = np.all(anomaly_file.isel(L=[1,2,3],M=model).RZSM.values < mean_21_days.values,axis=(0,1))\n",
    "\n",
    "                #Replace 1's with True\n",
    "                less_than_21_day_average = less_than_21_day_average==1\n",
    "                \n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                arr = smvi.RZSM[0,3,:,:].values\n",
    "            else:\n",
    "                arr = smvi.RZSM[0,:,3,:,:].values\n",
    "            np.count_nonzero(arr)\n",
    "            \n",
    "            # Replace values where less_than_21_day_average is True with 1\n",
    "            arr[less_than_21_day_average] = 1\n",
    "            np.count_nonzero(arr)\n",
    "            \n",
    "            #Replace values\n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                smvi.RZSM[0,3,:,:] = arr\n",
    "            else:\n",
    "                smvi.RZSM[0,:,3,:,:] = arr\n",
    "            np.count_nonzero(arr)\n",
    "            \n",
    "            #Check some values\n",
    "            # anomaly_file.RZSM[0,:3,1,0].values\n",
    "            # mean_21_days[1,0].values\n",
    "            # less_than_21_day_average[1,0]\n",
    "            # smvi.RZSM[0,2,1,0].values\n",
    "            \n",
    "            \n",
    "            #Now check percentile values. If True for smvi and if percentile is less than 20th, then FD has begun\n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                arr_percentile = percentile_file_to_open.RZSM[:,:,:].values\n",
    "                arr_percentile.shape\n",
    "                fd_begin = np.where((arr_percentile[3,:,:] <= 20) & (smvi.RZSM[0,3,:,:].values == 1),1,0)\n",
    "                fd_begin.shape\n",
    "                np.count_nonzero(fd_begin)\n",
    "                #Now continue flash drought if below the 20th percentile\n",
    "                fd_wk1 = np.where((arr_percentile[4,:,:] <= 20) & (fd_begin[:,:] == 1),1,0)\n",
    "                fd_wk2 = np.where((arr_percentile[5,:,:] <= 20) & (fd_wk1[:,:] == 1),1,0)\n",
    "                fd_duration = np.where((fd_begin==1) & (fd_wk1==1) & (fd_wk2==1),1,0)\n",
    "                np.count_nonzero(fd_duration)\n",
    "                #Add back to dataset\n",
    "\n",
    "\n",
    "            else:\n",
    "                fd_total = np.zeros(shape=(percentile_file_to_open.M.shape[0], mean_21_days.values.shape[0],mean_21_days.values.shape[1]))\n",
    "                fd_total.shape #(11, 48, 96)\n",
    "                for model in range(percentile_file_to_open.M.shape[0]):\n",
    "                    arr_percentile = percentile_file_to_open.RZSM[model,:,:,:].values\n",
    "                    arr_percentile.shape #(6, 48, 96)\n",
    "                    fd_begin = np.where((arr_percentile[3,:,:] <= 20) & (smvi.RZSM[0,model,3,:,:].values == 1),1,0)\n",
    "                    fd_begin.shape\n",
    "                    np.count_nonzero(fd_begin)\n",
    "                    #Now continue flash drought if below the 20th percentile\n",
    "                    fd_wk1 = np.where((arr_percentile[4,:,:] <= 20) & (fd_begin[:,:] == 1),1,0)\n",
    "                    fd_wk2 = np.where((arr_percentile[5,:,:] <= 20) & (fd_wk1[:,:] == 1),1,0)\n",
    "                    fd_duration = np.where((fd_begin==1) & (fd_wk1==1) & (fd_wk2==1),1,0)\n",
    "                    np.count_nonzero(fd_duration)\n",
    "                    #Add back to dataset\n",
    "                    fd_total[model,:,:] = fd_duration\n",
    "\n",
    "\n",
    "            #Now add back to dataset\n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                fd.RZSM[0,3,:,:] = fd_duration\n",
    "                fd.RZSM[0,4,:,:] = fd_duration\n",
    "                fd.RZSM[0,5,:,:] = fd_duration\n",
    "            else:\n",
    "                fd.RZSM[0,:,3,:,:] = fd_duration\n",
    "                fd.RZSM[0,:,4,:,:] = fd_duration\n",
    "                fd.RZSM[0,:,5,:,:] = fd_duration\n",
    "            \n",
    "            fd = fd.astype(np.int16).rename({'RZSM':'smvi'})\n",
    "            fd.to_netcdf(save_name)\n",
    "            \n",
    "            if MEM_or_by_model == 'MEM':\n",
    "                smvi_count = np.count_nonzero(fd.smvi.values)//3\n",
    "                print(f'Total FD events of all MEM: {smvi_count}')\n",
    "                smvi_count_by_week_obs.append(smvi_count)\n",
    "            else:\n",
    "                smvi_count = np.count_nonzero(fd.smvi.values)//(3*11)\n",
    "                print(f'Total FD events averaged across models: {smvi_count}')\n",
    "                smvi_count_by_week_obs.append(smvi_count)\n",
    "\n",
    "    if os.path.exists(save_smvi_obs_text):\n",
    "        # Create an empty list to store the lines\n",
    "        smvi_count_by_week_obs = []\n",
    "\n",
    "        # Open file in read mode\n",
    "        with open(save_smvi_obs_text, \"r\") as file:\n",
    "            # Read each line and append to the list\n",
    "            for line in file:\n",
    "                smvi_count_by_week_obs.append(int(line.strip()))  # strip() removes leading and trailing whitespaces\n",
    "    else:\n",
    "        with open(save_smvi_obs_text, \"w\") as file:\n",
    "            for item in smvi_count_by_week_obs:\n",
    "                file.write(\"%s\\n\" % item)\n",
    "                \n",
    "    return(smvi_count_by_week_obs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f7ade-9cca-4a28-95a7-3b01cbb96acf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de45763-72ac-4caa-9826-4231c24abf17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#UNET prediction reforecast data\n",
    "base_smvi_count_MEM =SMVI_REFORECASTS_ONLY(anomaly_file_list=unet_anomaly_file_list,\n",
    "                      percentile_file_list = baseline_percentile,\n",
    "                      percentile_file_list_MEM=baseline_percentile_MEM,\n",
    "                      obs_anomaly=obs_anomaly,\n",
    "                      save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI',\n",
    "                      MEM_or_by_model='MEM')\n",
    "\n",
    "#UNET prediction reforecast data\n",
    "base_smvi_count = SMVI_REFORECASTS_ONLY(anomaly_file_list=unet_anomaly_file_list,\n",
    "                      percentile_file_list = baseline_percentile,\n",
    "                      percentile_file_list_MEM=baseline_percentile_MEM,\n",
    "                      obs_anomaly=obs_anomaly,\n",
    "                      save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI',\n",
    "                      MEM_or_by_model='model_realization')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872f84df-04c4-4d4d-a241-1dc3b707d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline reforecast data\n",
    "unet_smvi_count_MEM = SMVI_REFORECASTS_ONLY(anomaly_file_list=baseline_anomaly_file_list,\n",
    "                      percentile_file_list = unet_percentile,\n",
    "                      percentile_file_list_MEM=unet_percentile_MEM,\n",
    "                      obs_anomaly=obs_anomaly,\n",
    "                      save_dir=f'predictions/UNET/SMVI/{experiment_name}',\n",
    "                      MEM_or_by_model='MEM')\n",
    "\n",
    "#Baseline reforecast data\n",
    "unet_smpvi_count = SMVI_REFORECASTS_ONLY(anomaly_file_list=baseline_anomaly_file_list,\n",
    "                      percentile_file_list = unet_percentile,\n",
    "                      percentile_file_list_MEM=unet_percentile_MEM,\n",
    "                      obs_anomaly=obs_anomaly,\n",
    "                      save_dir=f'predictions/UNET/SMVI/{experiment_name}',\n",
    "                      MEM_or_by_model='model_realization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002fb11-d2e9-4c0b-ad20-fbb626a3acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "############### FIRST LOAD ALL THE DATA, THEY ARE SMALLER FILES ####################################################################\n",
    "\n",
    "#Do not need to grab L=0, it cannot be used for analysis\n",
    "\n",
    "#Open observation files\n",
    "obs_smvi = xr.open_mfdataset('Data/GLEAM/SMVI/SMVI*.nc',combine='nested',concat_dim=['S']).sel(L=[6,13,20,27,34]).load()\n",
    "\n",
    "#Reforecast baseline files\n",
    "baseline_reforecast_smvi = xr.open_mfdataset('Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/SMVI_2*.nc',combine='nested',concat_dim=['S']).sel(L=[6,13,20,27,34]).load()\n",
    "baseline_reforecast_smvi_MEM = xr.open_mfdataset('Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/SMVI_MEM_2*.nc',combine='nested',concat_dim=['S']).sel(L=[6,13,20,27,34]).load()\n",
    "\n",
    "#Prediction (UNET) files\n",
    "unet_prediction_SMVI = xr.open_mfdataset(f'predictions/UNET/SMVI/{experiment_name}/SMVI_2*.nc',combine='nested',concat_dim=['S']).sel(L=[6,13,20,27,34]).load()\n",
    "unet_prediction_SMVI_MEM = xr.open_mfdataset(f'predictions/UNET/SMVI/{experiment_name}/SMVI_MEM_2*.nc',combine='nested',concat_dim=['S']).sel(L=[6,13,20,27,34]).load()\n",
    "\n",
    "\n",
    "#Test\n",
    "# anomaly_file_list=baseline_anomaly_file_list\n",
    "# percentile_file_list = baseline_percentile_file_list\n",
    "# percentile_file_list_MEM=baseline_percentile_MEM_file_list\n",
    "# obs_anomaly=obs_anomaly\n",
    "# save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI'\n",
    "# MEM_or_by_model='MEM'\n",
    "\n",
    "CONUS_mask_arr = CONUS_mask.to_array().values.squeeze().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c580f9f-98ed-4d5b-a1d4-99791299cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTRUCT TRUE POSITIVE, FALSE POSITIVE, FALSE NEGATIVE INFO\n",
    "\n",
    "def metric_MEM(reforecast_file, observation_file, save_dir):\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    #test \n",
    "    # reforecast_file = unet_prediction_SMPD_MEM\n",
    "    # observation_file = obs_smpd\n",
    "    # save_dir = f'predictions/UNET/SMVI/{experiment_name}'\n",
    "    \n",
    "    #Returns the CSI (critical success index metric, true positive rate, and false positive rate)\n",
    "    csi_out = reforecast_file.isel(S=0).copy(deep=True)\n",
    "    csi_out.smvi[:,:,:] = 0\n",
    "    csi_out = csi_out.astype(np.float32)\n",
    "    \n",
    "    tpr_out = reforecast_file.isel(S=0).copy(deep=True)\n",
    "    tpr_out.smvi[:,:,:] = 0\n",
    "    tpr_out = tpr_out.astype(np.float32)\n",
    "    \n",
    "    fpr_out = reforecast_file.isel(S=0).copy(deep=True)\n",
    "    fpr_out.smvi[:,:,:] = 0\n",
    "    fpr_out = fpr_out.astype(np.float32)\n",
    "    \n",
    "    observation_file = observation_file.mean(dim='M')\n",
    "    \n",
    "    #only choose warm season months (March - November)\n",
    "    reforecast_file_subset = reforecast_file.sel(S=reforecast_file['S.month'].isin([3,4,5,6,7,8,9,10,11]))\n",
    "    np.count_nonzero(reforecast_file_subset.smvi.values)\n",
    "    \n",
    "    observation_file = observation_file.assign_coords(S=reforecast_file.S.values)\n",
    "    \n",
    "    observation_file_subset = observation_file.sel(S=observation_file['S.month'].isin([3,4,5,6,7,8,9,10,11]))\n",
    "    np.count_nonzero(observation_file_subset.smvi.values)\n",
    "\n",
    "    #True positive\n",
    "    tp = np.where((observation_file_subset.smvi.values == 1) & (reforecast_file_subset.smvi.values == 1),1,0) \n",
    "    np.count_nonzero(tp)\n",
    "    tp.shape\n",
    "    np.count_nonzero(np.isnan(tp))\n",
    "\n",
    "    #False positive\n",
    "    fp = np.where((observation_file_subset.smvi.values == 0) & (reforecast_file_subset.smvi.values == 1),1,0) \n",
    "    np.count_nonzero(fp)\n",
    "    \n",
    "    fn = np.where((observation_file_subset.smvi.values == 1) & (reforecast_file_subset.smvi.values == 0),1,0) \n",
    "    np.count_nonzero(fn)\n",
    "\n",
    "    tn = np.where((observation_file_subset.smvi.values == 0) & (reforecast_file_subset.smvi.values == 0),1,0) \n",
    "    np.count_nonzero(tn)\n",
    "\n",
    "    tp_sum = tp.sum(axis=0)\n",
    "    fp_sum = fp.sum(axis=0)\n",
    "    fn_sum = fn.sum(axis=0)\n",
    "    tn_sum = tn.sum(axis=0)\n",
    "    \n",
    "    \n",
    "    TPR = (tp_sum/(tp_sum+fn_sum))*100\n",
    "    tpr_out.smvi[:,:,:] = TPR\n",
    "    len(np.unique(tpr_out.smvi.values))\n",
    "    \n",
    "    FPR = (fp_sum/(fp_sum+tn_sum))*100\n",
    "    fpr_out.smvi[:,:,:] = FPR\n",
    "    len(np.unique(fpr_out.smvi.values))\n",
    "    \n",
    "    CSI = (tp_sum/(tp_sum+fp_sum+fn_sum))*100\n",
    "    csi_out.smvi[:,:,:] = CSI\n",
    "    len(np.unique(csi_out.smvi.values))\n",
    "\n",
    "    \n",
    "    #We could apply numpy where functions, but it's easier just to explicitly loop through data\n",
    "    for X in range(reforecast_file_subset.X.shape[0]):\n",
    "        # print(X)\n",
    "        for Y in range(reforecast_file_subset.Y.shape[0]):\n",
    "            if CONUS_mask_arr[Y,X] == 1:\n",
    "\n",
    "                csi_out.smvi[:,Y,X] = np.nan\n",
    "                fpr_out.smvi[:,Y,X] = np.nan\n",
    "                tpr_out.smvi[:,Y,X] = np.nan\n",
    "    \n",
    "    #Now save the data\n",
    "    csi_out.to_netcdf(f'{save_dir}/csi_percentage_MEM.nc')\n",
    "    tpr_out.to_netcdf(f'{save_dir}/tpr_percentage_MEM.nc')\n",
    "    fpr_out.to_netcdf(f'{save_dir}/fpr_percentage_MEM.nc')\n",
    "            \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26641e62-6022-4d7e-a812-b18116080e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the different metrics across CONUS for MEM\n",
    "metric_MEM(reforecast_file=baseline_reforecast_smvi_MEM, observation_file=obs_smvi, save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/metric_values')\n",
    "metric_MEM(reforecast_file=unet_prediction_SMVI_MEM, observation_file=obs_smvi, save_dir=f'predictions/UNET/SMVI/{experiment_name}/metric_values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18729adb-c12b-4bf6-9154-e65a4225e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTRUCT TRUE POSITIVE, FALSE POSITIVE, FALSE NEGATIVE INFO\n",
    "\n",
    "def metric_ensemble_realization_probability(reforecast_file, observation_file, save_dir, probability):\n",
    "    \n",
    "    #test for \n",
    "    # reforecast_file = unet_prediction_SMPD_MEM\n",
    "    # observation_file = obs_smpd\n",
    "    # save_dir = f'predictions/UNET/SMVI/{experiment_name}'\n",
    "\n",
    "    #There are 11 different model realizations, so just assume 10 models for probability sake\n",
    "\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    #Returns the CSI (critical success index metric, true positive rate, and false positive rate)\n",
    "    csi_out = reforecast_file.isel(S=0).mean(dim='M').copy(deep=True)\n",
    "    csi_out.smvi[:,:,:] = 0\n",
    "    csi_out = csi_out.astype(np.float32)\n",
    "    \n",
    "    tpr_out = reforecast_file.isel(S=0).mean(dim='M').copy(deep=True)\n",
    "    tpr_out.smvi[:,:,:] = 0\n",
    "    tpr_out = tpr_out.astype(np.float32)\n",
    "    \n",
    "    fpr_out = reforecast_file.isel(S=0).mean(dim='M').copy(deep=True)\n",
    "    fpr_out.smvi[:,:,:] = 0\n",
    "    fpr_out = fpr_out.astype(np.float32)\n",
    "        \n",
    "    #only choose warm season months (March - November)\n",
    "    reforecast_file_subset = reforecast_file.sel(S=reforecast_file['S.month'].isin([3,4,5,6,7,8,9,10,11]))\n",
    "    np.count_nonzero(reforecast_file_subset.smvi.values)\n",
    "    \n",
    "    observation_file = observation_file.assign_coords(S=reforecast_file.S.values)\n",
    "    \n",
    "    observation_file_subset = observation_file.sel(S=observation_file['S.month'].isin([3,4,5,6,7,8,9,10,11]))\n",
    "    np.count_nonzero(observation_file_subset.smvi.values)\n",
    "    \n",
    "    #True positive\n",
    "    tp = np.where((observation_file_subset.smvi.values == 1) & (reforecast_file_subset.smvi.values == 1),1,0) \n",
    "    np.count_nonzero(tp)\n",
    "    tp.shape\n",
    "    np.count_nonzero(np.isnan(tp))\n",
    "\n",
    "    #False positive\n",
    "    fp = np.where((observation_file_subset.smvi.values == 0) & (reforecast_file_subset.smvi.values == 1),1,0) \n",
    "    np.count_nonzero(fp)\n",
    "    \n",
    "    fn = np.where((observation_file_subset.smvi.values == 1) & (reforecast_file_subset.smvi.values == 0),1,0) \n",
    "    np.count_nonzero(fn)\n",
    "\n",
    "    tn = np.where((observation_file_subset.smvi.values == 0) & (reforecast_file_subset.smvi.values == 0),1,0) \n",
    "    np.count_nonzero(tn)\n",
    "    \n",
    "    #Now only sum over the model realization and see if the probability exceeds it\n",
    "    tp_sum_prob = np.where(tp.sum(axis=1) >= probability*10,1,0).sum(axis=0)\n",
    "    tp_sum_prob.shape\n",
    "    fp_sum_prob = np.where(fp.sum(axis=1) >= probability*10,1,0).sum(axis=0)\n",
    "    fn_sum_prob = np.where(fn.sum(axis=1) >= probability*10,1,0).sum(axis=0)\n",
    "    tn_sum_prob = np.where(tn.sum(axis=1) >= probability*10,1,0).sum(axis=0)\n",
    "    \n",
    "    \n",
    "    TPR = (tp_sum_prob/(tp_sum_prob+fn_sum_prob))*100\n",
    "    tpr_out.smvi[:,:,:] = TPR\n",
    "    len(np.unique(tpr_out.smvi.values))\n",
    "    \n",
    "    FPR = (fp_sum_prob/(fp_sum_prob+tn_sum_prob))*100\n",
    "    fpr_out.smvi[:,:,:] = FPR\n",
    "    len(np.unique(fpr_out.smvi.values))\n",
    "    \n",
    "    CSI = (tp_sum_prob/(tp_sum_prob+fp_sum_prob+fn_sum_prob))*100\n",
    "    csi_out.smvi[:,:,:] = CSI\n",
    "    len(np.unique(csi_out.smvi.values))\n",
    "\n",
    "    \n",
    "    #We could apply numpy where functions, but it's easier just to explicitly loop through data\n",
    "    for X in range(reforecast_file_subset.X.shape[0]):\n",
    "        # print(X)\n",
    "        for Y in range(reforecast_file_subset.Y.shape[0]):\n",
    "            if CONUS_mask_arr[Y,X] == 1:\n",
    "\n",
    "                csi_out.smvi[:,Y,X] = np.nan\n",
    "                fpr_out.smvi[:,Y,X] = np.nan\n",
    "                tpr_out.smvi[:,Y,X] = np.nan\n",
    "    \n",
    "    #Now save the data\n",
    "    csi_out.to_netcdf(f'{save_dir}/csi_percentage_probability{probability}.nc')\n",
    "    tpr_out.to_netcdf(f'{save_dir}/tpr_percentage_probability{probability}.nc')\n",
    "    fpr_out.to_netcdf(f'{save_dir}/fpr_percentage_probability{probability}.nc')\n",
    "            \n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c917a8a-7f87-45a1-9f5e-96f3b5cc81ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the different metrics across CONUS for MEM\n",
    "metric_ensemble_realization_probability(reforecast_file=baseline_reforecast_smvi, observation_file=obs_smvi, save_dir='Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/metric_values', probability = 0.5)\n",
    "metric_ensemble_realization_probability(reforecast_file=unet_prediction_SMVI, observation_file=obs_smvi, save_dir=f'predictions/UNET/SMVI/{experiment_name}/metric_values', probability = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff49a2-7085-4083-a7ae-fd8e706bf2bd",
   "metadata": {},
   "source": [
    "# Open SMVI flash drought onset and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47492db3-33ef-42ba-9961-8e08f008101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_smvi = xr.open_mfdataset('Data/GLEAM/SMVI/SMVI*.nc').sel(L=[20,27,34]).load()\n",
    "baseline_smvi = xr.open_mfdataset('Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/SMVI_MEM*.nc').sel(L=[20,27,34]).load()\n",
    "unet_smvi = xr.open_mfdataset(f'predictions/UNET/SMVI/{experiment_name}/SMVI_MEM*.nc').sel(L=[20,27,34]).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d20a33ad-038c-4fc7-995c-78b31612d431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca08b3b-1d71-4b4a-a9cb-5614f1ff06ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dates\n",
    "start_ = '2019-08-01'\n",
    "end_ = '2019-10-30'\n",
    "\n",
    "#Mask with np.nan for non-CONUS land values\n",
    "mask_anom = CONUS_mask['NCA-LDAS_mask'][0,:,:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e479de-a35b-4c32-97dd-784251516a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_anom = obs_smvi.sel(S=slice(start_,end_)).mean(dim='M')\n",
    "unet_anom = unet_smvi.sel(S=slice(start_,end_))\n",
    "baseline_anom = baseline_smvi.sel(S=slice(start_,end_))\n",
    "\n",
    "obs_anom = xr.where(mask_anom ==1, obs_anom,np.nan)\n",
    "unet_anom = xr.where(mask_anom ==1, unet_anom,np.nan)\n",
    "baseline_anom = xr.where(mask_anom ==1, baseline_anom,np.nan)\n",
    "\n",
    "obs_anom = obs_anom.assign_coords({'S':unet_anom.S.values})\n",
    "\n",
    "unet_anom = xr.where(~np.isnan(obs_anom), unet_anom,np.nan)\n",
    "baseline_anom = xr.where(~np.isnan(obs_anom), baseline_anom,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bffc6829-f8e5-4159-aeeb-ed02198d4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_max_of_files(obs, unet, baseline, date):\n",
    "    #test \n",
    "    # date = '2019-08-07'\n",
    "    \n",
    "    min_ = []\n",
    "    max_ = []\n",
    "\n",
    "    min_.append(obs.sel(S=date).min().smvi.values)\n",
    "    min_.append(unet.sel(S=date).min().smvi.values)\n",
    "    min_.append(baseline.sel(S=date).min().smvi.values)\n",
    "\n",
    "    max_.append(obs.sel(S=date).max().smvi.values)\n",
    "    max_.append(unet.sel(S=date).max().smvi.values)\n",
    "    max_.append(baseline.sel(S=date).max().smvi.values)\n",
    "\n",
    "    return(min(min_),max(max_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08e48a03-dd7c-45b0-9389-0c4bc32dfee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_array(file,lead,date):\n",
    "    return(file.sel(L=lead,S=date).smvi.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88ed0bef-591e-4afa-a3ae-d9800dbd4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "   \n",
    "# cmap = 'coolwarm'\n",
    "def plot_case_study_rci(obs, unet, baseline, init_date):\n",
    "    cmap = plt.get_cmap('bwr')    \n",
    "    \n",
    "    save_dir = f'Outputs/Case_studies/Southeast_US/SMVI'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 3, ncols= 3, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(15, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    init_date = pd.to_datetime(init_date)\n",
    "    date = f'{init_date.year}-{init_date.month:02}-{init_date.day:02}'\n",
    "    \n",
    "    min_,max_ = get_min_max_of_files(obs, unet, baseline, date)\n",
    "    # test_file = mae_rzsm_keys\n",
    "    # for Subx original data\n",
    "    \n",
    "    lon = obs.X.values\n",
    "    lat = obs.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in [20,27,34]:\n",
    "        for data_to_plot,name in zip([obs, unet, baseline], ['GLEAM','UNET','Baseline']):\n",
    "            # break\n",
    "            data = return_array(file=data_to_plot,lead=lead, date=date)\n",
    "    \n",
    "            # v = np.linspace(min_, max_, 20, endpoint=True)\n",
    "        \n",
    "            map = Basemap(projection='cyl', llcrnrlat=25, urcrnrlat=50,\n",
    "                          llcrnrlon=-128, urcrnrlon=-60, resolution='l')\n",
    "            x, y = map(*np.meshgrid(lon, lat))\n",
    "            # Adjust the text coordinates based on the actual data coordinates\n",
    "        \n",
    "        \n",
    "            im = axs[axs_start].contourf(x, y, data, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap)\n",
    "    \n",
    "    \n",
    "            # axs[idx].title.set_text(f'SubX Lead {lead*7}')\n",
    "            gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                       linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "            gl.xlabels_top = False\n",
    "            gl.ylabels_right = False\n",
    "            if lead != 1:\n",
    "                gl.ylabels_left = False\n",
    "            gl.xformatter = LongitudeFormatter()\n",
    "            gl.yformatter = LatitudeFormatter()\n",
    "            axs[axs_start].coastlines()\n",
    "            # plt.colorbar(im)\n",
    "            # axs[idx].set_aspect('auto', adjustable=None)\n",
    "            axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "            axs[axs_start].set_title(f'{name} Lead {lead}',fontsize=15)\n",
    "            axs_start+=1\n",
    "    cbar_ax = fig.add_axes([0.05, -0.05, .9, .04])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    fig.suptitle(f'Init date: {date}', fontsize=30)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/Southeast_SMVI_init{date}.png',bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab933d0-a842-403e-9251-13c5adca129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for init_date in obs_anom.S.values:\n",
    "    plot_case_study_rci(obs=obs_anom, unet=unet_anom, baseline=baseline_anom, init_date=init_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6834dbaa-16ec-4305-9723-e91f984b9f9f",
   "metadata": {},
   "source": [
    "# Plot percentages of SMVI across CONUS (incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005293c9-4a2a-4466-b5b6-13898723a270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metric_file(metric, probability_or_MEM):\n",
    "    if probability_or_MEM == 0.5:\n",
    "        baseline = xr.open_dataset(f'Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/metric_values/{metric}_percentage_probability{probability_or_MEM}.nc').sel(L=[20,27,34])\n",
    "        unet = xr.open_dataset(f'predictions/UNET/SMVI/{experiment_name}/metric_values/{metric}_percentage_probability{probability_or_MEM}.nc').sel(L=[20,27,34])\n",
    "    elif probability_or_MEM == 'MEM':\n",
    "        baseline = xr.open_dataset(f'Data/GEFSv12_reforecast/soilw_bgrnd/SMVI/metric_values/{metric}_percentage_MEM.nc').sel(L=[20,27,34])\n",
    "        unet = xr.open_dataset(f'predictions/UNET/SMVI/{experiment_name}/metric_values/{metric}_percentage_MEM.nc').sel(L=[20,27,34])\n",
    "    return(baseline, unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668d391-4932-47e4-b70e-5dafa290d8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cmap = 'coolwarm'\n",
    "def plot_metric(metric,probability_or_MEM):\n",
    "\n",
    "    \n",
    "    cmap = plt.get_cmap('bwr')    \n",
    "    \n",
    "    save_dir = f'Outputs/FD_metrics'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    fig, axs = plt.subplots(\n",
    "        nrows = 1, ncols= 2, subplot_kw={'projection': ccrs.PlateCarree()}, figsize=(15, 10))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    \n",
    "    min_,max_ = get_min_max_of_files_anomaly(obs, unet, baseline, date)\n",
    "    # test_file = mae_rzsm_keys\n",
    "    # for Subx original data\n",
    "    \n",
    "    lon = obs.X.values\n",
    "    lat = obs.Y.values\n",
    "    \n",
    "    axs_start = 0\n",
    "    for lead in [20,27,34]:\n",
    "        for data_to_plot,name in zip([obs, unet, baseline], ['GLEAM','UNET','Baseline']):\n",
    "            # break\n",
    "            data = return_array_anomaly(file=data_to_plot,lead=lead, date=date)\n",
    "    \n",
    "            v = np.linspace(min_, max_, 20, endpoint=True)\n",
    "        \n",
    "            map = Basemap(projection='cyl', llcrnrlat=25, urcrnrlat=50,\n",
    "                          llcrnrlon=-128, urcrnrlon=-60, resolution='l')\n",
    "            x, y = map(*np.meshgrid(lon, lat))\n",
    "            # Adjust the text coordinates based on the actual data coordinates\n",
    "        \n",
    "            norm = TwoSlopeNorm(vmin=min_, vcenter=0, vmax=max_)\n",
    "        \n",
    "            im = axs[axs_start].contourf(x, y, data, levels=v, extend='both',\n",
    "                                  transform=ccrs.PlateCarree(), cmap=cmap,norm=norm)\n",
    "    \n",
    "    \n",
    "            # axs[idx].title.set_text(f'SubX Lead {lead*7}')\n",
    "            gl = axs[axs_start].gridlines(crs=ccrs.PlateCarree(), draw_labels=True,\n",
    "                                       linewidth=0.7, color='gray', alpha=0.5, linestyle='--')\n",
    "            gl.xlabels_top = False\n",
    "            gl.ylabels_right = False\n",
    "            if lead != 1:\n",
    "                gl.ylabels_left = False\n",
    "            gl.xformatter = LongitudeFormatter()\n",
    "            gl.yformatter = LatitudeFormatter()\n",
    "            axs[axs_start].coastlines()\n",
    "            # plt.colorbar(im)\n",
    "            # axs[idx].set_aspect('auto', adjustable=None)\n",
    "            axs[axs_start].set_aspect('equal')  # this makes the plots better\n",
    "            axs[axs_start].set_title(f'{name} Lead {lead}',fontsize=15)\n",
    "\n",
    "            if name in ['UNET','Baseline']:\n",
    "                # Calculate the Pearson correlation coefficient\n",
    "                obs_corr = return_array_anomaly(file=obs,lead=lead, date=date).flatten()\n",
    "                data_corr = data.flatten()\n",
    "\n",
    "                data_corr = data_corr[~np.isnan(obs_corr)]\n",
    "                obs_corr = obs_corr[~np.isnan(obs_corr)]\n",
    "                \n",
    "                correlation_matrix = np.corrcoef(obs_corr, data_corr)\n",
    "                # The correlation coefficient is in the top right corner of the correlation matrix\n",
    "                correlation_coefficient = correlation_matrix[0, 1]\n",
    "                correlation_coefficient = round(correlation_coefficient,4)\n",
    "                #find the correlation coefficient across the dataset\n",
    "                axs[axs_start].text(text_x, text_y, f'Corr: {correlation_coefficient}', ha='right', va='bottom', fontsize=font_size_corr, color='blue', weight = 'bold')\n",
    "            \n",
    "            \n",
    "            axs_start+=1\n",
    "            \n",
    "    cbar_ax = fig.add_axes([0.05, -0.05, .9, .04])\n",
    "    \n",
    "    # Draw the colorbar\n",
    "    cbar = fig.colorbar(im, cax=cbar_ax, orientation='horizontal')\n",
    "    fig.suptitle(f'Init date: {date}', fontsize=30)\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{save_dir}/{metric}.png',bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu]",
   "language": "python",
   "name": "conda-env-tf212gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
