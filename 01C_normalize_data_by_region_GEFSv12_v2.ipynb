{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb79f-b472-4171-b0ab-30c1b6585412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from metpy.units import units\n",
    "import preprocessUtils as putils\n",
    "from typing import Tuple\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b451f-7893-4304-813a-eca432631ab6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3ff845-71d9-422d-9e33-09a3af5be48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nGEFSv12 reforecast appears to have some missing values for some init dates. \\nWe are replacing those with the mean because this is messing up min/max scaling\\nvery badly\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.) Needs about 50GB RAM for CONUS (shape of file is 48 x 96 lat/lon)\n",
    "\n",
    "Select a specific region (either CONUS, NH (Northern Hemisphere), or australia)\n",
    "\n",
    "Takes about 4 hours to fully complete everything.\n",
    "'''\n",
    "\n",
    "''' \n",
    "GEFSv12 reforecast appears to have some missing values for some init dates. \n",
    "We are replacing those with the mean because this is messing up min/max scaling\n",
    "very badly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44223dd6-1a0f-4c8d-a95b-cdf86d30fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_run_OBS_reformat_to_SubX = False #For reformatting the observations to the same lead-time format as GEFSv12\n",
    "plot_distribution = False #This set of functions takes a long time to plot. But if you want to see the anomaly and min-max distribution, select True\n",
    "\n",
    "'''Daily lags for observations -  we can specify up to 12 weeks in lagged RZSM data (these are 7-day increments). \n",
    "You can select whatever values you want (as long as they are before the reforecast period'''\n",
    "daily_lags = putils.make_daily_lags()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d845cd-a1b4-454a-b548-c9ff8577e2a8",
   "metadata": {},
   "source": [
    "# Data directory locations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705e8a43-188b-46c5-a45f-02a2637c28c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n"
     ]
    }
   ],
   "source": [
    "region_name = 'china' #or ['australia', 'CONUS', 'china']\n",
    "\n",
    "reforecast_input = 'GEFSv12'  #['GEFSv12', 'ECMWF']\n",
    "\n",
    "start_date = '1999-01-01' #for reanalysis data. Select earlier dates than the forecasts to allow for smoothing and because we need lagged observations\n",
    "end_date = '2020-03-01' #for reanalysis data\n",
    "\n",
    "\n",
    "global verification_var\n",
    "verification_var = 'soilw_bgrnd_GLEAM' #this is for what we are verifying with DL outputs\n",
    "\n",
    "#Gleam observations\n",
    "gleam_dir = 'Data/GLEAM'\n",
    "\n",
    "#Forecast predictions\n",
    "if reforecast_input == 'GEFSv12':\n",
    "    fcst_dir = 'Data/GEFSv12_reforecast'\n",
    "\n",
    "#ERA5 observations\n",
    "era5_dir = 'Data/ERA5'\n",
    "\n",
    "if region_name != 'CONUS':\n",
    "    #Gleam observations\n",
    "    gleam_dir = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "    #Forecast predictions\n",
    "    fcst_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "    \n",
    "    #ERA5 observations\n",
    "    era5_dir = f'Data_{region_name}/ERA5'\n",
    "\n",
    "\n",
    "#Set years for training, validation, and testing\n",
    "train_end = 2015\n",
    "val_end = 2017\n",
    "test_start = 2018\n",
    "\n",
    "\n",
    "spfh_unit = 'kg/kg' #Make sure that our reforecast and observation have the same unit for specific humidity\n",
    "\n",
    "global mask\n",
    "mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "#Get initialized dates from GEFSv12 to convert observations for easier stacking\n",
    "global init_date_list\n",
    "\n",
    "init_date_list = putils.get_init_date_list(forecast_variable_path=f'{fcst_dir}/soilw_bgrnd')\n",
    "init_date_list_datetime = [pd.to_datetime(i) for i in init_date_list]\n",
    "\n",
    "global var_list\n",
    "var_list = ['soilw_bgrnd', 'tmax_2m', 'spfh_2m', 'pwat_eatm','diff_temp_2m','hgt_pres'] #Observation variables to pre-process\n",
    "\n",
    "global lead_select\n",
    "lead_select = [6,13,20,27,34] #For subsetting data by leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023cfa8-d5b4-4bc2-8031-d4d71d939e39",
   "metadata": {},
   "source": [
    "# Load Observation Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cc6fa0-f9ae-4c0a-89db-5bbb73ddff50",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading observations. Applying a 7-day rolling mean between 1999-01-01 and 2020-03-01 for china Region.\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "Number of np.nan values before rolling mean for GLEAM_RZSM_obs: 2744505\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "Number of np.nan values before rolling mean for ERA_precipitable_water_obs: 0\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "Number of np.nan values before rolling mean for ERA_geopotential_z200_obs: 0\n",
      "Creating tmax and difference in tmax and tmin variables.\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "Number of np.nan values before rolling mean for ERA_Tmax_obs: 0\n",
      "Number of np.nan values before rolling mean for ERA_Tmin_obs: 0\n",
      "\n",
      "Loading the previously created specific humidity from Data_china/ERA5\n",
      "Loaded all observations.\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading observations. Applying a 7-day rolling mean between {start_date} and {end_date} for {region_name} Region.')\n",
    "\n",
    "RZSM_obs_GLEAM,proper_date_time = putils.open_reanalysis_files_and_preprocess_rolling_mean_RZSM_only(path_to_file=f'{gleam_dir}/RZSM_weighted_mean_0_100cm.nc4',\n",
    "                                                             file_variable='GLEAM_RZSM_obs',start_date=start_date,end_date=end_date,region_name = region_name)\n",
    "\n",
    "precipitable_water_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/total_column_water_merged.nc4',\n",
    "                                                             file_variable='ERA_precipitable_water_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "geopotential_z200_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/geopotential_merged.nc4',\n",
    "                                                             file_variable='ERA_geopotential_z200_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "\n",
    "max_temp_ERA, diff_temp_ERA = putils.open_temperature_files_and_preprocess_rolling_mean(path_to_tmax =f'{era5_dir}/maximum_2m_temperature_merged.nc4', \n",
    "                                                                                 path_to_tmin =f'{era5_dir}/minimum_2m_temperature_merged.nc4',\n",
    "                                                                                 file_variable_tmax = 'ERA_Tmax_obs',file_variable_tmin = 'ERA_Tmin_obs',\n",
    "                                                                                 start_date=start_date, end_date=end_date, \n",
    "                                                                                 region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "spfh_ERA = putils.calculate_2m_specific_humidity_and_preprocess(path_to_dewpoint=f'{era5_dir}/2m_dewpoint_temperature_merged.nc4', \n",
    "                                              path_to_pressure=f'{era5_dir}/surface_pressure_merged.nc4', \n",
    "                                              file_variable_dewpoint='ERA_2m_dewpoint_obs', file_variable_pressure='ERA_surface_pressure_obs', \n",
    "                                              start_date=start_date, end_date=end_date, \n",
    "                                              region_name = region_name, proper_date_time=proper_date_time,\n",
    "                                              obs_dir = era5_dir, spfh_unit = spfh_unit)\n",
    "\n",
    "print('Loaded all observations.')\n",
    "\n",
    "#Print saving the anomaly for each region as the standard format\n",
    "climatology_season = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.year'] <= train_end)).groupby(f\"time.season\").mean()\n",
    "\n",
    "summer_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'JJA')) - climatology_season.sel(season='JJA')\n",
    "fall_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'SON')) - climatology_season.sel(season='SON')\n",
    "winter_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'DJF')) - climatology_season.sel(season='DJF')\n",
    "spring_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'MAM')) - climatology_season.sel(season='MAM')\n",
    "\n",
    "combined_files = xr.concat([summer_,fall_,winter_,spring_],dim='time').sortby('time')\n",
    "combined_files=combined_files.astype(np.float32)\n",
    "\n",
    "#Now save anomaly for later use\n",
    "if region_name == 'CONUS':\n",
    "    location = 'Data/GLEAM'\n",
    "else:\n",
    "    location = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "combined_files.to_netcdf(f'{location}/RZSM_anomaly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af54dc1-ab3a-4d10-afa1-9b81b016884d",
   "metadata": {},
   "source": [
    "# Now restructure the observation data to the same lead-time format as GEFSv12. This will allow for an easier computation of anomalies using climpred functions.\n",
    "\n",
    "### New indexes will be from range (-84 days to -1 day and then the typical 7-day lead times starting from 0 to 34). Because we have applied a 7-day rolling mean to observations so far, even index -1 for the day has the data from the previous 7-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35812da6-4e93-4db3-b0d2-6c23269410bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 246\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# # init_date_list.reverse()\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _date  \u001b[38;5;129;01min\u001b[39;00m init_date_list:\n\u001b[0;32m--> 246\u001b[0m     \u001b[43mconvert_OBS_to_SubX_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# if __name__ == '__main__':\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m#     p = Pool(5)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m#     p.map(convert_OBS_to_SubX_format,init_date_list)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 58\u001b[0m, in \u001b[0;36mconvert_OBS_to_SubX_format\u001b[0;34m(_date)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m region_name \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCONUS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     57\u001b[0m     open_date_SubX \u001b[38;5;241m=\u001b[39m template\n\u001b[0;32m---> 58\u001b[0m     open_date_SubX \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_date_SubX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m var_list:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     model_name, obs_file, output_OBSERVATIONS_reformat_dir, save_dir \u001b[38;5;241m=\u001b[39m return_info_for_processing(var)     \n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/common.py:1853\u001b[0m, in \u001b[0;36mzeros_like\u001b[0;34m(other, dtype, chunks, chunked_array_type, from_array_kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros_like\u001b[39m(\n\u001b[1;32m   1782\u001b[0m     other: Dataset \u001b[38;5;241m|\u001b[39m DataArray \u001b[38;5;241m|\u001b[39m Variable,\n\u001b[1;32m   1783\u001b[0m     dtype: DTypeMaybeMapping \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1787\u001b[0m     from_array_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1788\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset \u001b[38;5;241m|\u001b[39m DataArray \u001b[38;5;241m|\u001b[39m Variable:\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new object of zeros with the same shape and\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;124;03m    type as a given dataarray or dataset.\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1851\u001b[0m \n\u001b[1;32m   1852\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfull_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1860\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/common.py:1639\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(other, fill_value, dtype, chunks, chunked_array_type, from_array_kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1637\u001b[0m         dtype_ \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m-> 1639\u001b[0m     data_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1640\u001b[0m         k: _full_like_variable(\n\u001b[1;32m   1641\u001b[0m             v\u001b[38;5;241m.\u001b[39mvariable,\n\u001b[1;32m   1642\u001b[0m             fill_value\u001b[38;5;241m.\u001b[39mget(k, dtypes\u001b[38;5;241m.\u001b[39mNA),\n\u001b[1;32m   1643\u001b[0m             dtype_\u001b[38;5;241m.\u001b[39mget(k, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1644\u001b[0m             chunks,\n\u001b[1;32m   1645\u001b[0m             chunked_array_type,\n\u001b[1;32m   1646\u001b[0m             from_array_kwargs,\n\u001b[1;32m   1647\u001b[0m         )\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata_vars\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1649\u001b[0m     }\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(data_vars, coords\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mcoords, attrs\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, DataArray):\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/common.py:1640\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1637\u001b[0m         dtype_ \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m   1639\u001b[0m     data_vars \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m-> 1640\u001b[0m         k: \u001b[43m_full_like_variable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1641\u001b[0m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1642\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNA\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1643\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1645\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunked_array_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfrom_array_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1648\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m other\u001b[38;5;241m.\u001b[39mdata_vars\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1649\u001b[0m     }\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(data_vars, coords\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mcoords, attrs\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mattrs)\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, DataArray):\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/common.py:1716\u001b[0m, in \u001b[0;36m_full_like_variable\u001b[0;34m(other, fill_value, dtype, chunks, chunked_array_type, from_array_kwargs)\u001b[0m\n\u001b[1;32m   1708\u001b[0m     data \u001b[38;5;241m=\u001b[39m chunkmanager\u001b[38;5;241m.\u001b[39marray_api\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m   1709\u001b[0m         other\u001b[38;5;241m.\u001b[39mshape,\n\u001b[1;32m   1710\u001b[0m         fill_value,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1713\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfrom_array_kwargs,\n\u001b[1;32m   1714\u001b[0m     )\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1716\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Variable(dims\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mdims, data\u001b[38;5;241m=\u001b[39mdata, attrs\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mattrs)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/numpy/core/numeric.py:423\u001b[0m, in \u001b[0;36mfull_like\u001b[0;34m(a, fill_value, dtype, order, subok, shape)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03mReturn a full array with the same shape and type as a given array.\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m        [  0,   0, 255]]])\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    422\u001b[0m res \u001b[38;5;241m=\u001b[39m empty_like(a, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder, subok\u001b[38;5;241m=\u001b[39msubok, shape\u001b[38;5;241m=\u001b[39mshape)\n\u001b[0;32m--> 423\u001b[0m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mcopyto\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def return_info_for_processing(var: str) -> Tuple[str, xr.DataArray]:\n",
    "    #Names to save the data so that it is in the exact same format as the Reforecasts\n",
    "    \n",
    "    if var == 'soilw_bgrnd':\n",
    "        # var_name = var #var_name is the actual name of the variable in the xarray file\n",
    "        model_name = 'GLEAM' #source of the data\n",
    "        obs_file = RZSM_obs_GLEAM #the actual data file\n",
    "    elif var == 'tmax_2m':\n",
    "        # var_name = 'tasmax'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = max_temp_ERA\n",
    "    elif var == 'spfh_2m':\n",
    "        # var_name = 'huss'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = spfh_ERA\n",
    "    elif var == 'pwat_eatm':\n",
    "        # var_name = 'precipitable_water'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = precipitable_water_ERA\n",
    "    elif var == 'diff_temp_2m':\n",
    "        # var_name = 'diff_tmax_tmin'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = diff_temp_ERA\n",
    "    elif var == 'hgt_pres':\n",
    "        # var_name = 'z'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = geopotential_z200_ERA\n",
    "    \n",
    "    output_OBSERVATIONS_reformat_dir = f'Data/{model_name}/reformat_to_reforecast_shape/{region_name}' #Directory to save \n",
    "    save_dir = f'{output_OBSERVATIONS_reformat_dir}/{var}'\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    return(model_name,obs_file,output_OBSERVATIONS_reformat_dir,save_dir)\n",
    "\n",
    "\n",
    "\n",
    "def convert_OBS_to_SubX_format(_date: str) -> None:  \n",
    "# for _date in init_date_list:\n",
    "    # var='RZSM_weighted'\n",
    "    # _date=init_date_list[0]\n",
    "\n",
    "    \n",
    "    '''We are going to create new leads that are different than reforecast. The reasoning for this is that we want the actual weekly lags (and 1 day lag) and this will\n",
    "    assist with future predictions within the deep learning model'''\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        #Currently the GLEAM and ERA observations are in a format with only positive values for the longitude.\n",
    "        #The current reforecast has negative values for those West of 0 degrees\n",
    "        \n",
    "        new_X_coords = [i+360 if i < 0 else i for i in template.X.values]\n",
    "        single_file = template.assign_coords({'X':new_X_coords})\n",
    "        single_file = xr.zeros_like(single_file)\n",
    "        open_date_SubX = putils.restrict_to_bounding_box(single_file,mask)\n",
    "        \n",
    "    elif region_name != 'CONUS':\n",
    "        open_date_SubX = template\n",
    "        open_date_SubX = xr.zeros_like(open_date_SubX)\n",
    "    \n",
    "    for var in var_list:\n",
    "        # break\n",
    "        model_name, obs_file, output_OBSERVATIONS_reformat_dir, save_dir = return_info_for_processing(var)     \n",
    "        \n",
    "        obs_file_name = f'{var}_reformat_{reforecast_input}_{_date}.nc4'\n",
    "        save_file = f'{save_dir}/{obs_file_name}'\n",
    "        \n",
    "        # reforecast_input\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "        # if os.path.exists('redo000000.nc'):\n",
    "            # print(f'Completed date {_date}')\n",
    "            pass\n",
    "        else:\n",
    "            print(f'Working on variable {var} for date {_date}. Saving into dir {save_dir}')\n",
    "            out_file = open_date_SubX.copy(deep=True)\n",
    "    \n",
    "            '''We are going to create a new lead day that represents the previous day before the forecast was initialized\n",
    "            #New shape will be (1x11x48xlatxlon)\n",
    "            This will include the day lag 1, and weekly lags 1-12'''\n",
    "            \n",
    "            file_shape = out_file[putils.xarray_varname(out_file)].shape\n",
    "            \n",
    "            named_str_leads = [str(i) for i in np.arange(-1,open_date_SubX.L.shape[0])]\n",
    "            new_shape = np.empty(shape=(1,file_shape[1],file_shape[2]+13,file_shape[3],file_shape[4]))\n",
    "            new_shape.shape\n",
    "            \n",
    "            daily_lead_lags = daily_lags + list(out_file.L.values[:])\n",
    "            \n",
    "            new_lead_days = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    test = (['S','M','L','Y','X'], new_shape[:,:,:,:,:]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    S = np.atleast_1d(_date),\n",
    "                    X = open_date_SubX.X.values,\n",
    "                    Y = open_date_SubX.Y.values,\n",
    "                    L = daily_lead_lags,\n",
    "                    M = open_date_SubX.M.values,\n",
    "    \n",
    "                ),\n",
    "                attrs = dict(\n",
    "                    Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                    cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "            )   \n",
    "            \n",
    "            #Create a file to overwrite\n",
    "            out_file = xr.zeros_like(new_lead_days)\n",
    "    \n",
    "            # print(f'Working on initialized day {_date} to find values integrating with SubX models, leads, & coordinates and saving data into {save_dir}.')\n",
    "            \n",
    "            for idx,i_lead in enumerate(new_lead_days.L.values):\n",
    "                # break\n",
    "    \n",
    "                date_val = pd.to_datetime(pd.to_datetime(_date) + dt.timedelta(days=int(i_lead)+0)) #Adding +1 may be suitable for other forecasts which predict the next day. But GEFSv12 predicts lead 0 as 12 UTC on the same date it is initialized\n",
    "                #But be careful if you adapt this code to a new script. We are looking backwards in time from the first date.\n",
    "                    \n",
    "                date_val = f'{date_val.year}-{date_val.month:02}-{date_val.day:02}'\n",
    "    \n",
    "                out_file[putils.xarray_varname(out_file)][0,:, idx, :, :] = \\\n",
    "                    obs_file[putils.xarray_varname(obs_file)].sel(time = date_val).values\n",
    "    \n",
    "    \n",
    "            if var == 'soilw_bgrnd':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        RZSM = (['S','M','L','Y','X'],    out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )                    \n",
    "            elif var == 'tmax_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        tmax = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'spfh_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        spfh_2m = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'pwat_eatm':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        pwat = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )      \n",
    "            elif var == 'diff_temp_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        diff_tmax_tmin = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            elif var == 'hgt_pres':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        z200 = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            var_OUT = var_OUT.astype(np.float32) #all the files need to be in float32 for the deep learning algorithm to work best\n",
    "            \n",
    "            #Save as a netcdf for later processing\n",
    "            var_OUT.to_netcdf(path = save_file, mode ='w')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "#template for overwriting\n",
    "ref_dir = f'{fcst_dir}/soilw_bgrnd' #Just use a single reference directory to serve as the template for file creation\n",
    "\n",
    "global template\n",
    "#Grab a single SubX to use as the template. Doesn't matter if it is the same variable or not or the same date\n",
    "\n",
    "template = sorted(glob(f'{ref_dir}/*.n*'))[0]\n",
    "template = xr.open_dataset(template)\n",
    "\n",
    "# # init_date_list.reverse()\n",
    "for _date  in init_date_list:\n",
    "    convert_OBS_to_SubX_format(_date)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p = Pool(5)\n",
    "#     p.map(convert_OBS_to_SubX_format,init_date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0b05a-0cad-435a-b67b-d5b08427c9a8",
   "metadata": {},
   "source": [
    "# Now process all the observations including:\n",
    "### 1.) Create anomaly\n",
    "### 2.) Plot distribution after anomaly\n",
    "### 3.) Create min-max standardization\n",
    "### 4.) Plot distribution after min-max standardization\n",
    "### 5.) Stack verification file (for RZSM) into a seperate directory (as a pickle file)\n",
    "### 6.) Stack all the different lags for all variables into seperate directory (as a pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d6618f5-cc10-483d-a630-ddc197f251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_observations_only(obs_source: str, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                    return_obs_minmax: bool, fcst_dir: str, rerun_function: bool, residual_min_max) -> None:\n",
    "\n",
    "\n",
    "    #Args for creating anomaly (testing)\n",
    "    # obs_source = 'GLEAM'\n",
    "    # region_name = region_name\n",
    "    # var_name= 'soilw_bgrnd'\n",
    "    # obs_or_forecast='obs' \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "        #7-day rolling mean has already been applied to observations\n",
    "\n",
    "        #Create the seasonal mean using climpred functions\n",
    "        file_locations = f'Data/{obs_source}/reformat_to_reforecast_shape/{region_name}/{var_name}/*{reforecast_input}*.n*'\n",
    "\n",
    "        print(f'\\nRetrieving data from {file_locations}\\n')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(xr.open_mfdataset(file_locations),train_end=train_end)\n",
    "        \n",
    "        print(f'\\nCreated seasonal anomalies on all {anom.L.values} lead days')\n",
    "        \n",
    "        print('\\n7-day rolling mean not being applied. We already did this when creating the data.')\n",
    "\n",
    "        anom = anom.load()\n",
    "        \n",
    "        anom_mean_subset = anom.sel(L=lead_select) #Make a small subset of the data\n",
    "\n",
    "\n",
    "        if var_source_combined == verification_var:\n",
    "            #Save the anomaly files\n",
    "            print(f'Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads {anom_mean_subset.L.values}')\n",
    "            putils.convert_OBS_anomaly_to_SubX_format(init_date_list = init_date_list, region_name = region_name, anomaly_file = anom_mean_subset, fcst_dir = fcst_dir, lead_select = lead_select, )\n",
    "            \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('\\nLooking at what the first and last dates are within the anomaly file.')\n",
    "        print_dates(anom_mean_subset)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('\\nPlotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'anomaly', region_name = region_name,\n",
    "                                                       lead_select=lead_select)\n",
    "\n",
    "        if residual_min_max == True:\n",
    "            #Min max standardize\n",
    "            min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, obs_min_max = None,\n",
    "                                                                lead_select = lead_select)\n",
    "            return(min_max)\n",
    "        \n",
    "        else:\n",
    "            #Min max standardize\n",
    "            min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, obs_min_max = None,\n",
    "                                                                lead_select = lead_select)\n",
    "            \n",
    "            #Plot min max distribution\n",
    "            if plot_distribution:\n",
    "                print('\\nPlotting min max distribution by lead')\n",
    "                putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "                                                           lead_select=lead_select)\n",
    "        \n",
    "            if var_source_combined == verification_var:\n",
    "                #We only need to save this as the verification file for deep learning model\n",
    "                putils.create_stacked_files_by_lead_for_verification(file = min_max, train_end=train_end, val_end=val_end, test_start=test_start, variable=var_source_combined,\n",
    "                                                              obs_or_forecast=obs_or_forecast, region_name = region_name, init_date_list = init_date_list,\n",
    "                                                                    lead_select=lead_select)\n",
    "        \n",
    "            #Now create stacked files for each individual variable for each lead time\n",
    "            putils.retrieve_stacked_files_and_save_for_model_inputs(file = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                                                    val_end = val_end, test_start = test_start, init_date_list = init_date_list)\n",
    "    \n",
    "            if return_obs_minmax == False:\n",
    "                return(f'Completed variable {var_source_combined}')\n",
    "            else:\n",
    "                return(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d01fdc-5a08-4b64-9803-f2c3b3203dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from Data/GLEAM/reformat_to_reforecast_shape/china/soilw_bgrnd/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads [ 6 13 20 27 34]\n",
      "\n",
      "Adding data within function convert_OBS_anomaly_to_SubX_format to file for leads [6, 13, 20, 27, 34]\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for soilw_bgrnd_GLEAM. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.251941978931427 for lead -84.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.20311447978019714 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45505645871162415.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2520337998867035 for lead -77.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.20096305012702942 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4529968500137329.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2532241940498352 for lead -70.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.21372577548027039 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4669499695301056.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2556397318840027 for lead -63.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1917486935853958 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4473884254693985.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25811767578125 for lead -56.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19685189425945282 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4549695700407028.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.26119309663772583 for lead -49.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1921233832836151 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45331647992134094.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.26459330320358276 for lead -42.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19107486307621002 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4556681662797928.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2677640914916992 for lead -35.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.20050597190856934 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.46827006340026855.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.26990213990211487 for lead -28.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19024035334587097 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.46014249324798584.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25784146785736084 for lead -21.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.2003559172153473 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45819738507270813.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.25420081615448 for lead -14.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.2095370888710022 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4637379050254822.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2532687187194824 for lead -7.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18708980083465576 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4403585195541382.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.247810497879982 for lead -1.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19003009796142578 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4378405958414078.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24731333553791046 for lead 6.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1992965191602707 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.44660985469818115.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24743781983852386 for lead 13.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.2077101618051529 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45514798164367676.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24828100204467773 for lead 20.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.2173357754945755 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.46561677753925323.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2506116032600403 for lead 27.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19097062945365906 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.44158223271369934.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2614518105983734 for lead 34.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19444124400615692 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45589305460453033.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GLEAM: 0\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GLEAM: 73520194\n",
      "\n",
      "Working on file soilw_bgrnd_GLEAM to save as verification observation data. This takes a while because data is not loaded into memory\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on validation data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on testing data for soilw_bgrnd_GLEAM\n",
      "Saved soilw_bgrnd_GLEAM data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/china/tmax_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for tmax_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 19.4044189453125 for lead -84.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.04510498046875 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 44.44952392578125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.6424560546875 for lead -77.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.368743896484375 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 47.011199951171875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.34869384765625 for lead -70.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -29.263702392578125 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 49.612396240234375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.225494384765625 for lead -63.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -27.61053466796875 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.836029052734375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.87921142578125 for lead -56.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.824493408203125 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.703704833984375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.620025634765625 for lead -49.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.175506591796875 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 44.7955322265625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.56768798828125 for lead -42.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -23.312744140625 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 44.88043212890625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.23370361328125 for lead -35.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.911865234375 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 47.14556884765625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.941162109375 for lead -28.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.5146484375 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.455810546875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.164459228515625 for lead -21.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.167633056640625 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 46.33209228515625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 19.18603515625 for lead -14.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.82281494140625 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 44.00885009765625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.486328125 for lead -7.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.1650390625 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 46.6513671875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 18.1845703125 for lead -1.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.689697265625 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 43.874267578125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 19.314178466796875 for lead 6.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.875579833984375 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 44.18975830078125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.860626220703125 for lead 13.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.799530029296875 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 46.66015625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.65020751953125 for lead 20.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -28.1300048828125 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.78021240234375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.0308837890625 for lead 27.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.4757080078125 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 47.506591796875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.26495361328125 for lead 34.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.607452392578125 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 46.872406005859375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tmax_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling tmax_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for tmax_2m_ERA5\n",
      "\n",
      "Working on validation data for tmax_2m_ERA5\n",
      "\n",
      "Working on testing data for tmax_2m_ERA5\n",
      "Saved tmax_2m_ERA5 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/china/spfh_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for spfh_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011581039987504482 for lead -84.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010980190709233284 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022561230696737766.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011154280975461006 for lead -77.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010983424261212349 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022137705236673355.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011400874704122543 for lead -70.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011268328875303268 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022669203579425812.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01093805767595768 for lead -63.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011065546423196793 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022003604099154472.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010577935725450516 for lead -56.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010219280607998371 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020797216333448887.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01005428098142147 for lead -49.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011837049387395382 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021891330368816853.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.009556612931191921 for lead -42.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.01123873796314001 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020795350894331932.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010149199515581131 for lead -35.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011590882204473019 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02174008172005415.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.00966207031160593 for lead -28.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011211978271603584 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020874048583209515.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010361377149820328 for lead -21.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011065012775361538 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021426389925181866.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.009681418538093567 for lead -14.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011698060669004917 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021379479207098484.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01017298549413681 for lead -7.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.012252158485352993 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022425143979489803.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010514612309634686 for lead -1.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.012757141143083572 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.023271753452718258.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01139462273567915 for lead 6.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010707318782806396 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022101941518485546.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011400117538869381 for lead 13.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010904114693403244 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022304232232272625.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01122407428920269 for lead 20.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011091507971286774 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022315582260489464.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010571349412202835 for lead 27.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.01090780645608902 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021479155868291855.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010732805356383324 for lead 34.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010026244446635246 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02075904980301857.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for spfh_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling spfh_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for spfh_2m_ERA5\n",
      "\n",
      "Working on validation data for spfh_2m_ERA5\n",
      "\n",
      "Working on testing data for spfh_2m_ERA5\n",
      "Saved spfh_2m_ERA5 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/china/pwat_eatm/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for pwat_eatm_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 41.452117919921875 for lead -84.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -37.855995178222656 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 79.30811309814453.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 42.361671447753906 for lead -77.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.592063903808594 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 80.9537353515625.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 43.06083297729492 for lead -70.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.63011932373047 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 81.69095230102539.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 40.27451705932617 for lead -63.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -33.40947341918945 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 73.68399047851562.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 39.46964645385742 for lead -56.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -37.26795959472656 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 76.73760604858398.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 36.835777282714844 for lead -49.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -40.45869827270508 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 77.29447555541992.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 36.9803466796875 for lead -42.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -37.86674880981445 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 74.84709548950195.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 33.700836181640625 for lead -35.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.54927444458008 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 72.2501106262207.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.481178283691406 for lead -28.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.675174713134766 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 73.15635299682617.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.648948669433594 for lead -21.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -39.90686798095703 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 75.55581665039062.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.8500862121582 for lead -14.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.80910873413086 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 74.65919494628906.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 37.973323822021484 for lead -7.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -39.065032958984375 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 77.03835678100586.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 38.43086242675781 for lead -1.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -38.79943084716797 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 77.23029327392578.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 41.58083724975586 for lead 6.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -39.01628494262695 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 80.59712219238281.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 41.85584259033203 for lead 13.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -39.87109375 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 81.72693634033203.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 43.41419219970703 for lead 20.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -40.11863327026367 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 83.5328254699707.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 40.57581329345703 for lead 27.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -33.208316802978516 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 73.78413009643555.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 38.51527404785156 for lead 34.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -35.30298614501953 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 73.8182601928711.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for pwat_eatm_ERA5: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for pwat_eatm_ERA5\n",
      "\n",
      "Working on validation data for pwat_eatm_ERA5\n",
      "\n",
      "Working on testing data for pwat_eatm_ERA5\n",
      "Saved pwat_eatm_ERA5 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/china/diff_temp_2m/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for diff_temp_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 15.261430740356445 for lead -84.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.742502212524414 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 25.00393295288086.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.13841724395752 for lead -77.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.367335319519043 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 23.505752563476562.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.297625541687012 for lead -70.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.77090072631836 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.06852626800537.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.379908561706543 for lead -63.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.111741065979004 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.491649627685547.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.391180992126465 for lead -56.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.65554428100586 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 25.046725273132324.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 13.657276153564453 for lead -49.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.015458106994629 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.672734260559082.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 15.668036460876465 for lead -42.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.59482192993164 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 26.262858390808105.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 15.236608505249023 for lead -35.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.831947326660156 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 26.06855583190918.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.83665657043457 for lead -28.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.80787467956543 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.64453125.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.4142427444458 for lead -21.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -9.873085021972656 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.287327766418457.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.317705154418945 for lead -14.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.134468078613281 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.452173233032227.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 14.967317581176758 for lead -7.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.00365924835205 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 25.97097682952881.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 15.549842834472656 for lead -1.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.851778984069824 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 26.40162181854248.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 16.13375473022461 for lead 6.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.108817100524902 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 26.24257183074951.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 13.59631061553955 for lead 13.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.050492286682129 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 23.64680290222168.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 13.140706062316895 for lead 20.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -10.533255577087402 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 23.673961639404297.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 12.941879272460938 for lead 27.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.098697662353516 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 24.040576934814453.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_ERA5 is 13.75040054321289 for lead 34.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_ERA5 is -11.648733139038086 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_ERA5 is 25.399133682250977.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling diff_temp_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for diff_temp_2m_ERA5\n",
      "\n",
      "Working on validation data for diff_temp_2m_ERA5\n",
      "\n",
      "Working on testing data for diff_temp_2m_ERA5\n",
      "Saved diff_temp_2m_ERA5 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/china/hgt_pres/*GEFSv12*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for hgt_pres_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5591.4375 for lead -84.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5231.4609375 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10822.8984375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5504.0234375 for lead -77.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5023.59375 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10527.6171875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5924.1875 for lead -70.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4498.34375 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10422.53125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5593.515625 for lead -63.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4420.4921875 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10014.0078125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5819.7265625 for lead -56.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4583.2578125 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10402.984375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4448.65625 for lead -49.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4883.921875 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 9332.578125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4903.65625 for lead -42.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4631.234375 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 9534.890625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4808.15625 for lead -35.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5077.9453125 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 9886.1015625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4714.75 for lead -28.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5669.0234375 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10383.7734375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4817.1328125 for lead -21.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5202.25 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10019.3828125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5310.3671875 for lead -14.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5388.90625 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10699.2734375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 4561.7734375 for lead -7.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5052.7109375 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 9614.484375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5002.7734375 for lead -1.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4832.796875 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 9835.5703125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5506.328125 for lead 6.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5080.484375 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10586.8125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5788.2421875 for lead 13.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5576.6953125 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 11364.9375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 6277.4296875 for lead 20.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -5051.171875 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 11328.6015625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5604.390625 for lead 27.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4888.65625 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10493.046875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_ERA5 is 5790.578125 for lead 34.\n",
      "\n",
      "Minimum value for variable hgt_pres_ERA5 is -4503.109375 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_ERA5 is 10293.6875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for hgt_pres_ERA5: 0\n",
      "Number of 0 values after min max scaling hgt_pres_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for hgt_pres_ERA5\n",
      "\n",
      "Working on validation data for hgt_pres_ERA5\n",
      "\n",
      "Working on testing data for hgt_pres_ERA5\n",
      "Saved hgt_pres_ERA5 data into Data/model_npy_inputs/china/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rerun_function = True\n",
    "\n",
    "################### OBSERVATIONS ###########################\n",
    "\n",
    "obs_RZSM_minmax = anomaly_plot_standardize_save_observations_only(obs_source= 'GLEAM', region_name = region_name, var_name='soilw_bgrnd', train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = True, fcst_dir = fcst_dir,\n",
    "                                                                 rerun_function = rerun_function, residual_min_max=False)\n",
    "\n",
    "for var_name in var_list:\n",
    "    if 'soil' in var_name:\n",
    "        pass\n",
    "    else:\n",
    "        anomaly_plot_standardize_save_observations_only(obs_source= 'ERA5', region_name = region_name, var_name=var_name, train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                        obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = False, fcst_dir = fcst_dir,\n",
    "                                                       rerun_function = rerun_function, residual_min_max=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fddf36a-6343-405a-8079-b2387f36edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "# print(f'\\nLoading files for variable {name_of_var}')\n",
    "\n",
    "# file_paths = sorted(glob(f'{dir_path}/{name_of_var}/*.n*'))\n",
    "\n",
    "# # Open individual xarray datasets from the files\n",
    "# datasets = [xr.open_dataset(file) for file in file_paths]\n",
    "\n",
    "# #We have different coordinate systems. So we need to add 360 to each of the X coordinates if they are negative\n",
    "# if region_name == 'CONUS':\n",
    "#     print(f'We are changing the coordinates of CONUS to match similar format as GLEAM')\n",
    "#     new_X_coords = [i+360 if i < 0 else i for i in datasets[0].X.values]\n",
    "#     #Add the new coordinates\n",
    "#     datasets = [file.assign_coords({'X':new_X_coords}) for file in datasets]\n",
    "#     datasets = [putils.restrict_to_bounding_box(file,mask) for file in datasets]\n",
    "    \n",
    "# # Concatenate the datasets along a specific dimension\n",
    "# print(f'\\nConcatenating files for {name_of_var}. Takes about 10 minutes for CONUS.')\n",
    "# file = xr.concat(datasets, dim='S', combine_attrs=\"override\")\n",
    "\n",
    "# putils._count_zero_values(file, f'{name_of_var}_GEFSv12')\n",
    "# # file = dask.delayed(xr.concat)(datasets, dim=\"S\")\n",
    "\n",
    "# # Close individual datasets to free up resources\n",
    "# for dataset in datasets:\n",
    "#     dataset.close()\n",
    "\n",
    "# # Some files have a value of zero which is skewing results. So we need to replace with the mean.\n",
    "# print('Masking files if they have value of zero')\n",
    "# file_masked = file.where(file == 0, np.nan, file)\n",
    "# #Now take the mean \n",
    "# file_masked_mean = file_masked.mean(dim='S')\n",
    "# #Now replace the values where there was a zero\n",
    "# file = xr.where(np.isnan(file), file_masked_mean, file)\n",
    "\n",
    "# _count_np_nan_values(file, f'{name_of_var}_GEFSv12')\n",
    "# _count_zero_values(file, f'{name_of_var}_GEFSv12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad12ffb-6b77-4e23-9970-c7729c4c52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_reforecast(obs_source, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                  obs_min_max: xr.DataArray, rerun_function: bool, residual_min_max: bool) -> None:\n",
    "    # #Testing\n",
    "    # obs_source= 'GEFSv12'\n",
    "    # region_name = region_name\n",
    "    # var_name='tmax_2m'\n",
    "    # train_end = train_end\n",
    "    # val_end = val_end\n",
    "    # test_start = test_start\n",
    "    # obs_or_forecast = 'reforecast'\n",
    "\n",
    "    \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "\n",
    "        file = putils.return_reforecast_files_by_concatenation(dir_path = fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "\n",
    "        if var_name == 'hgt_pres':\n",
    "            file = file* 9.80665 #must place in the same unit as ERA5. the unit is gravity\n",
    "\n",
    "        #Testing if rolling mean first fixes anything. Currenlty the hgt_pres values are way too low for the min\n",
    "\n",
    "        file_test_full = file.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        file_test_small = file_test_full.sel(L=lead_select)\n",
    "        \n",
    "        print(f'\\nCreating seasonal anomalies on all {file_test_small.L.values} lead days')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(file_test_small,train_end=train_end)\n",
    "        \n",
    "        anom_full, mean_season_full = putils.create_seasonal_anomaly(file_test_full,train_end=train_end)\n",
    "        # print('\\nNow taking the 7-day rolling mean of anomalies. We are not replacing lead day 0.')\n",
    "        # anom_mean = anom.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        # anom_mean[putils.xarray_varname(file)][:,:,0,:,:] = anom[putils.xarray_varname(anom)][:,:,0,:,:]\n",
    "        \n",
    "        anom = anom.load()\n",
    "        \n",
    "        print(f'\\nShape of anomaly mean = {anom[putils.xarray_varname(anom)].shape}')\n",
    "        \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('Looking at what dates are within the file')\n",
    "        print_dates(anom)\n",
    "    \n",
    "        #Plot anomaly distribution\n",
    "        if plot_distribution:\n",
    "            print('Plotting anomaly distribution by lead')\n",
    "            putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, \n",
    "                                                        anomaly_or_min_max = 'anomaly', region_name = region_name,lead_select=lead_select)\n",
    "\n",
    "        if residual_min_max == True:\n",
    "            putils.save_baseline_RZSM_anomaly(anom_full.load(),region_name, fcst_dir)\n",
    "            min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = obs_min_max, lead_select = lead_select)\n",
    "            return(min_max)\n",
    "        else:\n",
    "            #Min max standardize\n",
    "            if 'soilw_bgrnd' in var_source_combined:\n",
    "                putils.save_baseline_RZSM_anomaly(anom_full.load(),region_name, fcst_dir)\n",
    "                min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                         obs_min_max = obs_min_max, lead_select = lead_select)\n",
    "               \n",
    "            else:\n",
    "                min_max = putils.choose_training_years_and_min_max_scale(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                         obs_min_max = None, lead_select = lead_select)\n",
    "    \n",
    "            \n",
    "            \n",
    "            #Plot min max distribution\n",
    "            if plot_distribution:\n",
    "                print('Plotting min min distribution by lead')\n",
    "                putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "                                                           lead_select=lead_select)\n",
    "    \n",
    "            putils.stack_reforecasts(var_min_max = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                     val_end = val_end, test_start = test_start, lead_select=lead_select)\n",
    "\n",
    "            return(f'Completed variable {var_source_combined}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12858852-e5c8-43ed-bcef-71de866056a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First load tmax and tmin and then create diff_temp_2m (difference in temperature)\n",
    "def create_diff_temp_reforecast(region_name: str, delete=False) -> None:\n",
    "    if region_name == 'CONUS':\n",
    "        data_dir = f'Data/GEFSv12_reforecast'\n",
    "    else:\n",
    "        data_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "\n",
    "    save_dir = f'{data_dir}/diff_temp_2m'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    tmax_ = sorted(glob(f'{data_dir}/tmax_2m/*.n*'))\n",
    "    tmin_ = sorted(glob(f'{data_dir}/tmin_2m/*.n*'))\n",
    "\n",
    "    # #first open all the files and get the array values\n",
    "    # max_files = xr.open_mfdataset(tmax_)\n",
    "    # min_files = xr.open_mfdataset(tmin_)\n",
    "\n",
    "    for idx, file in enumerate(tmax_):\n",
    "        # break\n",
    "        max_ = xr.open_dataset(file)\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        # print(f'Max: {max_[putils.xarray_varname(max_)].max().values}')\n",
    "        \n",
    "        date_ = pd.to_datetime(max_.S.values[0])\n",
    "        save_file = f'{save_dir}/diff_temp_2m_{date_.year}-{date_.month:02}-{date_.day:02}.nc4'\n",
    "\n",
    "        if delete:\n",
    "            os.system(f'rm {save_file}')\n",
    "        \n",
    "        if os.path.exists(save_file):\n",
    "            pass\n",
    "        else:\n",
    "            min_ = xr.open_dataset(tmin_[idx])\n",
    "            # min_['S'] = pd.to_datetime(min_['S'].values)\n",
    "            # min_.to_netcdf(tmin_[idx])\n",
    "            # print(f'Min: {min_[putils.xarray_varname(min_)].min().values}')\n",
    "            assert max_.S.values == min_.S.values, 'Dates are not equal, something is wrong with either glob or there are missing files from either tmax or tmin'\n",
    "    \n",
    "            diff_temp = (max_.tasmax - min_.tasmin).to_dataset(name='diff_tmax_tmin')\n",
    "            \n",
    "            if region_name == 'china':\n",
    "                '''There is some error in the data when downloading from new source'''\n",
    "                diff_temp['diff_tmax_tmin'][:,:,-1,:,:] = diff_temp['diff_tmax_tmin'][:,:,-2,:,:]\n",
    "            \n",
    "            diff_temp.to_netcdf(save_file)\n",
    "    return(0)\n",
    "\n",
    "create_diff_temp_reforecast(region_name,delete=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc7d0c9-4836-49ec-808d-8a50dcd58aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable soilw_bgrnd\n",
      "\n",
      "Concatenating files for soilw_bgrnd. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GEFSv12: 506880\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GEFSv12: 0\n",
      "Loaded files for soilw_bgrnd\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for soilw_bgrnd_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.239866703748703 for lead 6.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.23404628038406372 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.4739129841327667.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.2697097659111023 for lead 13.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.25635045766830444 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.5260602235794067.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.29989859461784363 for lead 20.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.28134000301361084 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.5812385976314545.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.3044864535331726 for lead 27.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.29106563329696655 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.5955520868301392.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GEFSv12 is 0.3155444860458374 for lead 34.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GEFSv12 is -0.30395448207855225 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GEFSv12 is 0.6194989681243896.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GEFSv12: 0\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GEFSv12: 20507225\n",
      "\n",
      "Stacking together REFORECAST variable soilw_bgrnd_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved soilw_bgrnd_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable spfh_2m\n",
      "\n",
      "Concatenating files for spfh_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for spfh_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling spfh_2m_GEFSv12: 0\n",
      "Loaded files for spfh_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for spfh_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.012558527290821075 for lead 6.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.012674851343035698 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.025233378633856773.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.01349584385752678 for lead 13.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.013317571952939034 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.026813415810465813.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.012885335832834244 for lead 20.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.013213036581873894 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.026098372414708138.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.012834904715418816 for lead 27.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.013787195086479187 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.026622099801898003.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_GEFSv12 is 0.013296190649271011 for lead 34.\n",
      "\n",
      "Minimum value for variable spfh_2m_GEFSv12 is -0.014017538167536259 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_GEFSv12 is 0.02731372881680727.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for spfh_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling spfh_2m_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable spfh_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved spfh_2m_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable tmax_2m\n",
      "\n",
      "Concatenating files for tmax_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for tmax_2m_GEFSv12: 57512448\n",
      "Number of 0 values after min max scaling tmax_2m_GEFSv12: 0\n",
      "Loaded files for tmax_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for tmax_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 20.235107421875 for lead 6.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -25.9608154296875 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 46.1959228515625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 21.644561767578125 for lead 13.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -28.11480712890625 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 49.759368896484375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 23.4105224609375 for lead 20.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -30.174209594726562 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 53.58473205566406.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is 24.077362060546875 for lead 27.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is -29.852554321289062 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is 53.92991638183594.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_GEFSv12 is nan for lead 34.\n",
      "\n",
      "Minimum value for variable tmax_2m_GEFSv12 is nan for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_GEFSv12 is nan.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tmax_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling tmax_2m_GEFSv12: 53448196\n",
      "\n",
      "Stacking together REFORECAST variable tmax_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved tmax_2m_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable hgt_pres\n",
      "\n",
      "Concatenating files for hgt_pres. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 0\n",
      "Loaded files for hgt_pres\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for hgt_pres_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 5644.09375 for lead 6.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5302.2578125 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 10946.3515625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6272.734375 for lead 13.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5600.6953125 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 11873.4296875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6862.9765625 for lead 20.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5407.078125 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 12270.0546875.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 5844.625 for lead 27.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5793.2265625 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 11637.8515625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6534.03125 for lead 34.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5689.0546875 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 12223.0859375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable hgt_pres_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved hgt_pres_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable diff_temp_2m\n",
      "\n",
      "Concatenating files for diff_temp_2m. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_GEFSv12: 287354880\n",
      "Number of 0 values after min max scaling diff_temp_2m_GEFSv12: 3942\n",
      "Loaded files for diff_temp_2m\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for diff_temp_2m_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 9.75324821472168 for lead 6.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -7.839216709136963 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 17.592464923858643.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 9.107229232788086 for lead 13.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -7.934863090515137 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 17.042092323303223.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 8.747354507446289 for lead 20.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -8.404679298400879 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 17.152033805847168.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 9.690908432006836 for lead 27.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -8.776047706604004 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 18.46695613861084.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable diff_temp_2m_GEFSv12 is 9.47341537475586 for lead 34.\n",
      "\n",
      "Minimum value for variable diff_temp_2m_GEFSv12 is -8.87476921081543 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable diff_temp_2m_GEFSv12 is 18.34818458557129.\n",
      "\n",
      "Standardizing data with min max for diff_temp_2m_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for diff_temp_2m_GEFSv12: 0\n",
      "Number of 0 values after min max scaling diff_temp_2m_GEFSv12: 50411525\n",
      "\n",
      "Stacking together REFORECAST variable diff_temp_2m_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved diff_temp_2m_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n",
      "\n",
      "Latitude values for mask is [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Longitude values for mask is [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Loading files for variable pwat_eatm\n",
      "\n",
      "Concatenating files for pwat_eatm. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for pwat_eatm_GEFSv12: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_GEFSv12: 41\n",
      "Loaded files for pwat_eatm\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [ 74.5  75.   75.5  76.   76.5  77.   77.5  78.   78.5  79.   79.5  80.\n",
      "  80.5  81.   81.5  82.   82.5  83.   83.5  84.   84.5  85.   85.5  86.\n",
      "  86.5  87.   87.5  88.   88.5  89.   89.5  90.   90.5  91.   91.5  92.\n",
      "  92.5  93.   93.5  94.   94.5  95.   95.5  96.   96.5  97.   97.5  98.\n",
      "  98.5  99.   99.5 100.  100.5 101.  101.5 102.  102.5 103.  103.5 104.\n",
      " 104.5 105.  105.5 106.  106.5 107.  107.5 108.  108.5 109.  109.5 110.\n",
      " 110.5 111.  111.5 112.  112.5 113.  113.5 114.  114.5 115.  115.5 116.\n",
      " 116.5 117.  117.5 118.  118.5 119.  119.5 120.  120.5 121.  121.5 122. ]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [44.  43.5 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5\n",
      " 37.  36.5 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5\n",
      " 30.  29.5 29.  28.5 28.  27.5 27.  26.5 26.  25.5 25.  24.5 24.  23.5\n",
      " 23.  22.5 22.  21.5 21.  20.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for pwat_eatm_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 44.66631317138672 for lead 6.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -40.29766845703125 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 84.96398162841797.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 48.037288665771484 for lead 13.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -39.89405822753906 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 87.93134689331055.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 46.7822265625 for lead 20.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -39.12409973144531 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 85.90632629394531.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 48.859432220458984 for lead 27.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -39.33820724487305 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 88.19763946533203.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_GEFSv12 is 45.37394714355469 for lead 34.\n",
      "\n",
      "Minimum value for variable pwat_eatm_GEFSv12 is -41.44138717651367 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_GEFSv12 is 86.81533432006836.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for pwat_eatm_GEFSv12: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable pwat_eatm_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved pwat_eatm_GEFSv12 data into Data/model_npy_inputs/china/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "rerun_function = True\n",
    "\n",
    "if reforecast_input == 'GEFSv12':\n",
    "    for var_name in ['soilw_bgrnd', 'spfh_2m', 'tmax_2m', 'hgt_pres', 'diff_temp_2m', 'pwat_eatm',]:\n",
    "        if var_name == 'soilw_bgrnd':\n",
    "            obs_min_max = obs_RZSM_minmax\n",
    "        else:\n",
    "            obs_min_max = None\n",
    "        anomaly_plot_standardize_save_reforecast(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                      val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                      obs_min_max = obs_min_max, rerun_function=rerun_function,residual_min_max = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f7ed4-0e45-41b4-babc-08ba215fd9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a539e4-0f54-4967-b5b2-95919578730b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4031500-9b73-4418-ae0d-6055029439e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e323e1-a4f3-4ce6-8176-efdfe5218712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07db4ac7-7179-4c72-b745-7d89d325e08a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985201bf-8dd8-4523-afd4-0e8fdc1b0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "rerun_function = True\n",
    "\n",
    "for var_name in ['soilw_bgrnd']:\n",
    "    obs_min_max = obs_RZSM_minmax\n",
    "    gefs_min_max = anomaly_plot_standardize_save_reforecast(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                  val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                  obs_min_max = obs_min_max, rerun_function=rerun_function, residual_min_max=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5007e4-4b3f-452f-b5c0-30bbb2c443ca",
   "metadata": {},
   "source": [
    "# Now make the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8a71c-bff6-4888-9813-b6219cca0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_residuals():\n",
    "    '''Only return the residuals of the soil moisture from GEFSv12\n",
    "\n",
    "    We are going to subtract (OBS - REForecast)'''\n",
    "    #First open the anomaly files for observation\n",
    "    obs_ANOM = xr.open_mfdataset(f'Data/GLEAM/RZSM_anomaly_reformat_SubX_format/{region_name}/*.n*').sel(L=lead_select)\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        ref_ANOM = xr.open_mfdataset(f'Data/GEFSv12_reforecast/soilw_bgrnd/baseline_RZSM_anomaly/*.nc').sel(L=lead_select)\n",
    "\n",
    "    residual_ANOM = obs_ANOM - ref_ANOM\n",
    "    \n",
    "    save_dir_GEFS = 'Data/GEFSv12_reforecast/soilw_bgrnd/residuals_RZSM_min_max'\n",
    "\n",
    "    print('Now saving the residuals of the anomalies')\n",
    "    putils.save_residuals(residual_ANOM.load(),region_name, fcst_dir)\n",
    "\n",
    "    residual_ANOM.max()\n",
    "\n",
    "    min_max = putils.choose_training_years_and_min_max_scale(file = residual_ANOM, train_end = train_end, variable = 'residual_GEFSv12', obs_or_forecast = 'reforecast', region_name = region_name, obs_min_max = None,\n",
    "                                                    lead_select = lead_select)\n",
    "    \n",
    "    #Now we need to save the residuals to not have to re-do over and over again\n",
    "    putils.save_residuals_min_max_RZSM_OBS_minus_GEFS(out_file, region_name, fcst_dir)\n",
    "\n",
    "    #We only need to save this as the verification file for deep learning model\n",
    "    putils.create_stacked_files_by_lead_for_verification_residual_min_max(file = out_file, train_end=train_end, val_end=val_end, test_start=test_start, variable='residual_GEFSv12',\n",
    "                                                  obs_or_forecast='reforecast', region_name = region_name, init_date_list = init_date_list,\n",
    "                                                        lead_select=lead_select)\n",
    "    \n",
    "    return('Completed making residuals and saving for Verification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0621d8-1972-4f6d-8e58-fb3b2e98e1bf",
   "metadata": {},
   "source": [
    "# Double check the units with precipitable water and surface specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b37468-bc7d-4af9-8359-45ef70a7e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precipitable water units\n",
    "#Reforecast (kg m-2, i.e., mm)   Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations (kg m-2)    Source; https://codes.ecmwf.int/grib/param-db/136 \n",
    "\n",
    "#check the scale of the data for pwat and spfh between forecasts and observations\n",
    "pwat_ref = xr.open_dataset('Data/GEFSv12_reforecast/pwat_eatm/precipitable_water_EMC_2000-01-05.nc4')\n",
    "pwat_obs = xr.open_dataset('Data/ERA5/reformat_to_reforecast_shape/CONUS/pwat_eatm/pwat_eatm_reformat_2000-01-05.nc4')\n",
    "\n",
    "pwat_ref.max()\n",
    "pwat_obs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33a018-0b72-4626-874d-ea50580df62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific humidity units\n",
    "#Reforecast (kg kg-1 dry air) Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations ('kg/kg'). I computed this using Metpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136936-2c05-4788-9d93-1645f74b7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check stacked files\n",
    "import pickle\n",
    "with open(f'/glade/work/klesinger/FD_RZSM_deep_learning/Data/model_npy_inputs/CONUS/Model_input_data/reforecast_tmax_2m_GEFSv12_training.pickle', 'rb') as handle:\n",
    "    a= pickle.load(handle)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745becd8-5f48-4432-8a19-cefb688ab4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if verification data is fine\n",
    "# output_npy_dir = f'Data/model_npy_inputs/{}Verification_data'\n",
    "# from glob import glob\n",
    "\n",
    "# for file in glob(f'{output_npy_dir}/OBS*'):\n",
    "#     print(file)\n",
    "#     print(np.load(file))\n",
    "\n",
    "##### The Data loads fine and fast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbd22a-841a-4a52-8185-b3190c61c681",
   "metadata": {},
   "source": [
    "# Make residuals dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34db4c3c-3d6b-4d70-a284-679196da97af",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_minmax = xr.open_mfdataset('Data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
