{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54b35f35-4977-4886-b246-ecd855a0ace0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-03 20:54:54.971585: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-03 20:54:57.726493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xarray as xr\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import keras as k\n",
    "from keras.layers import Conv2D, Input, AvgPool2D, MaxPool2D, Concatenate, Add, Dropout, BatchNormalization, Conv2DTranspose, Activation, DepthwiseConv2D, concatenate\n",
    "import keras_cv\n",
    "from tensorflow.keras.layers import MultiHeadAttention\n",
    "import keras\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb779b61-dfe7-4979-8c31-a7ddeb3b6c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_norm = None\n",
    "inputs = Input(shape=(48,96,21), name='input_image') \n",
    "output_channels=1\n",
    "\n",
    "dropout_rate_initial = 0.1\n",
    "dropout_rate_later = 0.25\n",
    "nb_filter = [32,64,128,256]\n",
    "num_heads =8\n",
    "num_transformer_loops = 6\n",
    "\n",
    "\n",
    "# Set image data format to channels first\n",
    "global bn_axis\n",
    "\n",
    "k.backend.set_image_data_format(\"channels_last\")\n",
    "bn_axis = -1\n",
    "\n",
    "kernel_initializer = 'glorot_uniform'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f594ddea-a2cd-495a-8859-8c8387ce94c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_batchnorm_relu_block_SE_and_residual_connections(input_tensor, nb_filter, dropout_rate, kernel_norm, kernel_size=3,kernel_initializer=kernel_initializer):\n",
    "    depth_multiplier=1\n",
    "    \n",
    "    shortcut = Conv2D(nb_filter, kernel_size=(1, 1), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    shortcut = Activation('relu')(shortcut)\n",
    "    \n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    # x = DepthwiseConv2D(3, padding ='same', depth_multiplier=depth_multiplier, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(x,training=True)\n",
    "    #x = Dropout(dropout_rate)(x, training =True) #add dropout \n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # x = Activation('relu')(x)\n",
    "    # x = Dropout(dropout_rate)(x, training =True) #add dropout \n",
    "    \n",
    "    residual_SE_block = Add()([shortcut, x])\n",
    "    residual_SE_block = keras_cv.layers.SqueezeAndExcite2D(residual_SE_block.shape[-1])(residual_SE_block)\n",
    "    \n",
    "    return residual_SE_block\n",
    "\n",
    "def inception_block(prevlayer, a, b,dropout_rate, kernel_norm, depth_multiplier = False):\n",
    "    #shortcut = Conv2D(a,kernel_size=(1, 1), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    #shortcut = BatchNormalization()(shortcut)\n",
    "    #shortcut = Activation('relu')(shortcut)\n",
    "\n",
    "    shortcut= prevlayer\n",
    "    \n",
    "    if depth_multiplier == True:\n",
    "        depth_multiplier=3\n",
    "    else:\n",
    "        depth_multiplier=2\n",
    "    \n",
    "    conva = DepthwiseConv2D(3, padding ='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    conva = BatchNormalization()(conva)\n",
    "    conva = tf.keras.activations.relu(conva)\n",
    "    conva = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(conva,training=True)\n",
    "    #conva = Dropout(dropout_rate)(conva, training =True) #add dropout\n",
    "    \n",
    "    conva = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conva)\n",
    "    conva = BatchNormalization()(conva)\n",
    "    #conva = tf.keras.activations.relu(conva)\n",
    "    #conva = Dropout(dropout_rate)(conva, training =True) #add dropout\n",
    "\n",
    "    convb = DepthwiseConv2D(5, padding ='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    convb = BatchNormalization()(convb)\n",
    "    convb = tf.keras.activations.relu(convb)\n",
    "    convb = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convb,training=True)\n",
    "    #convb = Dropout(dropout_rate)(convb, training =True) #add dropout\n",
    "    \n",
    "    convb = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convb)\n",
    "    convb = BatchNormalization()(convb)\n",
    "    #convb = tf.keras.activations.relu(convb)\n",
    "    #convb = Dropout(dropout_rate)(convb, training =True) #add dropout\n",
    "\n",
    "    convc = DepthwiseConv2D(7, padding='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    convc = BatchNormalization()(convc)\n",
    "    convc = tf.keras.activations.relu(convc)\n",
    "    convc = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convc,training=True)\n",
    "    #convc = Dropout(dropout_rate)(convc, training =True) #add dropout\n",
    "    \n",
    "    convc = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convc)\n",
    "    convc = BatchNormalization()(convc)\n",
    "    #convc = tf.keras.activations.relu(convc)\n",
    "    #convc = Dropout(dropout_rate)(convc, training =True) #add dropout\n",
    "\n",
    "    # if True == pooling:\n",
    "    #     convd = MaxPooling2D(pool_size=(2, 2))(convd)\n",
    "    \n",
    "    #Max pool\n",
    "    convd = MaxPool2D((5,5), strides=(1, 1), padding='same')(prevlayer)\n",
    "    convd = Conv2D(a,(1, 1), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convd)\n",
    "    convd = BatchNormalization()(convd)\n",
    "    convd = tf.keras.activations.relu(convd)\n",
    "    convd = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convd,training=True)\n",
    "    #onvd = Dropout(dropout_rate)(convd, training =True) #add dropout\n",
    "    \n",
    "    convd = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convd)\n",
    "    convd = BatchNormalization()(convd)\n",
    "    #convd = tf.keras.activations.relu(convd)\n",
    "    #convd = Dropout(dropout_rate)(convd, training =True) #add dropout\n",
    "\n",
    "    up = concatenate([conva, convb, convc, convd])\n",
    "    \n",
    "    #residual_block = Concatenate()([shortcut, up])\n",
    "    \n",
    "    # Adjust the number of channels in the shortcut connection\n",
    "    if shortcut.shape[-1] != up.shape[-1]:\n",
    "        shortcut = Conv2D(up.shape[-1], kernel_size=1, strides=1, padding='same')(shortcut)\n",
    "\n",
    "    residual_block = Add()([shortcut, up])\n",
    "\n",
    "    residual_SE_block = keras_cv.layers.SqueezeAndExcite2D(residual_block.shape[-1])(residual_block)\n",
    "    \n",
    "    return residual_SE_block\n",
    "\n",
    "\n",
    "def attention_gate(up, skip, num_filters, kernel_size=3, kernel_norm=None):\n",
    "    wg = Conv2D(num_filters, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(up)\n",
    "    wg = BatchNormalization()(wg)\n",
    "    \n",
    "    ws = Conv2D(num_filters, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(skip)\n",
    "    ws = BatchNormalization()(wg)\n",
    "\n",
    "    out = tf.keras.activations.relu(wg + ws)\n",
    "    out = Conv2D(num_filters, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(skip)\n",
    "    out = tf.keras.activations.relu(out)\n",
    "\n",
    "    if out.shape[-1] != skip.shape[-1]:\n",
    "        skip = Conv2D(out.shape[-1], kernel_size=1, strides=1, padding='same')(skip)\n",
    "    return(out*skip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27625815-507e-4737-8901-a56f816eeed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 24, 48, 256), (None, 48, 96, 128), (None, 48, 96, 256), (None, 48, 96, 128)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m up1_4 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mdepth_to_space(conv1_1, block_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     61\u001b[0m up1_4 \u001b[38;5;241m=\u001b[39m attention_gate(up1_4, conv2_1, num_filters\u001b[38;5;241m=\u001b[39mnb_filter[\u001b[38;5;241m3\u001b[39m], kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m conv1_4 \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mup1_4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconv1_3\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbn_axis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m conv1_4 \u001b[38;5;241m=\u001b[39m inception_block(conv1_4, nb_filter[\u001b[38;5;241m0\u001b[39m], nb_filter[\u001b[38;5;241m0\u001b[39m], dropout_rate \u001b[38;5;241m=\u001b[39m dropout_rate_later,depth_multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, kernel_norm \u001b[38;5;241m=\u001b[39mkernel_norm)\n\u001b[1;32m     64\u001b[0m conv1_4 \u001b[38;5;241m=\u001b[39m Conv2D(nb_filter[\u001b[38;5;241m2\u001b[39m],kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, strides\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39mkernel_initializer, kernel_constraint \u001b[38;5;241m=\u001b[39m kernel_norm)(conv1_4)\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/keras/layers/merging/concatenate.py:231\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.layers.concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcatenate\u001b[39m(inputs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Functional interface to the `Concatenate` layer.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    >>> x = np.arange(20).reshape(2, 2, 5)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m        A tensor, the concatenation of the inputs alongside axis `axis`.\u001b[39;00m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/keras/layers/merging/concatenate.py:131\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    125\u001b[0m unique_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m    126\u001b[0m     shape[axis]\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m )\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dims) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 24, 48, 256), (None, 48, 96, 128), (None, 48, 96, 256), (None, 48, 96, 128)]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "conv1_1 = inception_block(inputs, nb_filter[0], nb_filter[0],dropout_rate=dropout_rate_initial,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_1 = Conv2D(nb_filter[2],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv1_1)\n",
    "# conv1_1a = inception_block(conv1_1, nb_filter[0], nb_filter[0],dropout_rate=dropout_rate_initial,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "# conv1_1 = concatenate([conv1_1, conv1_1a])\n",
    "pool1 = MaxPool2D((2, 2), strides=(2, 2))(conv1_1)\n",
    "\n",
    "conv2_1 = inception_block(pool1, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv2_1 = Conv2D(nb_filter[2],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_1)\n",
    "# conv2_1a = inception_block(conv2_1, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "# conv2_1 = concatenate([conv2_1, conv2_1a])\n",
    "pool2 = MaxPool2D((2, 2), strides=(2, 2))(conv2_1)\n",
    "\n",
    "up1_2 = tf.nn.depth_to_space(conv2_1, block_size=2)\n",
    "up1_2 = attention_gate(up1_2, conv1_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv1_2 = concatenate([up1_2, conv1_1], axis=bn_axis)\n",
    "conv1_2 = inception_block(conv1_2, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_2 = Conv2D(nb_filter[3],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv1_2)\n",
    "# conv1_2a = inception_block(conv1_2, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "# conv1_2 = concatenate([conv1_2, conv1_2a])\n",
    "\n",
    "conv3_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "conv3_1 = inception_block(conv3_1, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "pool3 = MaxPool2D((2, 2), strides=(2, 2),)(conv3_1)\n",
    "\n",
    "up2_2 = tf.nn.depth_to_space(conv3_1, block_size=2)\n",
    "up2_2 = attention_gate(up2_2, conv2_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv2_2 = concatenate([up2_2, conv2_1], axis=bn_axis)\n",
    "conv2_2 = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv2_2 = Conv2D(nb_filter[3],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_2)\n",
    "# conv2_2a = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "# conv2_2 = concatenate([conv2_2, conv2_2a])\n",
    "\n",
    "\n",
    "up1_3 = tf.nn.depth_to_space(conv2_2, block_size=2)\n",
    "up1_3 = attention_gate(up1_3, conv1_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis=bn_axis)\n",
    "conv1_3 = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_3 = Conv2D(nb_filter[2],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv1_3)\n",
    "# conv1_3a = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "# conv1_3 = concatenate([conv1_3, conv1_3a])\n",
    "\n",
    "\n",
    "conv4_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool3, nb_filter=nb_filter[3], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "\n",
    "up3_2 = tf.nn.depth_to_space(conv4_1, block_size=2)\n",
    "up3_2 = attention_gate(up3_2, conv3_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv3_2 = concatenate([up3_2, conv3_1], axis=bn_axis)\n",
    "conv3_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv3_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "up2_3 = tf.nn.depth_to_space(conv3_2, block_size=2)\n",
    "up2_3 = attention_gate(up2_3, conv2_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv2_3 = concatenate([up2_3, conv2_1, conv2_2],axis=bn_axis)\n",
    "conv2_3 = inception_block(conv2_3, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv2_3 = Conv2D(nb_filter[2],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_3)\n",
    "# conv2_3a = inception_block(conv2_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "# conv2_3 = concatenate([conv2_3, conv2_3a])\n",
    "\n",
    "\n",
    "up1_4 = tf.nn.depth_to_space(conv2_3, block_size=2)\n",
    "up1_4 = attention_gate(up1_4, conv1_1, num_filters=nb_filter[3], kernel_size=3)\n",
    "conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], axis=bn_axis)\n",
    "conv1_4 = inception_block(conv1_4, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_4 = Conv2D(nb_filter[2],kernel_size=(3,3), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv1_4)\n",
    "# conv1_4a = inception_block(conv1_4, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "# conv1_4 = concatenate([conv1_4, conv1_4a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a18fd8-dfce-42e7-b7f7-e5e2b75b02b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce27891-ec22-40bf-88ae-83e0b20ecccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conv1_1 = inception_block(inputs, nb_filter[0], nb_filter[0],dropout_rate=dropout_rate_initial,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_1 =  conv_batchnorm_relu_block_SE_and_residual_connections(conv1_1, nb_filter=nb_filter[0], dropout_rate = dropout_rate_initial, kernel_norm=kernel_norm)   \n",
    "pool1 = MaxPool2D((2, 2), strides=(2, 2))(conv1_1)\n",
    "\n",
    "conv2_1 = inception_block(pool1, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv2_1 = transformer_encoder(conv2_1, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "pool2 = MaxPool2D((2, 2), strides=(2, 2))(conv2_1)\n",
    "\n",
    "up1_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_1)\n",
    "conv1_2 = concatenate([up1_2, conv1_1], axis=bn_axis)\n",
    "conv1_2 = inception_block(conv1_2, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv1_2 = transformer_encoder(conv1_2, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "#conv1_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv1_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "#conv3_1=inception_block(pool2, nb_filter[2], nb_filter[2],dropout_rate=dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv3_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv3_1 = transformer_encoder(conv3_1, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "pool3 = MaxPool2D((2, 2), strides=(2, 2),)(conv3_1)\n",
    "\n",
    "up2_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv3_1)\n",
    "conv2_2 = concatenate([up2_2, conv2_1], axis=bn_axis)\n",
    "#conv2_2 = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv2_2 = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv2_2 = transformer_encoder(conv2_2, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "\n",
    "up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_2)\n",
    "conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis=bn_axis)\n",
    "#conv1_3 = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv1_3 = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv1_3 = transformer_encoder(conv1_3, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "\n",
    "\n",
    "#conv4_1 = inception_block(pool3, nb_filter[3], nb_filter[3], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv4_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool3, nb_filter=nb_filter[3], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv4_1 = transformer_encoder(conv4_1, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "conv4_1 = conv_batchnorm_relu_block_SE_and_residual_connections(conv4_1, nb_filter=nb_filter[3], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv4_1)\n",
    "conv3_2 = concatenate([up3_2, conv3_1], axis=bn_axis)\n",
    "conv3_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv3_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv3_2 = transformer_encoder(conv3_2, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "conv3_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv3_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "#conv3_2 = inception_block(conv3_2, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm=kernel_norm)\n",
    "\n",
    "up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2),padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv3_2)\n",
    "conv2_3 = concatenate([up2_3, conv2_1, conv2_2],axis=bn_axis)\n",
    "conv2_3 = inception_block(conv2_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv2_3 = transformer_encoder(conv2_3, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "\n",
    "up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2),padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_3)\n",
    "conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], axis=bn_axis)\n",
    "conv1_4 = inception_block(conv1_4, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "for i in range(num_transformer_loops):\n",
    "    conv1_4 = transformer_encoder(conv1_4, num_heads=num_heads, ff_dim=0, dropout=dropout_rate_later)\n",
    "\n",
    "#For backbone == 5\n",
    "#pool4 = MaxPool2D((2, 2), strides=(2, 2),)(conv4_1)\n",
    "#conv5_1 = inception_block(pool4, nb_filter[3], nb_filter[3], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "#conv5_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool4, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm =kernel_norm)\n",
    "\n",
    "#up4_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv5_1)\n",
    "#conv4_2 = concatenate([up4_2, conv4_1], axis=bn_axis)\n",
    "#conv4_2 = inception_block(conv4_2, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "#conv4_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv4_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm =kernel_norm)\n",
    "\n",
    "#up3_3 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv4_2)\n",
    "#conv3_3 = concatenate([up3_3, conv3_1, conv3_2], axis=bn_axis)\n",
    "#conv3_3 = inception_block(conv3_3, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "#conv3_3 = conv_batchnorm_relu_block_SE_and_residual_connections(conv3_3, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "#up2_4 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2),  padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv3_3)\n",
    "#conv2_4 = concatenate([up2_4, conv2_1, conv2_2, conv2_3],  axis=bn_axis)\n",
    "#conv2_4 = inception_block(conv2_4, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "#conv2_4 =  conv_batchnorm_relu_block_SE_and_residual_connections(conv2_4, nb_filter=nb_filter[1], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "#up1_5 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2),padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_4)\n",
    "#conv1_5 = concatenate([up1_5, conv1_1, conv1_2, conv1_3, conv1_4], axis=bn_axis)\n",
    "#conv1_5 = inception_block(conv1_5, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm=kernel_norm)\n",
    "#conv1_5 =  conv_batchnorm_relu_block_SE_and_residual_connections(conv1_5, nb_filter=nb_filter[0], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "nestnet_output_1 = Conv2D(output_channels, (1, 1), activation='relu', name=f'RZSM_output_1',padding='same',kernel_initializer=kernel_initializer,dtype=tf.float32)(conv1_2)\n",
    "nestnet_output_2 = Conv2D(output_channels, (1, 1), activation='relu', name=f'RZSM_output_2', padding='same',kernel_initializer=kernel_initializer ,dtype=tf.float32)(conv1_3)\n",
    "nestnet_output_3 = Conv2D(output_channels, (1, 1), activation='relu', name=f'RZSM_output_3', padding='same',kernel_initializer=kernel_initializer,dtype=tf.float32)(conv1_4)\n",
    "#nestnet_output_4 = Conv2D(output_channels, (1, 1), activation='relu', name=f'RZSM_output_4', padding='same',kernel_initializer=kernel_initializer,dtype=tf.float32)(conv1_5)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[nestnet_output_1,nestnet_output_2,nestnet_output_3])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfdeab-72e5-4f2e-921a-bd5956b5c141",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d6f9c7-922c-4d54-8af5-dfe671224832",
   "metadata": {},
   "source": [
    "# Large model modelRzsmBackbone4TESTINGrelu.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f8618-b5fd-4e72-84d5-362c025b15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_initializer = 'glorot_uniform'\n",
    "\n",
    "def conv_batchnorm_relu_block_SE_and_residual_connections(input_tensor, nb_filter, dropout_rate, kernel_norm, kernel_size=3,kernel_initializer=kernel_initializer):\n",
    "    depth_multiplier=1\n",
    "    \n",
    "    shortcut = Conv2D(nb_filter, kernel_size=(1, 1), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "    shortcut = Activation('relu')(shortcut)\n",
    "    \n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    #x = DepthwiseConv2D(3, padding ='same', depth_multiplier=depth_multiplier, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(x,training=True)\n",
    "    #x = Dropout(dropout_rate)(x, training =True) #add dropout \n",
    "\n",
    "    x = Conv2D(nb_filter, (kernel_size, kernel_size), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropout_rate)(x, training =True) #add dropout \n",
    "    \n",
    "    residual_SE_block = Add()([shortcut, x])\n",
    "    residual_SE_block = keras_cv.layers.SqueezeAndExcite2D(residual_SE_block.shape[-1])(residual_SE_block)\n",
    "    \n",
    "    return residual_SE_block\n",
    "\n",
    "def inception_block(prevlayer, a, b,dropout_rate, kernel_norm, depth_multiplier = False):\n",
    "    #shortcut = Conv2D(a,kernel_size=(1, 1), padding='same', strides=1, kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    #shortcut = BatchNormalization()(shortcut)\n",
    "    #shortcut = Activation('relu')(shortcut)\n",
    "\n",
    "    shortcut= prevlayer\n",
    "    \n",
    "    if depth_multiplier == True:\n",
    "        depth_multiplier=2\n",
    "    else:\n",
    "        depth_multiplier=1\n",
    "    \n",
    "    conva = DepthwiseConv2D(3, padding ='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    conva = BatchNormalization()(conva)\n",
    "    conva = tf.keras.activations.relu(conva)\n",
    "    conva = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(conva,training=True)\n",
    "    #conva = Dropout(dropout_rate)(conva, training =True) #add dropout\n",
    "    \n",
    "    conva = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conva)\n",
    "    conva = BatchNormalization()(conva)\n",
    "    #conva = tf.keras.activations.relu(conva)\n",
    "    #conva = Dropout(dropout_rate)(conva, training =True) #add dropout\n",
    "\n",
    "    convb = DepthwiseConv2D(5, padding ='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    convb = BatchNormalization()(convb)\n",
    "    convb = tf.keras.activations.relu(convb)\n",
    "    convb = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convb,training=True)\n",
    "    #convb = Dropout(dropout_rate)(convb, training =True) #add dropout\n",
    "    \n",
    "    convb = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convb)\n",
    "    convb = BatchNormalization()(convb)\n",
    "    #convb = tf.keras.activations.relu(convb)\n",
    "    #convb = Dropout(dropout_rate)(convb, training =True) #add dropout\n",
    "\n",
    "    convc = DepthwiseConv2D(7, padding='same',depth_multiplier=depth_multiplier,kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(prevlayer)\n",
    "    convc = BatchNormalization()(convc)\n",
    "    convc = tf.keras.activations.relu(convc)\n",
    "    convc = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convc,training=True)\n",
    "    #convc = Dropout(dropout_rate)(convc, training =True) #add dropout\n",
    "    \n",
    "    convc = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convc)\n",
    "    convc = BatchNormalization()(convc)\n",
    "    #convc = tf.keras.activations.relu(convc)\n",
    "    #convc = Dropout(dropout_rate)(convc, training =True) #add dropout\n",
    "\n",
    "    # if True == pooling:\n",
    "    #     convd = MaxPooling2D(pool_size=(2, 2))(convd)\n",
    "    \n",
    "    #Max pool\n",
    "    convd = MaxPool2D((5,5), strides=(1, 1), padding='same')(prevlayer)\n",
    "    convd = Conv2D(a,(1, 1), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convd)\n",
    "    convd = BatchNormalization()(convd)\n",
    "    convd = tf.keras.activations.relu(convd)\n",
    "    convd = tf.keras.layers.SpatialDropout2D(rate=dropout_rate, data_format='channels_last')(convd,training=True)\n",
    "    #onvd = Dropout(dropout_rate)(convd, training =True) #add dropout\n",
    "    \n",
    "    convd = Conv2D(a,(1,1), padding ='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(convd)\n",
    "    convd = BatchNormalization()(convd)\n",
    "    #convd = tf.keras.activations.relu(convd)\n",
    "    #convd = Dropout(dropout_rate)(convd, training =True) #add dropout\n",
    "\n",
    "    up = concatenate([conva, convb, convc, convd])\n",
    "    \n",
    "    #residual_block = Concatenate()([shortcut, up])\n",
    "    \n",
    "    # Adjust the number of channels in the shortcut connection\n",
    "    if shortcut.shape[-1] != up.shape[-1]:\n",
    "        shortcut = Conv2D(up.shape[-1], kernel_size=1, strides=1, padding='same')(shortcut)\n",
    "\n",
    "    residual_block = Add()([shortcut, up])\n",
    "\n",
    "    residual_SE_block = keras_cv.layers.SqueezeAndExcite2D(residual_block.shape[-1])(residual_block)\n",
    "    \n",
    "    return residual_SE_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d32183-c9bb-4b27-bd18-c750e31b1810",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_1 = inception_block(inputs, nb_filter[0], nb_filter[0],dropout_rate=dropout_rate_initial,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "pool1 = MaxPool2D((2, 2), strides=(2, 2))(conv1_1)\n",
    "\n",
    "conv2_1 = inception_block(pool1, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "pool2 = MaxPool2D((2, 2), strides=(2, 2))(conv2_1)\n",
    "\n",
    "up1_2 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_1)\n",
    "conv1_2 = concatenate([up1_2, conv1_1], axis=bn_axis)\n",
    "conv1_2 = inception_block(conv1_2, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_2 = keras_cv.layers.SqueezeAndExcite2D(conv1_2.shape[-1])(conv1_2)\n",
    "#conv1_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv1_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "#conv3_1=inception_block(pool2, nb_filter[2], nb_filter[2],dropout_rate=dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv3_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "pool3 = MaxPool2D((2, 2), strides=(2, 2),)(conv3_1)\n",
    "\n",
    "up2_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv3_1)\n",
    "conv2_2 = concatenate([up2_2, conv2_1], axis=bn_axis)\n",
    "#conv2_2 = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv2_2 = inception_block(conv2_2, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "\n",
    "up1_3 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_2)\n",
    "conv1_3 = concatenate([up1_3, conv1_1, conv1_2], axis=bn_axis)\n",
    "#conv1_3 = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv1_3 = inception_block(conv1_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_3 = keras_cv.layers.SqueezeAndExcite2D(conv1_3.shape[-1])(conv1_3)\n",
    "\n",
    "#conv4_1 = inception_block(pool3, nb_filter[3], nb_filter[3], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "conv4_1 = conv_batchnorm_relu_block_SE_and_residual_connections(pool3, nb_filter=nb_filter[3], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "\n",
    "\n",
    "up3_2 = Conv2DTranspose(nb_filter[2], (2, 2), strides=(2, 2), padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv4_1)\n",
    "conv3_2 = concatenate([up3_2, conv3_1], axis=bn_axis)\n",
    "conv3_2 = conv_batchnorm_relu_block_SE_and_residual_connections(conv3_2, nb_filter=nb_filter[2], dropout_rate = dropout_rate_later, kernel_norm=kernel_norm)\n",
    "#conv3_2 = inception_block(conv3_2, nb_filter[2], nb_filter[2], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm=kernel_norm)\n",
    "\n",
    "up2_3 = Conv2DTranspose(nb_filter[1], (2, 2), strides=(2, 2),padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv3_2)\n",
    "conv2_3 = concatenate([up2_3, conv2_1, conv2_2],axis=bn_axis)\n",
    "conv2_3 = inception_block(conv2_3, nb_filter[1], nb_filter[1], dropout_rate = dropout_rate_later,depth_multiplier=False, kernel_norm =kernel_norm)\n",
    "\n",
    "up1_4 = Conv2DTranspose(nb_filter[0], (2, 2), strides=(2, 2),padding='same',kernel_initializer=kernel_initializer, kernel_constraint = kernel_norm)(conv2_3)\n",
    "conv1_4 = concatenate([up1_4, conv1_1, conv1_2, conv1_3], axis=bn_axis)\n",
    "conv1_4 = inception_block(conv1_4, nb_filter[0], nb_filter[0], dropout_rate = dropout_rate_later,depth_multiplier=True, kernel_norm =kernel_norm)\n",
    "conv1_4 = keras_cv.layers.SqueezeAndExcite2D(conv1_4.shape[-1])(conv1_4)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[nestnet_output_1,nestnet_output_2,nestnet_output_3])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c9e6de-1d8e-46da-a0a0-391eb1b9c0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
