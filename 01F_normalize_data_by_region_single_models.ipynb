{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bdbb79f-b472-4171-b0ab-30c1b6585412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "    \n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "from metpy.units import units\n",
    "import preprocessUtils as putils\n",
    "from typing import Tuple\n",
    "import dask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b451f-7893-4304-813a-eca432631ab6",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d3ff845-71d9-422d-9e33-09a3af5be48f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nGEFSv12 reforecast appears to have some missing values for some init dates. \\nWe are replacing those with the mean because this is messing up min/max scaling\\nvery badly\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1.) Needs about 50GB RAM for CONUS (shape of file is 48 x 96 lat/lon)\n",
    "\n",
    "Select a specific region (either CONUS, NH (Northern Hemisphere), or australia)\n",
    "\n",
    "Takes about 4 hours to fully complete everything.\n",
    "'''\n",
    "\n",
    "''' \n",
    "GEFSv12 reforecast appears to have some missing values for some init dates. \n",
    "We are replacing those with the mean because this is messing up min/max scaling\n",
    "very badly\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44223dd6-1a0f-4c8d-a95b-cdf86d30fa8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "re_run_OBS_reformat_to_SubX = False #For reformatting the observations to the same lead-time format as GEFSv12\n",
    "plot_distribution = False #This set of functions takes a long time to plot. But if you want to see the anomaly and min-max distribution, select True\n",
    "\n",
    "'''Daily lags for observations -  we can specify up to 12 weeks in lagged RZSM data (these are 7-day increments). \n",
    "You can select whatever values you want (as long as they are before the reforecast period'''\n",
    "daily_lags = putils.make_daily_lags()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d845cd-a1b4-454a-b548-c9ff8577e2a8",
   "metadata": {},
   "source": [
    "# Data directory locations and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "705e8a43-188b-46c5-a45f-02a2637c28c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n"
     ]
    }
   ],
   "source": [
    "region_name = 'CONUS' #or ['australia', 'CONUS']\n",
    "\n",
    "start_date = '1999-01-01' #for reanalysis data. Select earlier dates than the forecasts to allow for smoothing and because we need lagged observations\n",
    "end_date = '2020-03-01' #for reanalysis data\n",
    "\n",
    "\n",
    "global verification_var\n",
    "verification_var = 'soilw_bgrnd_GLEAM' #this is for what we are verifying with DL outputs\n",
    "\n",
    "#Gleam observations\n",
    "gleam_dir = 'Data/GLEAM'\n",
    "\n",
    "#Forecast predictions\n",
    "gefsv12_fcst_dir = 'Data/GEFSv12_reforecast'\n",
    "\n",
    "#ERA5 observations\n",
    "era5_dir = 'Data/ERA5'\n",
    "\n",
    "if region_name == 'australia':\n",
    "    #Gleam observations\n",
    "    gleam_dir = f'Data_{region_name}/GLEAM'\n",
    "    \n",
    "    #Forecast predictions\n",
    "    gefsv12_fcst_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "    \n",
    "    #ERA5 observations\n",
    "    era5_dir = f'Data_{region_name}/ERA5'\n",
    "\n",
    "\n",
    "#Set years for training, validation, and testing\n",
    "train_end = 2015\n",
    "val_end = 2017\n",
    "test_start = 2018\n",
    "\n",
    "\n",
    "spfh_unit = 'kg/kg' #Make sure that our reforecast and observation have the same unit for specific humidity\n",
    "\n",
    "global mask\n",
    "mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "#Get initialized dates from GEFSv12 to convert observations for easier stacking\n",
    "global init_date_list\n",
    "\n",
    "init_date_list = putils.get_init_date_list(forecast_variable_path=f'{gefsv12_fcst_dir}/soilw_bgrnd')\n",
    "init_date_list_datetime = [pd.to_datetime(i) for i in init_date_list]\n",
    "\n",
    "global var_list\n",
    "var_list = ['soilw_bgrnd', 'tmax_2m', 'spfh_2m', 'pwat_eatm','diff_temp_2m','hgt_pres'] #Observation variables to pre-process\n",
    "\n",
    "global lead_select\n",
    "lead_select = [6,13,20,27,34] #For subsetting data by leads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f023cfa8-d5b4-4bc2-8031-d4d71d939e39",
   "metadata": {},
   "source": [
    "# Load Observation Data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cc6fa0-f9ae-4c0a-89db-5bbb73ddff50",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading observations. Applying a 7-day rolling mean between 1999-01-01 and 2020-03-01 for CONUS Region.\n",
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "Number of np.nan values before rolling mean for GLEAM_RZSM_obs: 6741432\n",
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "Number of np.nan values before rolling mean for ERA_precipitable_water_obs: 0\n",
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "Number of np.nan values before rolling mean for ERA_geopotential_z200_obs: 0\n",
      "Creating tmax and difference in tmax and tmin variables.\n",
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "Number of np.nan values before rolling mean for ERA_Tmax_obs: 0\n",
      "Number of np.nan values before rolling mean for ERA_Tmin_obs: 0\n",
      "\n",
      "Loading the previously created specific humidity from Data/ERA5\n",
      "Loaded all observations.\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading observations. Applying a 7-day rolling mean between {start_date} and {end_date} for {region_name} Region.')\n",
    "\n",
    "RZSM_obs_GLEAM,proper_date_time = putils.open_reanalysis_files_and_preprocess_rolling_mean_RZSM_only(path_to_file=f'{gleam_dir}/RZSM_weighted_mean_0_100cm.nc4',\n",
    "                                                             file_variable='GLEAM_RZSM_obs',start_date=start_date,end_date=end_date,region_name = region_name)\n",
    "\n",
    "precipitable_water_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/total_column_water_merged.nc4',\n",
    "                                                             file_variable='ERA_precipitable_water_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "geopotential_z200_ERA = putils.open_reanalysis_files_and_preprocess_rolling_mean_other_files(path_to_file=f'{era5_dir}/geopotential_merged.nc4',\n",
    "                                                             file_variable='ERA_geopotential_z200_obs',start_date=start_date,end_date=end_date,\n",
    "                                                               region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "\n",
    "max_temp_ERA, diff_temp_ERA = putils.open_temperature_files_and_preprocess_rolling_mean(path_to_tmax =f'{era5_dir}/maximum_2m_temperature_merged.nc4', \n",
    "                                                                                 path_to_tmin =f'{era5_dir}/minimum_2m_temperature_merged.nc4',\n",
    "                                                                                 file_variable_tmax = 'ERA_Tmax_obs',file_variable_tmin = 'ERA_Tmin_obs',\n",
    "                                                                                 start_date=start_date, end_date=end_date, \n",
    "                                                                                 region_name = region_name, proper_date_time=proper_date_time)\n",
    "\n",
    "spfh_ERA = putils.calculate_2m_specific_humidity_and_preprocess(path_to_dewpoint=f'{era5_dir}/2m_dewpoint_temperature_merged.nc4', \n",
    "                                              path_to_pressure=f'{era5_dir}/surface_pressure_merged.nc4', \n",
    "                                              file_variable_dewpoint='ERA_2m_dewpoint_obs', file_variable_pressure='ERA_surface_pressure_obs', \n",
    "                                              start_date=start_date, end_date=end_date, \n",
    "                                              region_name = region_name, proper_date_time=proper_date_time,\n",
    "                                              obs_dir = era5_dir, spfh_unit = spfh_unit)\n",
    "\n",
    "print('Loaded all observations.')\n",
    "\n",
    "#Print saving the anomaly for each region as the standard format\n",
    "climatology_season = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.year'] <= train_end)).groupby(f\"time.season\").mean()\n",
    "\n",
    "summer_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'JJA')) - climatology_season.sel(season='JJA')\n",
    "fall_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'SON')) - climatology_season.sel(season='SON')\n",
    "winter_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'DJF')) - climatology_season.sel(season='DJF')\n",
    "spring_ = RZSM_obs_GLEAM.sel(time=(RZSM_obs_GLEAM['time.season'] == 'MAM')) - climatology_season.sel(season='MAM')\n",
    "\n",
    "combined_files = xr.concat([summer_,fall_,winter_,spring_],dim='time').sortby('time')\n",
    "combined_files=combined_files.astype(np.float32)\n",
    "\n",
    "#Now save anomaly for later use\n",
    "if region_name == 'CONUS':\n",
    "    location = 'Data/GLEAM'\n",
    "elif region_name == 'australia':\n",
    "    location = 'Data_australia/GLEAM'\n",
    "    \n",
    "combined_files.to_netcdf(f'{location}/RZSM_anomaly.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af54dc1-ab3a-4d10-afa1-9b81b016884d",
   "metadata": {},
   "source": [
    "# Now restructure the observation data to the same lead-time format as GEFSv12. This will allow for an easier computation of anomalies using climpred functions.\n",
    "\n",
    "### New indexes will be from range (-84 days to -1 day and then the typical 7-day lead times starting from 0 to 34). Because we have applied a 7-day rolling mean to observations so far, even index -1 for the day has the data from the previous 7-days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35812da6-4e93-4db3-b0d2-6c23269410bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def return_info_for_processing(var: str) -> Tuple[str, xr.DataArray]:\n",
    "    #Names to save the data so that it is in the exact same format as the Reforecasts\n",
    "    \n",
    "    if var == 'soilw_bgrnd':\n",
    "        # var_name = var #var_name is the actual name of the variable in the xarray file\n",
    "        model_name = 'GLEAM' #source of the data\n",
    "        obs_file = RZSM_obs_GLEAM #the actual data file\n",
    "    elif var == 'tmax_2m':\n",
    "        # var_name = 'tasmax'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = max_temp_ERA\n",
    "    elif var == 'spfh_2m':\n",
    "        # var_name = 'huss'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = spfh_ERA\n",
    "    elif var == 'pwat_eatm':\n",
    "        # var_name = 'precipitable_water'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = precipitable_water_ERA\n",
    "    elif var == 'diff_temp_2m':\n",
    "        # var_name = 'diff_tmax_tmin'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = diff_temp_ERA\n",
    "    elif var == 'hgt_pres':\n",
    "        # var_name = 'z'\n",
    "        model_name = 'ERA5'\n",
    "        obs_file = geopotential_z200_ERA\n",
    "    \n",
    "    output_OBSERVATIONS_reformat_dir = f'Data/{model_name}/reformat_to_reforecast_shape/{region_name}' #Directory to save \n",
    "    save_dir = f'{output_OBSERVATIONS_reformat_dir}/{var}'\n",
    "    \n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "    \n",
    "    return(model_name,obs_file,output_OBSERVATIONS_reformat_dir,save_dir)\n",
    "\n",
    "\n",
    "\n",
    "def convert_OBS_to_SubX_format(_date: str) -> None:  \n",
    "# for _date in init_date_list:\n",
    "    # var='RZSM_weighted'\n",
    "    # _date=init_date_list[0]\n",
    "\n",
    "    \n",
    "    '''We are going to create new leads that are different than reforecast. The reasoning for this is that we want the actual weekly lags (and 1 day lag) and this will\n",
    "    assist with future predictions within the deep learning model'''\n",
    "\n",
    "    if region_name == 'CONUS':\n",
    "        #Currently the GLEAM and ERA observations are in a format with only positive values for the longitude.\n",
    "        #The current reforecast has negative values for those West of 0 degrees\n",
    "        \n",
    "        new_X_coords = [i+360 if i < 0 else i for i in template.X.values]\n",
    "        single_file = template.assign_coords({'X':new_X_coords})\n",
    "        single_file = xr.zeros_like(single_file)\n",
    "        open_date_SubX = putils.restrict_to_bounding_box(single_file,mask)\n",
    "        \n",
    "    elif region_name == 'australia':\n",
    "        open_date_SubX = template\n",
    "        open_date_SubX = xr.zeros_like(open_date_SubX)\n",
    "    \n",
    "    for var in var_list:\n",
    "        # break\n",
    "        model_name, obs_file, output_OBSERVATIONS_reformat_dir, save_dir = return_info_for_processing(var)     \n",
    "\n",
    "        obs_file_name = f'{var}_reformat_{_date}.nc4'\n",
    "        save_file = f'{save_dir}/{obs_file_name}'\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            print(f'Completed date {_date}')\n",
    "            pass\n",
    "        else:\n",
    "            print(f'Working on variable {var} for date {_date}')\n",
    "            out_file = open_date_SubX.copy(deep=True)\n",
    "    \n",
    "            '''We are going to create a new lead day that represents the previous day before the forecast was initialized\n",
    "            #New shape will be (1x11x48xlatxlon)\n",
    "            This will include the day lag 1, and weekly lags 1-12'''\n",
    "            \n",
    "            file_shape = out_file[putils.xarray_varname(out_file)].shape\n",
    "            \n",
    "            named_str_leads = [str(i) for i in np.arange(-1,open_date_SubX.L.shape[0])]\n",
    "            new_shape = np.empty(shape=(1,file_shape[1],file_shape[2]+13,file_shape[3],file_shape[4]))\n",
    "            new_shape.shape\n",
    "            \n",
    "            daily_lead_lags = daily_lags + list(out_file.L.values[:])\n",
    "            \n",
    "            new_lead_days = xr.Dataset(\n",
    "                data_vars = dict(\n",
    "                    test = (['S','M','L','Y','X'], new_shape[:,:,:,:,:]),\n",
    "                ),\n",
    "                coords = dict(\n",
    "                    S = np.atleast_1d(_date),\n",
    "                    X = open_date_SubX.X.values,\n",
    "                    Y = open_date_SubX.Y.values,\n",
    "                    L = daily_lead_lags,\n",
    "                    M = open_date_SubX.M.values,\n",
    "    \n",
    "                ),\n",
    "                attrs = dict(\n",
    "                    Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                    cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "            )   \n",
    "            \n",
    "            #Create a file to overwrite\n",
    "            out_file = xr.zeros_like(new_lead_days)\n",
    "    \n",
    "            print(f'Working on initialized day {_date} to find values integrating with SubX models, leads, & coordinates and saving data into {save_dir}.')\n",
    "            \n",
    "            for idx,i_lead in enumerate(new_lead_days.L.values):\n",
    "                # break\n",
    "    \n",
    "                date_val = pd.to_datetime(pd.to_datetime(_date) + dt.timedelta(days=int(i_lead)+0)) #Adding +1 may be suitable for other forecasts which predict the next day. But GEFSv12 predicts lead 0 as 12 UTC on the same date it is initialized\n",
    "                #But be careful if you adapt this code to a new script. We are looking backwards in time from the first date.\n",
    "                    \n",
    "                date_val = f'{date_val.year}-{date_val.month:02}-{date_val.day:02}'\n",
    "    \n",
    "                out_file[putils.xarray_varname(out_file)][0,:, idx, :, :] = \\\n",
    "                    obs_file[putils.xarray_varname(obs_file)].sel(time = date_val).values\n",
    "    \n",
    "    \n",
    "            if var == 'soilw_bgrnd':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        RZSM = (['S','M','L','Y','X'],    out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )                    \n",
    "            elif var == 'tmax_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        tmax = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'spfh_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        spfh_2m = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )    \n",
    "            elif var == 'pwat_eatm':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        pwat = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )      \n",
    "            elif var == 'diff_temp_2m':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        diff_tmax_tmin = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            elif var == 'hgt_pres':\n",
    "                var_OUT = xr.Dataset(\n",
    "                    data_vars = dict(\n",
    "                        z200 = (['S','M','L','Y','X'],   out_file[putils.xarray_varname(out_file)].values),\n",
    "                    ),\n",
    "                    coords = dict(\n",
    "                        S = np.atleast_1d(_date),\n",
    "                        X = open_date_SubX.X.values,\n",
    "                        Y = open_date_SubX.Y.values,\n",
    "                        L = daily_lead_lags,\n",
    "                        M = open_date_SubX.M.values,\n",
    "    \n",
    "                    ),\n",
    "                    attrs = dict(\n",
    "                        Description = f'{model_name} {var} values on the exact same date and grid \\\n",
    "                        cell as EMC reforecast data. 7-day rolling mean already applied.'),\n",
    "                )  \n",
    "                \n",
    "            var_OUT = var_OUT.astype(np.float32) #all the files need to be in float32 for the deep learning algorithm to work best\n",
    "            \n",
    "            #Save as a netcdf for later processing\n",
    "            var_OUT.to_netcdf(path = save_file, mode ='w')\n",
    "\n",
    "    return(0)\n",
    "\n",
    "\n",
    "#template for overwriting\n",
    "ref_dir = f'{gefsv12_fcst_dir}/soilw_bgrnd' #Just use a single reference directory to serve as the template for file creation\n",
    "\n",
    "global template\n",
    "#Grab a single SubX to use as the template. Doesn't matter if it is the same variable or not or the same date\n",
    "\n",
    "template = sorted(glob(f'{ref_dir}/*.n*'))[0]\n",
    "template = xr.open_dataset(template)\n",
    "\n",
    "# init_date_list.reverse()\n",
    "for _date  in init_date_list:\n",
    "    convert_OBS_to_SubX_format(_date)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     p = Pool(5)\n",
    "#     p.map(convert_OBS_to_SubX_format,init_date_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0b05a-0cad-435a-b67b-d5b08427c9a8",
   "metadata": {},
   "source": [
    "# Now process all the observations including:\n",
    "### 1.) Create anomaly\n",
    "### 2.) Plot distribution after anomaly\n",
    "### 3.) Create min-max standardization\n",
    "### 4.) Plot distribution after min-max standardization\n",
    "### 5.) Stack verification file (for RZSM) into a seperate directory (as a pickle file)\n",
    "### 6.) Stack all the different lags for all variables into seperate directory (as a pickle file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d6618f5-cc10-483d-a630-ddc197f251a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_observations_only(obs_source: str, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                    return_obs_minmax: bool, gefsv12_fcst_dir: str, rerun_function: bool) -> None:\n",
    "\n",
    "\n",
    "    #Args for creating anomaly (testing)\n",
    "    # obs_source = 'GLEAM'\n",
    "    # region_name = region_name\n",
    "    # var_name= 'soilw_bgrnd'\n",
    "    # obs_or_forecast='obs' \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "        #7-day rolling mean has already been applied to observations\n",
    "\n",
    "        #Create the seasonal mean using climpred functions\n",
    "        file_locations = f'Data/{obs_source}/reformat_to_reforecast_shape/{region_name}/{var_name}/*.n*'\n",
    "\n",
    "        print(f'\\nRetrieving data from {file_locations}\\n')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(xr.open_mfdataset(file_locations),train_end=train_end)\n",
    "        \n",
    "        print(f'\\nCreated seasonal anomalies on all {anom.L.values} lead days')\n",
    "        \n",
    "        print('\\n7-day rolling mean not being applied. We already did this when creating the data.')\n",
    "\n",
    "        anom = anom.load()\n",
    "        \n",
    "        anom_mean_subset = anom.sel(L=lead_select) #Make a small subset of the data\n",
    "\n",
    "\n",
    "        if var_source_combined == verification_var:\n",
    "            #Save the anomaly files\n",
    "            print(f'Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads {anom_mean_subset.L.values}')\n",
    "            putils.convert_OBS_anomaly_to_SubX_format(init_date_list = init_date_list, region_name = region_name, anomaly_file = anom_mean_subset, gefsv12_fcst_dir = gefsv12_fcst_dir, lead_select = lead_select)\n",
    "            \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('\\nLooking at what the first and last dates are within the anomaly file.')\n",
    "        print_dates(anom_mean_subset)\n",
    "    \n",
    "        # #Plot anomaly distribution\n",
    "        # if plot_distribution:\n",
    "        #     print('\\nPlotting anomaly distribution by lead')\n",
    "        #     putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'anomaly', region_name = region_name,\n",
    "        #                                                lead_select=lead_select)\n",
    "    \n",
    "        #Min max standardize\n",
    "        min_max = putils.choose_training_years_and_min_max_scale_mean(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, obs_min_max = None,\n",
    "                                                                lead_select = lead_select)\n",
    "        # #Plot min max distribution\n",
    "        # if plot_distribution:\n",
    "        #     print('\\nPlotting min max distribution by lead')\n",
    "        #     putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "        #                                                lead_select=lead_select)\n",
    "    \n",
    "        if var_source_combined == verification_var:\n",
    "            #We only need to save this as the verification file for deep learning model\n",
    "            putils.create_stacked_files_by_lead_for_verification_ensemble_mean(file = min_max, train_end=train_end, val_end=val_end, test_start=test_start, variable=var_source_combined,\n",
    "                                                          obs_or_forecast=obs_or_forecast, region_name = region_name, init_date_list = init_date_list,\n",
    "                                                                lead_select=lead_select)\n",
    "    \n",
    "        #Now create stacked files for each individual variable for each lead time\n",
    "        putils.retrieve_stacked_files_and_save_for_model_inputs_ensemble_mean(file = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                                                val_end = val_end, test_start = test_start, init_date_list = init_date_list)\n",
    "\n",
    "    if return_obs_minmax == False:\n",
    "        return(f'Completed variable {var_source_combined}')\n",
    "    else:\n",
    "        return(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d01fdc-5a08-4b64-9803-f2c3b3203dd4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieving data from Data/GLEAM/reformat_to_reforecast_shape/CONUS/soilw_bgrnd/*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "Saving anomaly files. If they have already been created, then they will not be re-created. But it still takes a few minutes. Only saving for leads [ 6 13 20 27 34]\n",
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Adding data within function convert_OBS_anomaly_to_SubX_format to file for leads [6, 13, 20, 27, 34]\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for soilw_bgrnd_GLEAM. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.21756413578987122 for lead -84.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1879359781742096 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4055001139640808.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2053094506263733 for lead -77.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1846986711025238 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.3900081217288971.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.20463483035564423 for lead -70.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1982356458902359 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4028704762458801.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22390910983085632 for lead -63.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.1909327358007431 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4148418456315994.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22375306487083435 for lead -56.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.2164846807718277 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.44023774564266205.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.21908098459243774 for lead -49.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.22980783879756927 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.448888823390007.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22184237837791443 for lead -42.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.23530852794647217 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4571509063243866.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22426638007164001 for lead -35.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.225440114736557 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.449706494808197.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22982656955718994 for lead -28.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.21922902762889862 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.44905559718608856.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.23531582951545715 for lead -21.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.21131987869739532 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4466357082128525.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2399522066116333 for lead -14.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.21959079802036285 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.45954300463199615.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.24531131982803345 for lead -7.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18936024606227875 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4346715658903122.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22994668781757355 for lead -1.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18023720383644104 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4101838916540146.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.21657325327396393 for lead 6.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18731509149074554 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.4038883447647095.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.20440731942653656 for lead 13.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19006788730621338 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.39447520673274994.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.2026367038488388 for lead 20.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.18553230166435242 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.3881690055131912.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.23208889365196228 for lead 27.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19124937057495117 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.42333826422691345.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable soilw_bgrnd_GLEAM is 0.22964343428611755 for lead 34.\n",
      "\n",
      "Minimum value for variable soilw_bgrnd_GLEAM is -0.19070173799991608 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable soilw_bgrnd_GLEAM is 0.42034517228603363.\n",
      "\n",
      "Standardizing data with min max for soilw_bgrnd_GLEAM. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for soilw_bgrnd_GLEAM: 0\n",
      "Number of 0 values after min max scaling soilw_bgrnd_GLEAM: 180080780\n",
      "\n",
      "Working on file soilw_bgrnd_GLEAM to save as verification observation data. This takes a while because data is not loaded into memory\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "Train observation shape: (9185, 48, 96)\n",
      "Validation observation shape: (1144, 48, 96)\n",
      "Testing observation shape: (1144, 48, 96)\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on validation data for soilw_bgrnd_GLEAM\n",
      "\n",
      "Working on testing data for soilw_bgrnd_GLEAM\n",
      "Saved soilw_bgrnd_GLEAM data into Data/model_npy_inputs/CONUS/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/CONUS/tmax_2m/*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for tmax_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.046783447265625 for lead -84.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -29.046356201171875 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 51.0931396484375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.59283447265625 for lead -77.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -30.22528076171875 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 51.818115234375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 27.366485595703125 for lead -70.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -28.54095458984375 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 55.907440185546875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 26.52825927734375 for lead -63.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.730377197265625 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 53.258636474609375.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 25.537841796875 for lead -56.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.970855712890625 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 50.508697509765625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 25.252899169921875 for lead -49.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -23.777008056640625 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 49.0299072265625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 23.931884765625 for lead -42.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.44512939453125 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 49.37701416015625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.1878662109375 for lead -35.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -24.899948120117188 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 47.08781433105469.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.5955810546875 for lead -28.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.48614501953125 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 47.08172607421875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 22.783935546875 for lead -21.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -26.412109375 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 49.196044921875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.974151611328125 for lead -14.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -30.774200439453125 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 51.74835205078125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 19.332489013671875 for lead -7.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -29.03497314453125 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.367462158203125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 21.40838623046875 for lead -1.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -28.98431396484375 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 50.3927001953125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.10675048828125 for lead 6.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -28.741424560546875 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 48.848175048828125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 20.21405029296875 for lead 13.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -30.77392578125 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 50.98797607421875.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 27.201080322265625 for lead 20.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -29.0341796875 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 56.235260009765625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 26.45782470703125 for lead 27.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -27.30499267578125 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 53.7628173828125.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable tmax_2m_ERA5 is 25.40234375 for lead 34.\n",
      "\n",
      "Minimum value for variable tmax_2m_ERA5 is -25.446685791015625 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable tmax_2m_ERA5 is 50.849029541015625.\n",
      "\n",
      "Standardizing data with min max for tmax_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for tmax_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling tmax_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for tmax_2m_ERA5\n",
      "\n",
      "Working on validation data for tmax_2m_ERA5\n",
      "\n",
      "Working on testing data for tmax_2m_ERA5\n",
      "Saved tmax_2m_ERA5 data into Data/model_npy_inputs/CONUS/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/CONUS/spfh_2m/*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for spfh_2m_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011665955185890198 for lead -84.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009896742179989815 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021562697365880013.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.012493420392274857 for lead -77.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010320892557501793 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02281431294977665.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.013168765231966972 for lead -70.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.00876993965357542 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021938704885542393.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011511572636663914 for lead -63.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.00926308985799551 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020774662494659424.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011034263297915459 for lead -56.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009476655162870884 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020510918460786343.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011541568674147129 for lead -49.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010989448055624962 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02253101672977209.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011913483962416649 for lead -42.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010563986375927925 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022477470338344574.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01135186292231083 for lead -35.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.011108724400401115 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022460587322711945.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011401853524148464 for lead -28.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010615055449306965 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02201690897345543.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010783757083117962 for lead -21.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009849552996456623 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020633310079574585.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.010592000558972359 for lead -14.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009575688280165195 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020167688839137554.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011065955273807049 for lead -7.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010128039866685867 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021193995140492916.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011702613905072212 for lead -1.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010491062887012959 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02219367679208517.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.01110452227294445 for lead 6.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010508609935641289 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.02161313220858574.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011884607374668121 for lead 13.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.010683493688702583 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.022568101063370705.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.012508973479270935 for lead 20.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.00946264248341322 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021971615962684155.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.012299854308366776 for lead 27.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.009272275492548943 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.021572129800915718.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable spfh_2m_ERA5 is 0.011300627142190933 for lead 34.\n",
      "\n",
      "Minimum value for variable spfh_2m_ERA5 is -0.00952286459505558 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable spfh_2m_ERA5 is 0.020823491737246513.\n",
      "\n",
      "Standardizing data with min max for spfh_2m_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for spfh_2m_ERA5: 0\n",
      "Number of 0 values after min max scaling spfh_2m_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for spfh_2m_ERA5\n",
      "\n",
      "Working on validation data for spfh_2m_ERA5\n",
      "\n",
      "Working on testing data for spfh_2m_ERA5\n",
      "Saved spfh_2m_ERA5 data into Data/model_npy_inputs/CONUS/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/CONUS/pwat_eatm/*.n*\n",
      "\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Created seasonal anomalies on all [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   0   1   2   3   4\n",
      "   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34] lead days\n",
      "\n",
      "7-day rolling mean not being applied. We already did this when creating the data.\n",
      "\n",
      "Looking at what the first and last dates are within the anomaly file.\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for pwat_eatm_ERA5. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [-84 -77 -70 -63 -56 -49 -42 -35 -28 -21 -14  -7  -1   6  13  20  27  34]\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.34703826904297 for lead -84.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -34.49543762207031 for lead -84.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 68.84247589111328.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 32.12218475341797 for lead -77.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -36.13338851928711 for lead -77.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 68.25557327270508.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.293128967285156 for lead -70.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -28.556875228881836 for lead -70.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 62.85000419616699.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.579246520996094 for lead -63.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.076162338256836 for lead -63.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 63.65540885925293.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 37.037445068359375 for lead -56.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.552364349365234 for lead -56.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 67.58980941772461.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.19654846191406 for lead -49.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -33.94413375854492 for lead -49.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 69.14068222045898.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 32.72868728637695 for lead -42.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.123260498046875 for lead -42.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 64.85194778442383.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 30.20757484436035 for lead -35.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.777233123779297 for lead -35.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 60.98480796813965.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 31.722421646118164 for lead -28.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -30.719707489013672 for lead -28.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 62.442129135131836.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 30.28449821472168 for lead -21.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -28.349506378173828 for lead -21.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 58.63400459289551.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 31.142255783081055 for lead -14.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -31.23123550415039 for lead -14.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 62.373491287231445.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 30.7224178314209 for lead -7.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.47890853881836 for lead -7.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 60.20132637023926.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 33.21211624145508 for lead -1.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -31.45740509033203 for lead -1.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 64.66952133178711.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.925384521484375 for lead 6.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -32.60393524169922 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 67.5293197631836.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 32.491546630859375 for lead 13.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -34.293880462646484 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 66.78542709350586.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 34.55989456176758 for lead 20.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -29.41714096069336 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 63.97703552246094.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.745452880859375 for lead 27.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -27.664337158203125 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 63.4097900390625.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable pwat_eatm_ERA5 is 35.51475524902344 for lead 34.\n",
      "\n",
      "Minimum value for variable pwat_eatm_ERA5 is -31.081632614135742 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable pwat_eatm_ERA5 is 66.59638786315918.\n",
      "\n",
      "Standardizing data with min max for pwat_eatm_ERA5. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for pwat_eatm_ERA5: 0\n",
      "Number of 0 values after min max scaling pwat_eatm_ERA5: 198\n",
      "This is only taking the model inputs which are from observations at daily lags (which are actually in weekly form except the very first day (-1))\n",
      "Loading into memory before we save. This takes a while.\n",
      "\n",
      "Working on training data for pwat_eatm_ERA5\n",
      "\n",
      "Working on validation data for pwat_eatm_ERA5\n",
      "\n",
      "Working on testing data for pwat_eatm_ERA5\n",
      "Saved pwat_eatm_ERA5 data into Data/model_npy_inputs/CONUS/Model_input_data\n",
      "\n",
      "Retrieving data from Data/ERA5/reformat_to_reforecast_shape/CONUS/diff_temp_2m/*.n*\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[43manomaly_plot_standardize_save_observations_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mERA5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mregion_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvar_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_end\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mobs_or_forecast\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_distribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplot_distribution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_obs_minmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgefsv12_fcst_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgefsv12_fcst_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mrerun_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrerun_function\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m, in \u001b[0;36manomaly_plot_standardize_save_observations_only\u001b[0;34m(obs_source, region_name, var_name, train_end, val_end, test_start, obs_or_forecast, plot_distribution, return_obs_minmax, gefsv12_fcst_dir, rerun_function)\u001b[0m\n\u001b[1;32m     19\u001b[0m file_locations \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobs_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/reformat_to_reforecast_shape/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregion_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.n*\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRetrieving data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_locations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m anom, mean_season \u001b[38;5;241m=\u001b[39m putils\u001b[38;5;241m.\u001b[39mcreate_seasonal_anomaly(\u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_locations\u001b[49m\u001b[43m)\u001b[49m,train_end\u001b[38;5;241m=\u001b[39mtrain_end)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreated seasonal anomalies on all \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manom\u001b[38;5;241m.\u001b[39mL\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lead days\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m7-day rolling mean not being applied. We already did this when creating the data.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/backends/api.py:1025\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m   1023\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m-> 1025\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [open_(p, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m   1026\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/backends/api.py:1025\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m   1023\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m-> 1025\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m   1026\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/backends/api.py:553\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/backends/plugins.py:154\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m engine, backend \u001b[38;5;129;01min\u001b[39;00m engines\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_can_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstore_spec\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m engine\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mPermissionError\u001b[39;00m:\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:561\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.guess_can_open\u001b[0;34m(self, filename_or_obj)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename_or_obj, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_remote_uri(filename_or_obj):\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mtry_read_magic_number_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;66;03m# netcdf 3 or HDF5\u001b[39;00m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m magic_number\u001b[38;5;241m.\u001b[39mstartswith((\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\211\u001b[39;00m\u001b[38;5;124mHDF\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\032\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/utils.py:673\u001b[0m, in \u001b[0;36mtry_read_magic_number_from_path\u001b[0;34m(pathlike, count)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 673\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_magic_number_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/glade/work/klesinger/conda-envs/tf212gpu_new/lib/python3.10/site-packages/xarray/core/utils.py:661\u001b[0m, in \u001b[0;36mread_magic_number_from_file\u001b[0;34m(filename_or_obj, count)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename_or_obj\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    660\u001b[0m         filename_or_obj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 661\u001b[0m     magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mfilename_or_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    662\u001b[0m     filename_or_obj\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rerun_function = True\n",
    "\n",
    "\n",
    "################### OBSERVATIONS ###########################\n",
    "\n",
    "obs_RZSM_minmax = anomaly_plot_standardize_save_observations_only(obs_source= 'GLEAM', region_name = region_name, var_name='soilw_bgrnd', train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = True, gefsv12_fcst_dir = gefsv12_fcst_dir,\n",
    "                                                                 rerun_function = rerun_function)\n",
    "\n",
    "for var_name in var_list:\n",
    "    if 'soil' in var_name:\n",
    "        pass\n",
    "    else:\n",
    "        anomaly_plot_standardize_save_observations_only(obs_source= 'ERA5', region_name = region_name, var_name=var_name, train_end = train_end, val_end = val_end, test_start = test_start, \n",
    "                                                        obs_or_forecast = 'obs', plot_distribution=plot_distribution, return_obs_minmax = False, gefsv12_fcst_dir = gefsv12_fcst_dir,\n",
    "                                                       rerun_function = rerun_function)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c26ab6c-dbcf-4358-b755-6462445d54a3",
   "metadata": {},
   "source": [
    "# Now create datasets for GEFSV12 reforecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fddf36a-6343-405a-8079-b2387f36edfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# mask = putils.return_proper_mask_for_bounding(region_name)\n",
    "\n",
    "# print(f'\\nLoading files for variable {name_of_var}')\n",
    "\n",
    "# file_paths = sorted(glob(f'{dir_path}/{name_of_var}/*.n*'))\n",
    "\n",
    "# # Open individual xarray datasets from the files\n",
    "# datasets = [xr.open_dataset(file) for file in file_paths]\n",
    "\n",
    "# #We have different coordinate systems. So we need to add 360 to each of the X coordinates if they are negative\n",
    "# if region_name == 'CONUS':\n",
    "#     print(f'We are changing the coordinates of CONUS to match similar format as GLEAM')\n",
    "#     new_X_coords = [i+360 if i < 0 else i for i in datasets[0].X.values]\n",
    "#     #Add the new coordinates\n",
    "#     datasets = [file.assign_coords({'X':new_X_coords}) for file in datasets]\n",
    "#     datasets = [putils.restrict_to_bounding_box(file,mask) for file in datasets]\n",
    "    \n",
    "# # Concatenate the datasets along a specific dimension\n",
    "# print(f'\\nConcatenating files for {name_of_var}. Takes about 10 minutes for CONUS.')\n",
    "# file = xr.concat(datasets, dim='S', combine_attrs=\"override\")\n",
    "\n",
    "# putils._count_zero_values(file, f'{name_of_var}_GEFSv12')\n",
    "# # file = dask.delayed(xr.concat)(datasets, dim=\"S\")\n",
    "\n",
    "# # Close individual datasets to free up resources\n",
    "# for dataset in datasets:\n",
    "#     dataset.close()\n",
    "\n",
    "# # Some files have a value of zero which is skewing results. So we need to replace with the mean.\n",
    "# print('Masking files if they have value of zero')\n",
    "# file_masked = file.where(file == 0, np.nan, file)\n",
    "# #Now take the mean \n",
    "# file_masked_mean = file_masked.mean(dim='S')\n",
    "# #Now replace the values where there was a zero\n",
    "# file = xr.where(np.isnan(file), file_masked_mean, file)\n",
    "\n",
    "# _count_np_nan_values(file, f'{name_of_var}_GEFSv12')\n",
    "# _count_zero_values(file, f'{name_of_var}_GEFSv12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad12ffb-6b77-4e23-9970-c7729c4c52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomaly_plot_standardize_save_reforecast_only_GEFSv12(obs_source, region_name: str, var_name: str, train_end: int, val_end: int, test_start: int, obs_or_forecast: str, plot_distribution: bool, \n",
    "                                                  obs_min_max: xr.DataArray, rerun_function: bool) -> None:\n",
    "    # #Testing\n",
    "    # obs_source= 'GEFSv12'\n",
    "    # region_name = region_name\n",
    "    # var_name='tmax_2m'\n",
    "    # train_end = train_end\n",
    "    # val_end = val_end\n",
    "    # test_start = test_start\n",
    "    # obs_or_forecast = 'reforecast'\n",
    "\n",
    "    \n",
    "    \n",
    "    #Check if the files have been made first, it's a pretty long process\n",
    "    npy_dir_for_model_inputs = putils.final_npy_model_input_directory(region_name)\n",
    "    var_source_combined = f'{var_name}_{obs_source}'\n",
    "\n",
    "    if rerun_function:\n",
    "\n",
    "        # file = putils.return_GEFSv12_reforecast_files(dir_path = gefsv12_fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "        file = putils.return_GEFSv12_reforecast_files_by_concatenation(dir_path = gefsv12_fcst_dir, name_of_var = var_name, region_name = region_name)\n",
    "\n",
    "        if var_name == 'hgt_pres':\n",
    "            file = file* 9.80665 #must place in the same unit as ERA5. the unit is gravity\n",
    "\n",
    "        #Testing if rolling mean first fixes anything. Currenlty the hgt_pres values are way too low for the min\n",
    "\n",
    "        file_test = file.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        file_test = file_test.sel(L=lead_select)\n",
    "        \n",
    "        print(f'\\nCreating seasonal anomalies on all {file_test.L.values} lead days')\n",
    "        \n",
    "        anom, mean_season = putils.create_seasonal_anomaly(file_test,train_end=train_end)\n",
    "\n",
    "        # print('\\nNow taking the 7-day rolling mean of anomalies. We are not replacing lead day 0.')\n",
    "        # anom_mean = anom.rolling(L=7, min_periods=7,center=False).mean()\n",
    "        # anom_mean[putils.xarray_varname(file)][:,:,0,:,:] = anom[putils.xarray_varname(anom)][:,:,0,:,:]\n",
    "        \n",
    "        anom = anom.load()\n",
    "        \n",
    "        print(f'\\nShape of anomaly mean = {anom[putils.xarray_varname(anom)].shape}')\n",
    "        \n",
    "        def print_dates(file):\n",
    "            print(f'First date = {file.S.values[0]}')\n",
    "            print(f'First date = {file.S.values[-1]}')\n",
    "        \n",
    "        print('Looking at what dates are within the file')\n",
    "        print_dates(anom)\n",
    "    \n",
    "        # #Plot anomaly distribution\n",
    "        # if plot_distribution:\n",
    "        #     print('Plotting anomaly distribution by lead')\n",
    "        #     putils.plot_distribution_by_lead_datashader(data_file= anom, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, \n",
    "        #                                                 anomaly_or_min_max = 'anomaly', region_name = region_name,lead_select=lead_select)\n",
    "    \n",
    "        #Min max standardize\n",
    "        if 'soilw_bgrnd' in var_source_combined:\n",
    "            putils.save_baseline_RZSM_anomaly(anom,region_name)\n",
    "            min_max = putils.choose_training_years_and_min_max_scale_mean(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = obs_min_max, lead_select = lead_select)\n",
    "           \n",
    "        else:\n",
    "            min_max = putils.choose_training_years_and_min_max_scale_mean(file = anom, train_end = train_end, variable = var_source_combined, obs_or_forecast = obs_or_forecast, region_name = region_name, \n",
    "                                                                     obs_min_max = None, lead_select = lead_select)\n",
    "\n",
    "        \n",
    "        \n",
    "        # #Plot min max distribution\n",
    "        # if plot_distribution:\n",
    "        #     print('Plotting min min distribution by lead')\n",
    "        #     putils.plot_distribution_by_lead_datashader(data_file= min_max, name_of_var_and_source = var_source_combined, obs_or_forecast = obs_or_forecast, anomaly_or_min_max = 'min_max', region_name = region_name,\n",
    "        #                                                lead_select=lead_select)\n",
    "\n",
    "        putils.stack_reforecasts_ensemble_mean(var_min_max = min_max, variable = var_source_combined,  obs_or_forecast=obs_or_forecast, region_name = region_name, train_end = train_end, \n",
    "                                 val_end = val_end, test_start = test_start, lead_select=lead_select)\n",
    "\n",
    "    return(f'Completed variable {var_source_combined}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12858852-e5c8-43ed-bcef-71de866056a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load tmax and tmin and then create diff_temp_2m (difference in temperature)\n",
    "def create_diff_temp_reforecast(region_name: str) -> None:\n",
    "    if region_name != 'australia':\n",
    "        data_dir = f'Data/GEFSv12_reforecast'\n",
    "    else:\n",
    "        data_dir = f'Data_{region_name}/GEFSv12_reforecast'\n",
    "\n",
    "    save_dir = f'{data_dir}/diff_temp_2m'\n",
    "    os.system(f'mkdir -p {save_dir}')\n",
    "        \n",
    "    tmax_ = sorted(glob(f'{data_dir}/tmax_2m/*.nc4'))\n",
    "    tmin_ = sorted(glob(f'{data_dir}/tmin_2m/*.nc4'))\n",
    "\n",
    "    for idx, file in enumerate(tmax_):\n",
    "        # break\n",
    "        max_ = xr.open_dataset(file)\n",
    "        \n",
    "        date_ = pd.to_datetime(max_.S.values[0])\n",
    "        save_file = f'{save_dir}/diff_temp_2m_{date_.year}-{date_.month:02}-{date_.day:02}.nc4'\n",
    "\n",
    "        if os.path.exists(save_file):\n",
    "            pass\n",
    "        else:\n",
    "            min_ = xr.open_dataset(tmin_[idx])\n",
    "    \n",
    "            assert max_.S.values == min_.S.values, 'Dates are not equal, something is wrong with either glob or there are missing files from either tmax or tmin'\n",
    "    \n",
    "            diff_temp = (max_.tasmax - min_.tasmin).to_dataset(name='diff_tmax_tmin')\n",
    "            \n",
    "            diff_temp.to_netcdf(save_file)\n",
    "    return(0)\n",
    "\n",
    "create_diff_temp_reforecast(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc7d0c9-4836-49ec-808d-8a50dcd58aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Latitude values for mask is [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Longitude values for mask is [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Loading files for variable hgt_pres\n",
      "We are changing the coordinates of CONUS to match similar format as GLEAM\n",
      "\n",
      "Concatenating files for hgt_pres. Takes about 10 minutes for CONUS.\n",
      "Masking files if they have value of zero\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 0\n",
      "Loaded files for hgt_pres\n",
      "\n",
      "Creating seasonal anomalies on all [ 6 13 20 27 34] lead days\n",
      "\n",
      "Longitude values of file we are performing anomaly on are [238.  238.5 239.  239.5 240.  240.5 241.  241.5 242.  242.5 243.  243.5\n",
      " 244.  244.5 245.  245.5 246.  246.5 247.  247.5 248.  248.5 249.  249.5\n",
      " 250.  250.5 251.  251.5 252.  252.5 253.  253.5 254.  254.5 255.  255.5\n",
      " 256.  256.5 257.  257.5 258.  258.5 259.  259.5 260.  260.5 261.  261.5\n",
      " 262.  262.5 263.  263.5 264.  264.5 265.  265.5 266.  266.5 267.  267.5\n",
      " 268.  268.5 269.  269.5 270.  270.5 271.  271.5 272.  272.5 273.  273.5\n",
      " 274.  274.5 275.  275.5 276.  276.5 277.  277.5 278.  278.5 279.  279.5\n",
      " 280.  280.5 281.  281.5 282.  282.5 283.  283.5 284.  284.5 285.  285.5]\n",
      "\n",
      "Latitude values of file we are performing anomaly on are [50.  49.5 49.  48.5 48.  47.5 47.  46.5 46.  45.5 45.  44.5 44.  43.5\n",
      " 43.  42.5 42.  41.5 41.  40.5 40.  39.5 39.  38.5 38.  37.5 37.  36.5\n",
      " 36.  35.5 35.  34.5 34.  33.5 33.  32.5 32.  31.5 31.  30.5 30.  29.5\n",
      " 29.  28.5 28.  27.5 27.  26.5]\n",
      "\n",
      "Getting all data before year 2016\n",
      "\n",
      "Making anomalies for each individual lead\n",
      "\n",
      "Shape of anomaly mean = (1043, 11, 5, 48, 96)\n",
      "Looking at what dates are within the file\n",
      "First date = 2000-01-05T00:00:00.000000000\n",
      "First date = 2019-12-25T00:00:00.000000000\n",
      "\n",
      "Working on lead [ 6 13 20 27 34] for min max scaling... This is the size of the input\n",
      "\n",
      "Creating min max for hgt_pres_GEFSv12. This takes a while because it is not loaded into memory.\n",
      "\n",
      "Training file dates with which we are creating the min and maximum files for scaling: 2000-01-05T00:00:00.000000000\n",
      "\n",
      "Training file dates end which we are creating the min and maximum files for scaling: 2015-12-30T00:00:00.000000000\n",
      "\n",
      "Training lead values: [ 6 13 20 27 34]\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 5339.6953125 for lead 6.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -5968.7109375 for lead 6.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 11308.40625.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6252.234375 for lead 13.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -6123.2890625 for lead 13.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 12375.5234375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6190.6796875 for lead 20.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -6538.6015625 for lead 20.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 12729.28125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6768.109375 for lead 27.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -6311.5859375 for lead 27.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 13079.6953125.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "\n",
      "Maximum value for variable hgt_pres_GEFSv12 is 6725.8984375 for lead 34.\n",
      "\n",
      "Minimum value for variable hgt_pres_GEFSv12 is -7108.3359375 for lead 34.\n",
      "\n",
      "Maximum minus minimum value for variable hgt_pres_GEFSv12 is 13834.234375.\n",
      "\n",
      "Standardizing data with min max for hgt_pres_GEFSv12. (x-min) / (max - min)\n",
      "Number of np.nan values before rolling mean for hgt_pres_GEFSv12: 0\n",
      "Number of 0 values after min max scaling hgt_pres_GEFSv12: 5\n",
      "\n",
      "Stacking together REFORECAST variable hgt_pres_GEFSv12\n",
      "Loading file into memory. This takes a while.\n",
      "Saved hgt_pres_GEFSv12 data into Data/model_npy_inputs/CONUS/Model_input_data\n"
     ]
    }
   ],
   "source": [
    "rerun_function = True\n",
    "\n",
    "for var_name in ['spfh_2m', 'tmax_2m', 'hgt_pres','soilw_bgrnd', 'diff_temp_2m', 'pwat_eatm']:\n",
    "    if var_name == 'soilw_bgrnd':\n",
    "        obs_min_max = obs_RZSM_minmax\n",
    "    else:\n",
    "        obs_min_max = None\n",
    "    anomaly_plot_standardize_save_reforecast_only_GEFSv12(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "                                                  val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "                                                  obs_min_max = obs_min_max, rerun_function=rerun_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985201bf-8dd8-4523-afd4-0e8fdc1b0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun_function = True\n",
    "\n",
    "# for var_name in ['soilw_bgrnd']:\n",
    "#     if var_name == 'soilw_bgrnd':\n",
    "#         obs_min_max = None\n",
    "#     else:\n",
    "#         obs_min_max = None\n",
    "#     anomaly_plot_standardize_save_reforecast_only_GEFSv12(obs_source= 'GEFSv12', region_name = region_name, var_name=var_name, train_end = train_end, \n",
    "#                                                   val_end = val_end, test_start = test_start, obs_or_forecast = 'reforecast', plot_distribution = plot_distribution,\n",
    "#                                                   obs_min_max = obs_min_max, rerun_function=rerun_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0621d8-1972-4f6d-8e58-fb3b2e98e1bf",
   "metadata": {},
   "source": [
    "# Double check the units with precipitable water and surface specific humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b37468-bc7d-4af9-8359-45ef70a7e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precipitable water units\n",
    "#Reforecast (kg m-2, i.e., mm)   Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations (kg m-2)    Source; https://codes.ecmwf.int/grib/param-db/136 \n",
    "\n",
    "#check the scale of the data for pwat and spfh between forecasts and observations\n",
    "pwat_ref = xr.open_dataset('Data/GEFSv12_reforecast/pwat_eatm/precipitable_water_EMC_2000-01-05.nc4')\n",
    "pwat_obs = xr.open_dataset('Data/ERA5/reformat_to_reforecast_shape/CONUS/pwat_eatm/pwat_eatm_reformat_2000-01-05.nc4')\n",
    "\n",
    "pwat_ref.max()\n",
    "pwat_obs.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd33a018-0b72-4626-874d-ea50580df62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specific humidity units\n",
    "#Reforecast (kg kg-1 dry air) Source; https://noaa-gefs-retrospective.s3.amazonaws.com/Description_of_reforecast_data.pdf\n",
    "#Observations ('kg/kg'). I computed this using Metpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46136936-2c05-4788-9d93-1645f74b7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check stacked files\n",
    "import pickle\n",
    "with open(f'/glade/work/klesinger/FD_RZSM_deep_learning/Data/model_npy_inputs/CONUS/Model_input_data/reforecast_tmax_2m_GEFSv12_training.pickle', 'rb') as handle:\n",
    "    a= pickle.load(handle)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745becd8-5f48-4432-8a19-cefb688ab4bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if verification data is fine\n",
    "# output_npy_dir = f'Data/model_npy_inputs/{}Verification_data'\n",
    "# from glob import glob\n",
    "\n",
    "# for file in glob(f'{output_npy_dir}/OBS*'):\n",
    "#     print(file)\n",
    "#     print(np.load(file))\n",
    "\n",
    "##### The Data loads fine and fast\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf212gpu_new]",
   "language": "python",
   "name": "conda-env-tf212gpu_new-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
